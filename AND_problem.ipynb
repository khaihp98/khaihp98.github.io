{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AND_problem.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"C39lOam3Q9nQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"91e06a5e-b23e-4bae-c201-2a3f6cc3d2bb","executionInfo":{"status":"ok","timestamp":1573831438538,"user_tz":-420,"elapsed":773,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n","y = np.array([0, 0, 0, 1])\n","# bias trick\n","Xbar = np.concatenate((X, np.ones((1, 4))), axis=0)\n","plt.plot(X[0],X[1] , 'o', markersize=8)\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.xlim((0, 2))\n","plt.ylim((0, 2))\n","\n","plt.show()"],"execution_count":84,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQ0AAAD8CAYAAABtq/EAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"zhuUbl8mUALl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6b532630-be4f-43fa-94f0-8e6c1f80da43","executionInfo":{"status":"ok","timestamp":1573827772052,"user_tz":-420,"elapsed":907,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["w_init = np.array([[0], [0.5], [1]])\n"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 1)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"_w1zg2M5VrAT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f931e3e1-93ca-4174-87d3-d9f8faeeba8a","executionInfo":{"status":"ok","timestamp":1573828959983,"user_tz":-420,"elapsed":881,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["def sigmoid(s):\n","  return 1/(1 + np.exp(-s))\n","def loss(y, yhat):\n","  return -np.mean(y*np.log(yhat) + (1-y)*np.log(1-yhat))\n","\n","yhat = sigmoid(w_init.T.dot(Xbar))\n","print(yhat)\n","print(loss(y, yhat))"],"execution_count":48,"outputs":[{"output_type":"stream","text":["[[0.73105858 0.81757448 0.73105858 0.81757448]]\n","1.1323374827504875\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_eXccNnCUhCl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"dec0054c-7dbd-4324-fa23-4d792c1bc096","executionInfo":{"status":"ok","timestamp":1573827562228,"user_tz":-420,"elapsed":849,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["\n","print(Xbar)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["[[1. 1. 1. 1.]]\n","[[0. 0. 1. 1.]\n"," [0. 1. 0. 1.]\n"," [1. 1. 1. 1.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qK8K_OfdUrg8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6488caa8-0816-4edf-bfb9-e787d2585c02","executionInfo":{"status":"ok","timestamp":1573830262209,"user_tz":-420,"elapsed":8300,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["def fit(w, X, y, alpha0, iters=5000, decay=1):\n","  for it in range(iters):\n","    #alpha = 1/(1+decay*(it+1))*alpha0\n","    z = w.T.dot(X)\n","    yhat = sigmoid(z)\n","    update = np.mean((yhat-y)*X, axis=1).reshape((3, 1))\n","    print('Iter: ', it+1, 'loss = ', loss(y, yhat), 'learning rate = ', alpha0, 'update = ', update)\n","    if np.linalg.norm(update) < 1e-4:\n","      break\n","    else:\n","      w = w - alpha0*update\n","  return w\n","ww = fit(w_init, Xbar, y, alpha0=0.5)"],"execution_count":76,"outputs":[{"output_type":"stream","text":["Iter:  1 loss =  1.1323374827504875 learning rate =  0.5 update =  [[0.13715826]\n"," [0.15878724]\n"," [0.52431653]]\n","Iter:  2 loss =  0.9831902240449825 learning rate =  0.5 update =  [[0.10242377]\n"," [0.12735192]\n"," [0.46181735]]\n","Iter:  3 loss =  0.8720273531481213 learning rate =  0.5 update =  [[0.06837007]\n"," [0.09533824]\n"," [0.40025868]]\n","Iter:  4 loss =  0.7917990512761588 learning rate =  0.5 update =  [[0.03791059]\n"," [0.06570817]\n"," [0.34426758]]\n","Iter:  5 loss =  0.7344388474205296 learning rate =  0.5 update =  [[0.01242312]\n"," [0.0401507 ]\n"," [0.29616382]]\n","Iter:  6 loss =  0.6929233254271219 learning rate =  0.5 update =  [[-0.00801292]\n"," [ 0.01909298]\n"," [ 0.25625711]]\n","Iter:  7 loss =  0.6620070550349542 learning rate =  0.5 update =  [[-0.023995  ]\n"," [ 0.00220529]\n"," [ 0.2237452 ]]\n","Iter:  8 loss =  0.6380921225973812 learning rate =  0.5 update =  [[-0.0363179 ]\n"," [-0.01113657]\n"," [ 0.19744485]]\n","Iter:  9 loss =  0.6188150272109664 learning rate =  0.5 update =  [[-0.0457349 ]\n"," [-0.02158953]\n"," [ 0.17618132]]\n","Iter:  10 loss =  0.6026513642200908 learning rate =  0.5 update =  [[-0.05287691]\n"," [-0.02973544]\n"," [ 0.15893795]]\n","Iter:  11 loss =  0.5886220340023735 learning rate =  0.5 update =  [[-0.058245  ]\n"," [-0.0360536 ]\n"," [ 0.14488612]]\n","Iter:  12 loss =  0.5760958787048682 learning rate =  0.5 update =  [[-0.06222845]\n"," [-0.04092629]\n"," [ 0.1333683 ]]\n","Iter:  13 loss =  0.5646627387078695 learning rate =  0.5 update =  [[-0.06512777]\n"," [-0.0446545 ]\n"," [ 0.12386808]]\n","Iter:  14 loss =  0.5540531552084388 learning rate =  0.5 update =  [[-0.06717524]\n"," [-0.04747445]\n"," [ 0.1159807 ]]\n","Iter:  15 loss =  0.5440877537134626 learning rate =  0.5 update =  [[-0.06855133]\n"," [-0.04957184]\n"," [ 0.1093882 ]]\n","Iter:  16 loss =  0.5346452406453456 learning rate =  0.5 update =  [[-0.06939717]\n"," [-0.05109329]\n"," [ 0.10383978]]\n","Iter:  17 loss =  0.5256420484340248 learning rate =  0.5 update =  [[-0.0698239 ]\n"," [-0.05215508]\n"," [ 0.09913662]]\n","Iter:  18 loss =  0.5170193000396058 learning rate =  0.5 update =  [[-0.06991959]\n"," [-0.0528499 ]\n"," [ 0.09512034]]\n","Iter:  19 loss =  0.5087344038932732 learning rate =  0.5 update =  [[-0.06975443]\n"," [-0.05325198]\n"," [ 0.09166422]]\n","Iter:  20 loss =  0.5007556005001229 learning rate =  0.5 update =  [[-0.0693846 ]\n"," [-0.05342096]\n"," [ 0.08866642]]\n","Iter:  21 loss =  0.49305840460394135 learning rate =  0.5 update =  [[-0.06885523]\n"," [-0.05340488]\n"," [ 0.08604483]]\n","Iter:  22 loss =  0.4856232728688343 learning rate =  0.5 update =  [[-0.06820267]\n"," [-0.0532426 ]\n"," [ 0.08373305]]\n","Iter:  23 loss =  0.47843406842654923 learning rate =  0.5 update =  [[-0.06745621]\n"," [-0.05296549]\n"," [ 0.08167724]]\n","Iter:  24 loss =  0.471477045990811 learning rate =  0.5 update =  [[-0.06663952]\n"," [-0.052599  ]\n"," [ 0.07983365]]\n","Iter:  25 loss =  0.4647401782812123 learning rate =  0.5 update =  [[-0.06577166]\n"," [-0.05216369]\n"," [ 0.07816669]]\n","Iter:  26 loss =  0.45821270683394766 learning rate =  0.5 update =  [[-0.06486801]\n"," [-0.05167622]\n"," [ 0.07664731]]\n","Iter:  27 loss =  0.4518848406149095 learning rate =  0.5 update =  [[-0.06394092]\n"," [-0.05115004]\n"," [ 0.07525182]]\n","Iter:  28 loss =  0.44574755211696493 learning rate =  0.5 update =  [[-0.0630003 ]\n"," [-0.050596  ]\n"," [ 0.07396085]]\n","Iter:  29 loss =  0.4397924378121436 learning rate =  0.5 update =  [[-0.06205406]\n"," [-0.05002283]\n"," [ 0.07275851]]\n","Iter:  30 loss =  0.43401162112109043 learning rate =  0.5 update =  [[-0.06110848]\n"," [-0.04943754]\n"," [ 0.07163182]]\n","Iter:  31 loss =  0.4283976835000294 learning rate =  0.5 update =  [[-0.06016853]\n"," [-0.04884572]\n"," [ 0.07057007]]\n","Iter:  32 loss =  0.4229436141534577 learning rate =  0.5 update =  [[-0.05923807]\n"," [-0.04825183]\n"," [ 0.0695645 ]]\n","Iter:  33 loss =  0.4176427721221084 learning rate =  0.5 update =  [[-0.05832012]\n"," [-0.04765938]\n"," [ 0.06860785]]\n","Iter:  34 loss =  0.4124888566364544 learning rate =  0.5 update =  [[-0.05741695]\n"," [-0.04707112]\n"," [ 0.06769413]]\n","Iter:  35 loss =  0.40747588303888027 learning rate =  0.5 update =  [[-0.05653028]\n"," [-0.04648919]\n"," [ 0.0668184 ]]\n","Iter:  36 loss =  0.40259816250879377 learning rate =  0.5 update =  [[-0.05566135]\n"," [-0.04591522]\n"," [ 0.06597652]]\n","Iter:  37 loss =  0.3978502844373286 learning rate =  0.5 update =  [[-0.05481102]\n"," [-0.04535046]\n"," [ 0.06516507]]\n","Iter:  38 loss =  0.39322710069988853 learning rate =  0.5 update =  [[-0.05397986]\n"," [-0.0447958 ]\n"," [ 0.06438116]]\n","Iter:  39 loss =  0.3887237113371901 learning rate =  0.5 update =  [[-0.05316818]\n"," [-0.04425189]\n"," [ 0.06362237]]\n","Iter:  40 loss =  0.3843354513261673 learning rate =  0.5 update =  [[-0.0523761 ]\n"," [-0.04371916]\n"," [ 0.06288666]]\n","Iter:  41 loss =  0.38005787823253034 learning rate =  0.5 update =  [[-0.0516036 ]\n"," [-0.04319786]\n"," [ 0.06217229]]\n","Iter:  42 loss =  0.37588676060774684 learning rate =  0.5 update =  [[-0.05085052]\n"," [-0.04268813]\n"," [ 0.06147776]]\n","Iter:  43 loss =  0.3718180670384653 learning rate =  0.5 update =  [[-0.05011663]\n"," [-0.04218998]\n"," [ 0.06080179]]\n","Iter:  44 loss =  0.36784795578498664 learning rate =  0.5 update =  [[-0.04940161]\n"," [-0.04170336]\n"," [ 0.06014327]]\n","Iter:  45 loss =  0.36397276496324704 learning rate =  0.5 update =  [[-0.04870509]\n"," [-0.04122814]\n"," [ 0.05950123]]\n","Iter:  46 loss =  0.36018900323579633 learning rate =  0.5 update =  [[-0.04802666]\n"," [-0.04076414]\n"," [ 0.05887482]]\n","Iter:  47 loss =  0.3564933409839657 learning rate =  0.5 update =  [[-0.04736588]\n"," [-0.04031116]\n"," [ 0.05826327]]\n","Iter:  48 loss =  0.35288260193745774 learning rate =  0.5 update =  [[-0.04672229]\n"," [-0.03986894]\n"," [ 0.0576659 ]]\n","Iter:  49 loss =  0.3493537552400048 learning rate =  0.5 update =  [[-0.04609543]\n"," [-0.03943722]\n"," [ 0.05708211]]\n","Iter:  50 loss =  0.345903907931198 learning rate =  0.5 update =  [[-0.04548481]\n"," [-0.03901575]\n"," [ 0.05651134]]\n","Iter:  51 loss =  0.34253029782550404 learning rate =  0.5 update =  [[-0.04488997]\n"," [-0.03860422]\n"," [ 0.05595308]]\n","Iter:  52 loss =  0.3392302867701155 learning rate =  0.5 update =  [[-0.04431043]\n"," [-0.03820236]\n"," [ 0.05540687]]\n","Iter:  53 loss =  0.33600135426378264 learning rate =  0.5 update =  [[-0.04374571]\n"," [-0.03780988]\n"," [ 0.05487227]]\n","Iter:  54 loss =  0.3328410914192197 learning rate =  0.5 update =  [[-0.04319536]\n"," [-0.03742649]\n"," [ 0.05434888]]\n","Iter:  55 loss =  0.32974719525214563 learning rate =  0.5 update =  [[-0.04265892]\n"," [-0.0370519 ]\n"," [ 0.05383634]]\n","Iter:  56 loss =  0.32671746328049045 learning rate =  0.5 update =  [[-0.04213595]\n"," [-0.03668585]\n"," [ 0.05333427]]\n","Iter:  57 loss =  0.3237497884178213 learning rate =  0.5 update =  [[-0.04162602]\n"," [-0.03632806]\n"," [ 0.05284236]]\n","Iter:  58 loss =  0.3208421541455829 learning rate =  0.5 update =  [[-0.04112871]\n"," [-0.03597825]\n"," [ 0.05236028]]\n","Iter:  59 loss =  0.3179926299493231 learning rate =  0.5 update =  [[-0.04064362]\n"," [-0.03563618]\n"," [ 0.05188774]]\n","Iter:  60 loss =  0.315199367004669 learning rate =  0.5 update =  [[-0.04017034]\n"," [-0.0353016 ]\n"," [ 0.05142446]]\n","Iter:  61 loss =  0.31246059409942123 learning rate =  0.5 update =  [[-0.03970849]\n"," [-0.03497425]\n"," [ 0.05097014]]\n","Iter:  62 loss =  0.30977461377875337 learning rate =  0.5 update =  [[-0.03925772]\n"," [-0.03465391]\n"," [ 0.05052455]]\n","Iter:  63 loss =  0.30713979870110664 learning rate =  0.5 update =  [[-0.03881765]\n"," [-0.03434035]\n"," [ 0.05008743]]\n","Iter:  64 loss =  0.3045545881929811 learning rate =  0.5 update =  [[-0.03838795]\n"," [-0.03403335]\n"," [ 0.04965853]]\n","Iter:  65 loss =  0.30201748499141773 learning rate =  0.5 update =  [[-0.03796828]\n"," [-0.03373269]\n"," [ 0.04923763]]\n","Iter:  66 loss =  0.29952705216354514 learning rate =  0.5 update =  [[-0.03755832]\n"," [-0.03343819]\n"," [ 0.04882451]]\n","Iter:  67 loss =  0.29708191019312785 learning rate =  0.5 update =  [[-0.03715776]\n"," [-0.03314963]\n"," [ 0.04841895]]\n","Iter:  68 loss =  0.29468073422460006 learning rate =  0.5 update =  [[-0.03676631]\n"," [-0.03286684]\n"," [ 0.04802074]]\n","Iter:  69 loss =  0.29232225145558643 learning rate =  0.5 update =  [[-0.03638368]\n"," [-0.03258962]\n"," [ 0.0476297 ]]\n","Iter:  70 loss =  0.2900052386694207 learning rate =  0.5 update =  [[-0.03600959]\n"," [-0.03231782]\n"," [ 0.04724562]]\n","Iter:  71 loss =  0.2877285198996453 learning rate =  0.5 update =  [[-0.03564376]\n"," [-0.03205125]\n"," [ 0.04686832]]\n","Iter:  72 loss =  0.28549096421893727 learning rate =  0.5 update =  [[-0.03528596]\n"," [-0.03178976]\n"," [ 0.04649762]]\n","Iter:  73 loss =  0.2832914836453384 learning rate =  0.5 update =  [[-0.03493592]\n"," [-0.0315332 ]\n"," [ 0.04613335]]\n","Iter:  74 loss =  0.2811290311590806 learning rate =  0.5 update =  [[-0.03459341]\n"," [-0.03128142]\n"," [ 0.04577534]]\n","Iter:  75 loss =  0.2790025988236888 learning rate =  0.5 update =  [[-0.03425821]\n"," [-0.03103426]\n"," [ 0.04542344]]\n","Iter:  76 loss =  0.27691121600541446 learning rate =  0.5 update =  [[-0.03393007]\n"," [-0.0307916 ]\n"," [ 0.04507747]]\n","Iter:  77 loss =  0.2748539476854025 learning rate =  0.5 update =  [[-0.03360881]\n"," [-0.03055331]\n"," [ 0.0447373 ]]\n","Iter:  78 loss =  0.2728298928593227 learning rate =  0.5 update =  [[-0.0332942 ]\n"," [-0.03031925]\n"," [ 0.04440277]]\n","Iter:  79 loss =  0.2708381830195132 learning rate =  0.5 update =  [[-0.03298604]\n"," [-0.0300893 ]\n"," [ 0.04407375]]\n","Iter:  80 loss =  0.26887798071497127 learning rate =  0.5 update =  [[-0.03268416]\n"," [-0.02986335]\n"," [ 0.04375009]]\n","Iter:  81 loss =  0.2669484781848087 learning rate =  0.5 update =  [[-0.03238836]\n"," [-0.02964129]\n"," [ 0.04343167]]\n","Iter:  82 loss =  0.2650488960610448 learning rate =  0.5 update =  [[-0.03209847]\n"," [-0.029423  ]\n"," [ 0.04311835]]\n","Iter:  83 loss =  0.2631784821368595 learning rate =  0.5 update =  [[-0.03181431]\n"," [-0.02920837]\n"," [ 0.04281001]]\n","Iter:  84 loss =  0.2613365101966541 learning rate =  0.5 update =  [[-0.03153572]\n"," [-0.02899731]\n"," [ 0.04250652]]\n","Iter:  85 loss =  0.25952227890448987 learning rate =  0.5 update =  [[-0.03126254]\n"," [-0.02878973]\n"," [ 0.04220778]]\n","Iter:  86 loss =  0.2577351107476714 learning rate =  0.5 update =  [[-0.03099461]\n"," [-0.02858552]\n"," [ 0.04191367]]\n","Iter:  87 loss =  0.25597435103243893 learning rate =  0.5 update =  [[-0.03073179]\n"," [-0.0283846 ]\n"," [ 0.04162407]]\n","Iter:  88 loss =  0.25423936692890814 learning rate =  0.5 update =  [[-0.03047394]\n"," [-0.02818688]\n"," [ 0.04133888]]\n","Iter:  89 loss =  0.252529546562569 learning rate =  0.5 update =  [[-0.0302209 ]\n"," [-0.02799227]\n"," [ 0.04105799]]\n","Iter:  90 loss =  0.2508442981498099 learning rate =  0.5 update =  [[-0.02997256]\n"," [-0.02780069]\n"," [ 0.04078131]]\n","Iter:  91 loss =  0.2491830491750831 learning rate =  0.5 update =  [[-0.02972879]\n"," [-0.02761208]\n"," [ 0.04050874]]\n","Iter:  92 loss =  0.24754524560746807 learning rate =  0.5 update =  [[-0.02948945]\n"," [-0.02742634]\n"," [ 0.04024018]]\n","Iter:  93 loss =  0.24593035115451978 learning rate =  0.5 update =  [[-0.02925442]\n"," [-0.02724342]\n"," [ 0.03997554]]\n","Iter:  94 loss =  0.24433784655141005 learning rate =  0.5 update =  [[-0.0290236 ]\n"," [-0.02706323]\n"," [ 0.03971473]]\n","Iter:  95 loss =  0.24276722888348795 learning rate =  0.5 update =  [[-0.02879686]\n"," [-0.02688571]\n"," [ 0.03945767]]\n","Iter:  96 loss =  0.24121801094049333 learning rate =  0.5 update =  [[-0.02857411]\n"," [-0.0267108 ]\n"," [ 0.03920427]]\n","Iter:  97 loss =  0.23968972060075666 learning rate =  0.5 update =  [[-0.02835523]\n"," [-0.02653843]\n"," [ 0.03895445]]\n","Iter:  98 loss =  0.2381819002438184 learning rate =  0.5 update =  [[-0.02814012]\n"," [-0.02636854]\n"," [ 0.03870813]]\n","Iter:  99 loss =  0.23669410618998596 learning rate =  0.5 update =  [[-0.02792869]\n"," [-0.02620107]\n"," [ 0.03846523]]\n","Iter:  100 loss =  0.23522590816543498 learning rate =  0.5 update =  [[-0.02772083]\n"," [-0.02603597]\n"," [ 0.03822569]]\n","Iter:  101 loss =  0.23377688879153707 learning rate =  0.5 update =  [[-0.02751647]\n"," [-0.02587317]\n"," [ 0.03798941]]\n","Iter:  102 loss =  0.23234664309717346 learning rate =  0.5 update =  [[-0.0273155 ]\n"," [-0.02571264]\n"," [ 0.03775634]]\n","Iter:  103 loss =  0.23093477805286228 learning rate =  0.5 update =  [[-0.02711785]\n"," [-0.0255543 ]\n"," [ 0.03752641]]\n","Iter:  104 loss =  0.22954091212559236 learning rate =  0.5 update =  [[-0.02692343]\n"," [-0.02539812]\n"," [ 0.03729955]]\n","Iter:  105 loss =  0.22816467485332192 learning rate =  0.5 update =  [[-0.02673215]\n"," [-0.02524405]\n"," [ 0.0370757 ]]\n","Iter:  106 loss =  0.2268057064381505 learning rate =  0.5 update =  [[-0.02654395]\n"," [-0.02509203]\n"," [ 0.03685479]]\n","Iter:  107 loss =  0.2254636573572386 learning rate =  0.5 update =  [[-0.02635874]\n"," [-0.02494203]\n"," [ 0.03663676]]\n","Iter:  108 loss =  0.2241381879905887 learning rate =  0.5 update =  [[-0.02617646]\n"," [-0.024794  ]\n"," [ 0.03642155]]\n","Iter:  109 loss =  0.22282896826485904 learning rate =  0.5 update =  [[-0.02599702]\n"," [-0.02464789]\n"," [ 0.0362091 ]]\n","Iter:  110 loss =  0.2215356773124218 learning rate =  0.5 update =  [[-0.02582037]\n"," [-0.02450367]\n"," [ 0.03599936]]\n","Iter:  111 loss =  0.22025800314492155 learning rate =  0.5 update =  [[-0.02564643]\n"," [-0.02436129]\n"," [ 0.03579228]]\n","Iter:  112 loss =  0.2189956423406303 learning rate =  0.5 update =  [[-0.02547515]\n"," [-0.02422072]\n"," [ 0.03558779]]\n","Iter:  113 loss =  0.21774829974493276 learning rate =  0.5 update =  [[-0.02530645]\n"," [-0.02408192]\n"," [ 0.03538585]]\n","Iter:  114 loss =  0.21651568818331166 learning rate =  0.5 update =  [[-0.02514028]\n"," [-0.02394485]\n"," [ 0.03518641]]\n","Iter:  115 loss =  0.21529752818623682 learning rate =  0.5 update =  [[-0.02497658]\n"," [-0.02380947]\n"," [ 0.03498941]]\n","Iter:  116 loss =  0.21409354772539163 learning rate =  0.5 update =  [[-0.0248153 ]\n"," [-0.02367576]\n"," [ 0.03479482]]\n","Iter:  117 loss =  0.21290348196070197 learning rate =  0.5 update =  [[-0.02465637]\n"," [-0.02354367]\n"," [ 0.03460258]]\n","Iter:  118 loss =  0.21172707299766197 learning rate =  0.5 update =  [[-0.02449974]\n"," [-0.02341318]\n"," [ 0.03441264]]\n","Iter:  119 loss =  0.21056406965447289 learning rate =  0.5 update =  [[-0.02434536]\n"," [-0.02328425]\n"," [ 0.03422497]]\n","Iter:  120 loss =  0.209414227238543 learning rate =  0.5 update =  [[-0.02419318]\n"," [-0.02315685]\n"," [ 0.03403952]]\n","Iter:  121 loss =  0.20827730733191302 learning rate =  0.5 update =  [[-0.02404316]\n"," [-0.02303096]\n"," [ 0.03385624]]\n","Iter:  122 loss =  0.20715307758519924 learning rate =  0.5 update =  [[-0.02389523]\n"," [-0.02290654]\n"," [ 0.03367511]]\n","Iter:  123 loss =  0.20604131151966532 learning rate =  0.5 update =  [[-0.02374937]\n"," [-0.02278357]\n"," [ 0.03349607]]\n","Iter:  124 loss =  0.2049417883370534 learning rate =  0.5 update =  [[-0.02360551]\n"," [-0.02266201]\n"," [ 0.03331909]]\n","Iter:  125 loss =  0.20385429273682348 learning rate =  0.5 update =  [[-0.02346363]\n"," [-0.02254184]\n"," [ 0.03314413]]\n","Iter:  126 loss =  0.20277861474047032 learning rate =  0.5 update =  [[-0.02332366]\n"," [-0.02242304]\n"," [ 0.03297115]]\n","Iter:  127 loss =  0.2017145495226003 learning rate =  0.5 update =  [[-0.02318559]\n"," [-0.02230558]\n"," [ 0.03280013]]\n","Iter:  128 loss =  0.20066189724846856 learning rate =  0.5 update =  [[-0.02304936]\n"," [-0.02218944]\n"," [ 0.03263101]]\n","Iter:  129 loss =  0.19962046291769206 learning rate =  0.5 update =  [[-0.02291493]\n"," [-0.02207458]\n"," [ 0.03246377]]\n","Iter:  130 loss =  0.19859005621386572 learning rate =  0.5 update =  [[-0.02278227]\n"," [-0.02196099]\n"," [ 0.03229838]]\n","Iter:  131 loss =  0.19757049135982593 learning rate =  0.5 update =  [[-0.02265134]\n"," [-0.02184865]\n"," [ 0.0321348 ]]\n","Iter:  132 loss =  0.19656158697831505 learning rate =  0.5 update =  [[-0.0225221 ]\n"," [-0.02173752]\n"," [ 0.031973  ]]\n","Iter:  133 loss =  0.19556316595781403 learning rate =  0.5 update =  [[-0.02239452]\n"," [-0.0216276 ]\n"," [ 0.03181294]]\n","Iter:  134 loss =  0.19457505532332092 learning rate =  0.5 update =  [[-0.02226857]\n"," [-0.02151886]\n"," [ 0.03165461]]\n","Iter:  135 loss =  0.19359708611186535 learning rate =  0.5 update =  [[-0.02214421]\n"," [-0.02141128]\n"," [ 0.03149796]]\n","Iter:  136 loss =  0.192629093252556 learning rate =  0.5 update =  [[-0.02202142]\n"," [-0.02130483]\n"," [ 0.03134298]]\n","Iter:  137 loss =  0.19167091545097154 learning rate =  0.5 update =  [[-0.02190015]\n"," [-0.02119951]\n"," [ 0.03118962]]\n","Iter:  138 loss =  0.19072239507771171 learning rate =  0.5 update =  [[-0.02178038]\n"," [-0.02109528]\n"," [ 0.03103787]]\n","Iter:  139 loss =  0.18978337806093468 learning rate =  0.5 update =  [[-0.02166208]\n"," [-0.02099214]\n"," [ 0.0308877 ]]\n","Iter:  140 loss =  0.18885371378271587 learning rate =  0.5 update =  [[-0.02154521]\n"," [-0.02089006]\n"," [ 0.03073908]]\n","Iter:  141 loss =  0.18793325497906954 learning rate =  0.5 update =  [[-0.02142977]\n"," [-0.02078902]\n"," [ 0.03059198]]\n","Iter:  142 loss =  0.18702185764348395 learning rate =  0.5 update =  [[-0.0213157 ]\n"," [-0.02068902]\n"," [ 0.03044639]]\n","Iter:  143 loss =  0.18611938093382463 learning rate =  0.5 update =  [[-0.02120299]\n"," [-0.02059002]\n"," [ 0.03030226]]\n","Iter:  144 loss =  0.18522568708247134 learning rate =  0.5 update =  [[-0.02109162]\n"," [-0.02049202]\n"," [ 0.03015959]]\n","Iter:  145 loss =  0.18434064130955635 learning rate =  0.5 update =  [[-0.02098155]\n"," [-0.020395  ]\n"," [ 0.03001835]]\n","Iter:  146 loss =  0.18346411173917962 learning rate =  0.5 update =  [[-0.02087277]\n"," [-0.02029893]\n"," [ 0.02987851]]\n","Iter:  147 loss =  0.1825959693184828 learning rate =  0.5 update =  [[-0.02076524]\n"," [-0.02020382]\n"," [ 0.02974006]]\n","Iter:  148 loss =  0.1817360877394672 learning rate =  0.5 update =  [[-0.02065895]\n"," [-0.02010964]\n"," [ 0.02960296]]\n","Iter:  149 loss =  0.1808843433634486 learning rate =  0.5 update =  [[-0.02055387]\n"," [-0.02001638]\n"," [ 0.0294672 ]]\n","Iter:  150 loss =  0.18004061514804431 learning rate =  0.5 update =  [[-0.02044997]\n"," [-0.01992402]\n"," [ 0.02933276]]\n","Iter:  151 loss =  0.179204784576593 learning rate =  0.5 update =  [[-0.02034725]\n"," [-0.01983255]\n"," [ 0.02919962]]\n","Iter:  152 loss =  0.17837673558991457 learning rate =  0.5 update =  [[-0.02024567]\n"," [-0.01974195]\n"," [ 0.02906776]]\n","Iter:  153 loss =  0.17755635452031693 learning rate =  0.5 update =  [[-0.02014522]\n"," [-0.01965222]\n"," [ 0.02893715]]\n","Iter:  154 loss =  0.17674353002776574 learning rate =  0.5 update =  [[-0.02004587]\n"," [-0.01956333]\n"," [ 0.02880778]]\n","Iter:  155 loss =  0.17593815303813298 learning rate =  0.5 update =  [[-0.01994761]\n"," [-0.01947528]\n"," [ 0.02867963]]\n","Iter:  156 loss =  0.17514011668344398 learning rate =  0.5 update =  [[-0.01985042]\n"," [-0.01938805]\n"," [ 0.02855268]]\n","Iter:  157 loss =  0.17434931624405137 learning rate =  0.5 update =  [[-0.01975427]\n"," [-0.01930164]\n"," [ 0.02842692]]\n","Iter:  158 loss =  0.17356564909265776 learning rate =  0.5 update =  [[-0.01965916]\n"," [-0.01921602]\n"," [ 0.02830232]]\n","Iter:  159 loss =  0.17278901464012253 learning rate =  0.5 update =  [[-0.01956505]\n"," [-0.01913119]\n"," [ 0.02817886]]\n","Iter:  160 loss =  0.17201931428298411 learning rate =  0.5 update =  [[-0.01947194]\n"," [-0.01904713]\n"," [ 0.02805654]]\n","Iter:  161 loss =  0.17125645135263426 learning rate =  0.5 update =  [[-0.01937981]\n"," [-0.01896384]\n"," [ 0.02793533]]\n","Iter:  162 loss =  0.17050033106608337 learning rate =  0.5 update =  [[-0.01928864]\n"," [-0.01888131]\n"," [ 0.02781522]]\n","Iter:  163 loss =  0.16975086047825957 learning rate =  0.5 update =  [[-0.01919841]\n"," [-0.01879951]\n"," [ 0.02769619]]\n","Iter:  164 loss =  0.16900794843578354 learning rate =  0.5 update =  [[-0.01910911]\n"," [-0.01871845]\n"," [ 0.02757823]]\n","Iter:  165 loss =  0.16827150553216802 learning rate =  0.5 update =  [[-0.01902072]\n"," [-0.0186381 ]\n"," [ 0.02746132]]\n","Iter:  166 loss =  0.16754144406438792 learning rate =  0.5 update =  [[-0.01893323]\n"," [-0.01855847]\n"," [ 0.02734544]]\n","Iter:  167 loss =  0.16681767799077385 learning rate =  0.5 update =  [[-0.01884662]\n"," [-0.01847954]\n"," [ 0.02723059]]\n","Iter:  168 loss =  0.16610012289018114 learning rate =  0.5 update =  [[-0.01876089]\n"," [-0.0184013 ]\n"," [ 0.02711674]]\n","Iter:  169 loss =  0.16538869592238742 learning rate =  0.5 update =  [[-0.018676  ]\n"," [-0.01832374]\n"," [ 0.02700389]]\n","Iter:  170 loss =  0.16468331578967882 learning rate =  0.5 update =  [[-0.01859196]\n"," [-0.01824685]\n"," [ 0.02689201]]\n","Iter:  171 loss =  0.16398390269957874 learning rate =  0.5 update =  [[-0.01850874]\n"," [-0.01817063]\n"," [ 0.0267811 ]]\n","Iter:  172 loss =  0.16329037832868237 learning rate =  0.5 update =  [[-0.01842634]\n"," [-0.01809505]\n"," [ 0.02667114]]\n","Iter:  173 loss =  0.1626026657875575 learning rate =  0.5 update =  [[-0.01834474]\n"," [-0.01802013]\n"," [ 0.02656212]]\n","Iter:  174 loss =  0.161920689586673 learning rate =  0.5 update =  [[-0.01826392]\n"," [-0.01794584]\n"," [ 0.02645403]]\n","Iter:  175 loss =  0.161244375603323 learning rate =  0.5 update =  [[-0.01818389]\n"," [-0.01787217]\n"," [ 0.02634685]]\n","Iter:  176 loss =  0.16057365104950883 learning rate =  0.5 update =  [[-0.01810461]\n"," [-0.01779913]\n"," [ 0.02624057]]\n","Iter:  177 loss =  0.15990844444074842 learning rate =  0.5 update =  [[-0.01802609]\n"," [-0.0177267 ]\n"," [ 0.02613518]]\n","Iter:  178 loss =  0.15924868556578123 learning rate =  0.5 update =  [[-0.0179483 ]\n"," [-0.01765487]\n"," [ 0.02603067]]\n","Iter:  179 loss =  0.15859430545713754 learning rate =  0.5 update =  [[-0.01787125]\n"," [-0.01758363]\n"," [ 0.02592702]]\n","Iter:  180 loss =  0.1579452363625442 learning rate =  0.5 update =  [[-0.01779491]\n"," [-0.01751298]\n"," [ 0.02582423]]\n","Iter:  181 loss =  0.15730141171713813 learning rate =  0.5 update =  [[-0.01771928]\n"," [-0.01744291]\n"," [ 0.02572227]]\n","Iter:  182 loss =  0.15666276611646057 learning rate =  0.5 update =  [[-0.01764435]\n"," [-0.01737342]\n"," [ 0.02562116]]\n","Iter:  183 loss =  0.15602923529020676 learning rate =  0.5 update =  [[-0.0175701 ]\n"," [-0.01730448]\n"," [ 0.02552086]]\n","Iter:  184 loss =  0.15540075607670562 learning rate =  0.5 update =  [[-0.01749652]\n"," [-0.01723611]\n"," [ 0.02542137]]\n","Iter:  185 loss =  0.15477726639810602 learning rate =  0.5 update =  [[-0.01742362]\n"," [-0.01716828]\n"," [ 0.02532268]]\n","Iter:  186 loss =  0.15415870523624614 learning rate =  0.5 update =  [[-0.01735136]\n"," [-0.017101  ]\n"," [ 0.02522478]]\n","Iter:  187 loss =  0.15354501260918305 learning rate =  0.5 update =  [[-0.01727976]\n"," [-0.01703426]\n"," [ 0.02512767]]\n","Iter:  188 loss =  0.15293612954836328 learning rate =  0.5 update =  [[-0.01720879]\n"," [-0.01696804]\n"," [ 0.02503132]]\n","Iter:  189 loss =  0.15233199807641146 learning rate =  0.5 update =  [[-0.01713844]\n"," [-0.01690235]\n"," [ 0.02493573]]\n","Iter:  190 loss =  0.15173256118551826 learning rate =  0.5 update =  [[-0.01706872]\n"," [-0.01683717]\n"," [ 0.02484089]]\n","Iter:  191 loss =  0.15113776281640842 learning rate =  0.5 update =  [[-0.0169996 ]\n"," [-0.01677251]\n"," [ 0.02474679]]\n","Iter:  192 loss =  0.15054754783787133 learning rate =  0.5 update =  [[-0.01693109]\n"," [-0.01670835]\n"," [ 0.02465343]]\n","Iter:  193 loss =  0.1499618620268341 learning rate =  0.5 update =  [[-0.01686317]\n"," [-0.01664468]\n"," [ 0.02456078]]\n","Iter:  194 loss =  0.1493806520489625 learning rate =  0.5 update =  [[-0.01679583]\n"," [-0.01658151]\n"," [ 0.02446886]]\n","Iter:  195 loss =  0.1488038654397725 learning rate =  0.5 update =  [[-0.01672906]\n"," [-0.01651882]\n"," [ 0.02437763]]\n","Iter:  196 loss =  0.14823145058623516 learning rate =  0.5 update =  [[-0.01666287]\n"," [-0.01645662]\n"," [ 0.02428711]]\n","Iter:  197 loss =  0.14766335670886213 learning rate =  0.5 update =  [[-0.01659723]\n"," [-0.01639488]\n"," [ 0.02419727]]\n","Iter:  198 loss =  0.14709953384425456 learning rate =  0.5 update =  [[-0.01653215]\n"," [-0.01633362]\n"," [ 0.02410811]]\n","Iter:  199 loss =  0.14653993282810257 learning rate =  0.5 update =  [[-0.01646761]\n"," [-0.01627282]\n"," [ 0.02401962]]\n","Iter:  200 loss =  0.1459845052786215 learning rate =  0.5 update =  [[-0.01640361]\n"," [-0.01621247]\n"," [ 0.0239318 ]]\n","Iter:  201 loss =  0.1454332035804102 learning rate =  0.5 update =  [[-0.01634014]\n"," [-0.01615258]\n"," [ 0.02384463]]\n","Iter:  202 loss =  0.14488598086872093 learning rate =  0.5 update =  [[-0.01627719]\n"," [-0.01609314]\n"," [ 0.02375811]]\n","Iter:  203 loss =  0.14434279101412706 learning rate =  0.5 update =  [[-0.01621476]\n"," [-0.01603414]\n"," [ 0.02367224]]\n","Iter:  204 loss =  0.1438035886075758 learning rate =  0.5 update =  [[-0.01615284]\n"," [-0.01597557]\n"," [ 0.02358699]]\n","Iter:  205 loss =  0.14326832894581656 learning rate =  0.5 update =  [[-0.01609141]\n"," [-0.01591743]\n"," [ 0.02350237]]\n","Iter:  206 loss =  0.1427369680171927 learning rate =  0.5 update =  [[-0.01603049]\n"," [-0.01585973]\n"," [ 0.02341837]]\n","Iter:  207 loss =  0.1422094624877852 learning rate =  0.5 update =  [[-0.01597005]\n"," [-0.01580244]\n"," [ 0.02333498]]\n","Iter:  208 loss =  0.1416857696878994 learning rate =  0.5 update =  [[-0.0159101 ]\n"," [-0.01574557]\n"," [ 0.02325219]]\n","Iter:  209 loss =  0.14116584759888362 learning rate =  0.5 update =  [[-0.01585062]\n"," [-0.01568911]\n"," [ 0.02317   ]]\n","Iter:  210 loss =  0.14064965484027037 learning rate =  0.5 update =  [[-0.01579161]\n"," [-0.01563306]\n"," [ 0.0230884 ]]\n","Iter:  211 loss =  0.14013715065723092 learning rate =  0.5 update =  [[-0.01573307]\n"," [-0.01557741]\n"," [ 0.02300739]]\n","Iter:  212 loss =  0.1396282949083344 learning rate =  0.5 update =  [[-0.01567498]\n"," [-0.01552216]\n"," [ 0.02292695]]\n","Iter:  213 loss =  0.1391230480536013 learning rate =  0.5 update =  [[-0.01561735]\n"," [-0.01546731]\n"," [ 0.02284708]]\n","Iter:  214 loss =  0.13862137114284515 learning rate =  0.5 update =  [[-0.01556017]\n"," [-0.01541284]\n"," [ 0.02276778]]\n","Iter:  215 loss =  0.1381232258042915 learning rate =  0.5 update =  [[-0.01550342]\n"," [-0.01535876]\n"," [ 0.02268904]]\n","Iter:  216 loss =  0.13762857423347 learning rate =  0.5 update =  [[-0.01544711]\n"," [-0.01530506]\n"," [ 0.02261085]]\n","Iter:  217 loss =  0.13713737918236704 learning rate =  0.5 update =  [[-0.01539124]\n"," [-0.01525174]\n"," [ 0.0225332 ]]\n","Iter:  218 loss =  0.13664960394883613 learning rate =  0.5 update =  [[-0.01533578]\n"," [-0.01519879]\n"," [ 0.02245609]]\n","Iter:  219 loss =  0.13616521236625634 learning rate =  0.5 update =  [[-0.01528075]\n"," [-0.01514621]\n"," [ 0.02237952]]\n","Iter:  220 loss =  0.13568416879343215 learning rate =  0.5 update =  [[-0.01522613]\n"," [-0.01509399]\n"," [ 0.02230347]]\n","Iter:  221 loss =  0.13520643810472846 learning rate =  0.5 update =  [[-0.01517192]\n"," [-0.01504214]\n"," [ 0.02222795]]\n","Iter:  222 loss =  0.13473198568043465 learning rate =  0.5 update =  [[-0.01511812]\n"," [-0.01499064]\n"," [ 0.02215295]]\n","Iter:  223 loss =  0.13426077739734993 learning rate =  0.5 update =  [[-0.01506471]\n"," [-0.01493949]\n"," [ 0.02207845]]\n","Iter:  224 loss =  0.13379277961958524 learning rate =  0.5 update =  [[-0.0150117 ]\n"," [-0.0148887 ]\n"," [ 0.02200446]]\n","Iter:  225 loss =  0.1333279591895757 learning rate =  0.5 update =  [[-0.01495908]\n"," [-0.01483825]\n"," [ 0.02193097]]\n","Iter:  226 loss =  0.13286628341929727 learning rate =  0.5 update =  [[-0.01490685]\n"," [-0.01478814]\n"," [ 0.02185798]]\n","Iter:  227 loss =  0.132407720081682 learning rate =  0.5 update =  [[-0.01485499]\n"," [-0.01473837]\n"," [ 0.02178548]]\n","Iter:  228 loss =  0.13195223740222747 learning rate =  0.5 update =  [[-0.01480352]\n"," [-0.01468893]\n"," [ 0.02171346]]\n","Iter:  229 loss =  0.13149980405079495 learning rate =  0.5 update =  [[-0.01475241]\n"," [-0.01463983]\n"," [ 0.02164192]]\n","Iter:  230 loss =  0.1310503891335898 learning rate =  0.5 update =  [[-0.01470168]\n"," [-0.01459106]\n"," [ 0.02157085]]\n","Iter:  231 loss =  0.1306039621853211 learning rate =  0.5 update =  [[-0.0146513 ]\n"," [-0.01454261]\n"," [ 0.02150026]]\n","Iter:  232 loss =  0.1301604931615357 learning rate =  0.5 update =  [[-0.01460129]\n"," [-0.01449448]\n"," [ 0.02143013]]\n","Iter:  233 loss =  0.12971995243112053 learning rate =  0.5 update =  [[-0.01455163]\n"," [-0.01444667]\n"," [ 0.02136046]]\n","Iter:  234 loss =  0.12928231076896946 learning rate =  0.5 update =  [[-0.01450232]\n"," [-0.01439917]\n"," [ 0.02129124]]\n","Iter:  235 loss =  0.1288475393488118 learning rate =  0.5 update =  [[-0.01445336]\n"," [-0.01435198]\n"," [ 0.02122248]]\n","Iter:  236 loss =  0.12841560973619626 learning rate =  0.5 update =  [[-0.01440474]\n"," [-0.01430511]\n"," [ 0.02115416]]\n","Iter:  237 loss =  0.127986493881627 learning rate =  0.5 update =  [[-0.01435647]\n"," [-0.01425854]\n"," [ 0.02108628]]\n","Iter:  238 loss =  0.12756016411384877 learning rate =  0.5 update =  [[-0.01430852]\n"," [-0.01421227]\n"," [ 0.02101884]]\n","Iter:  239 loss =  0.1271365931332755 learning rate =  0.5 update =  [[-0.01426091]\n"," [-0.0141663 ]\n"," [ 0.02095184]]\n","Iter:  240 loss =  0.1267157540055607 learning rate =  0.5 update =  [[-0.01421363]\n"," [-0.01412063]\n"," [ 0.02088526]]\n","Iter:  241 loss =  0.12629762015530444 learning rate =  0.5 update =  [[-0.01416667]\n"," [-0.01407525]\n"," [ 0.0208191 ]]\n","Iter:  242 loss =  0.1258821653598937 learning rate =  0.5 update =  [[-0.01412003]\n"," [-0.01403016]\n"," [ 0.02075337]]\n","Iter:  243 loss =  0.12546936374347434 learning rate =  0.5 update =  [[-0.01407371]\n"," [-0.01398536]\n"," [ 0.02068805]]\n","Iter:  244 loss =  0.12505918977104874 learning rate =  0.5 update =  [[-0.01402771]\n"," [-0.01394084]\n"," [ 0.02062314]]\n","Iter:  245 loss =  0.12465161824269783 learning rate =  0.5 update =  [[-0.01398201]\n"," [-0.0138966 ]\n"," [ 0.02055864]]\n","Iter:  246 loss =  0.12424662428792443 learning rate =  0.5 update =  [[-0.01393663]\n"," [-0.01385265]\n"," [ 0.02049455]]\n","Iter:  247 loss =  0.12384418336011299 learning rate =  0.5 update =  [[-0.01389154]\n"," [-0.01380897]\n"," [ 0.02043085]]\n","Iter:  248 loss =  0.1234442712311058 learning rate =  0.5 update =  [[-0.01384676]\n"," [-0.01376556]\n"," [ 0.02036755]]\n","Iter:  249 loss =  0.12304686398588988 learning rate =  0.5 update =  [[-0.01380227]\n"," [-0.01372243]\n"," [ 0.02030465]]\n","Iter:  250 loss =  0.12265193801739388 learning rate =  0.5 update =  [[-0.01375808]\n"," [-0.01367956]\n"," [ 0.02024213]]\n","Iter:  251 loss =  0.1222594700213913 learning rate =  0.5 update =  [[-0.01371418]\n"," [-0.01363697]\n"," [ 0.02017999]]\n","Iter:  252 loss =  0.12186943699150762 learning rate =  0.5 update =  [[-0.01367057]\n"," [-0.01359463]\n"," [ 0.02011824]]\n","Iter:  253 loss =  0.1214818162143288 learning rate =  0.5 update =  [[-0.01362724]\n"," [-0.01355255]\n"," [ 0.02005686]]\n","Iter:  254 loss =  0.12109658526460927 learning rate =  0.5 update =  [[-0.01358419]\n"," [-0.01351074]\n"," [ 0.01999586]]\n","Iter:  255 loss =  0.12071372200057585 learning rate =  0.5 update =  [[-0.01354143]\n"," [-0.01346918]\n"," [ 0.01993522]]\n","Iter:  256 loss =  0.12033320455932656 learning rate =  0.5 update =  [[-0.01349894]\n"," [-0.01342787]\n"," [ 0.01987496]]\n","Iter:  257 loss =  0.11995501135232088 learning rate =  0.5 update =  [[-0.01345672]\n"," [-0.01338682]\n"," [ 0.01981505]]\n","Iter:  258 loss =  0.11957912106096037 learning rate =  0.5 update =  [[-0.01341477]\n"," [-0.01334601]\n"," [ 0.01975551]]\n","Iter:  259 loss =  0.11920551263225657 learning rate =  0.5 update =  [[-0.01337309]\n"," [-0.01330545]\n"," [ 0.01969632]]\n","Iter:  260 loss =  0.11883416527458501 learning rate =  0.5 update =  [[-0.01333168]\n"," [-0.01326514]\n"," [ 0.01963749]]\n","Iter:  261 loss =  0.11846505845352226 learning rate =  0.5 update =  [[-0.01329053]\n"," [-0.01322506]\n"," [ 0.01957901]]\n","Iter:  262 loss =  0.11809817188776506 learning rate =  0.5 update =  [[-0.01324964]\n"," [-0.01318523]\n"," [ 0.01952087]]\n","Iter:  263 loss =  0.11773348554512905 learning rate =  0.5 update =  [[-0.01320901]\n"," [-0.01314564]\n"," [ 0.01946308]]\n","Iter:  264 loss =  0.11737097963862536 learning rate =  0.5 update =  [[-0.01316863]\n"," [-0.01310628]\n"," [ 0.01940562]]\n","Iter:  265 loss =  0.11701063462261316 learning rate =  0.5 update =  [[-0.0131285 ]\n"," [-0.01306715]\n"," [ 0.01934851]]\n","Iter:  266 loss =  0.11665243118902725 learning rate =  0.5 update =  [[-0.01308862]\n"," [-0.01302825]\n"," [ 0.01929173]]\n","Iter:  267 loss =  0.11629635026367668 learning rate =  0.5 update =  [[-0.01304899]\n"," [-0.01298959]\n"," [ 0.01923528]]\n","Iter:  268 loss =  0.1159423730026165 learning rate =  0.5 update =  [[-0.01300961]\n"," [-0.01295115]\n"," [ 0.01917915]]\n","Iter:  269 loss =  0.11559048078858689 learning rate =  0.5 update =  [[-0.01297046]\n"," [-0.01291294]\n"," [ 0.01912336]]\n","Iter:  270 loss =  0.11524065522752097 learning rate =  0.5 update =  [[-0.01293156]\n"," [-0.01287495]\n"," [ 0.01906788]]\n","Iter:  271 loss =  0.11489287814511877 learning rate =  0.5 update =  [[-0.01289289]\n"," [-0.01283718]\n"," [ 0.01901273]]\n","Iter:  272 loss =  0.1145471315834857 learning rate =  0.5 update =  [[-0.01285446]\n"," [-0.01279963]\n"," [ 0.01895789]]\n","Iter:  273 loss =  0.11420339779783456 learning rate =  0.5 update =  [[-0.01281627]\n"," [-0.01276229]\n"," [ 0.01890337]]\n","Iter:  274 loss =  0.1138616592532489 learning rate =  0.5 update =  [[-0.0127783 ]\n"," [-0.01272518]\n"," [ 0.01884915]]\n","Iter:  275 loss =  0.11352189862150751 learning rate =  0.5 update =  [[-0.01274056]\n"," [-0.01268827]\n"," [ 0.01879525]]\n","Iter:  276 loss =  0.11318409877796753 learning rate =  0.5 update =  [[-0.01270305]\n"," [-0.01265158]\n"," [ 0.01874165]]\n","Iter:  277 loss =  0.1128482427985053 learning rate =  0.5 update =  [[-0.01266577]\n"," [-0.0126151 ]\n"," [ 0.01868835]]\n","Iter:  278 loss =  0.11251431395651434 learning rate =  0.5 update =  [[-0.0126287 ]\n"," [-0.01257882]\n"," [ 0.01863536]]\n","Iter:  279 loss =  0.11218229571995786 learning rate =  0.5 update =  [[-0.01259186]\n"," [-0.01254275]\n"," [ 0.01858266]]\n","Iter:  280 loss =  0.11185217174847566 learning rate =  0.5 update =  [[-0.01255523]\n"," [-0.01250689]\n"," [ 0.01853026]]\n","Iter:  281 loss =  0.11152392589054416 learning rate =  0.5 update =  [[-0.01251882]\n"," [-0.01247123]\n"," [ 0.01847815]]\n","Iter:  282 loss =  0.11119754218068789 learning rate =  0.5 update =  [[-0.01248262]\n"," [-0.01243577]\n"," [ 0.01842633]]\n","Iter:  283 loss =  0.1108730048367415 learning rate =  0.5 update =  [[-0.01244664]\n"," [-0.0124005 ]\n"," [ 0.0183748 ]]\n","Iter:  284 loss =  0.1105502982571615 learning rate =  0.5 update =  [[-0.01241086]\n"," [-0.01236544]\n"," [ 0.01832355]]\n","Iter:  285 loss =  0.11022940701838638 learning rate =  0.5 update =  [[-0.0123753 ]\n"," [-0.01233057]\n"," [ 0.01827259]]\n","Iter:  286 loss =  0.10991031587224397 learning rate =  0.5 update =  [[-0.01233994]\n"," [-0.01229589]\n"," [ 0.0182219 ]]\n","Iter:  287 loss =  0.10959300974340568 learning rate =  0.5 update =  [[-0.01230478]\n"," [-0.01226141]\n"," [ 0.0181715 ]]\n","Iter:  288 loss =  0.1092774737268854 learning rate =  0.5 update =  [[-0.01226983]\n"," [-0.01222712]\n"," [ 0.01812137]]\n","Iter:  289 loss =  0.10896369308558361 learning rate =  0.5 update =  [[-0.01223507]\n"," [-0.01219302]\n"," [ 0.01807151]]\n","Iter:  290 loss =  0.10865165324787443 learning rate =  0.5 update =  [[-0.01220052]\n"," [-0.0121591 ]\n"," [ 0.01802193]]\n","Iter:  291 loss =  0.10834133980523548 learning rate =  0.5 update =  [[-0.01216616]\n"," [-0.01212537]\n"," [ 0.01797261]]\n","Iter:  292 loss =  0.10803273850991951 learning rate =  0.5 update =  [[-0.012132  ]\n"," [-0.01209183]\n"," [ 0.01792356]]\n","Iter:  293 loss =  0.10772583527266638 learning rate =  0.5 update =  [[-0.01209803]\n"," [-0.01205846]\n"," [ 0.01787478]]\n","Iter:  294 loss =  0.10742061616045587 learning rate =  0.5 update =  [[-0.01206426]\n"," [-0.01202528]\n"," [ 0.01782625]]\n","Iter:  295 loss =  0.10711706739429873 learning rate =  0.5 update =  [[-0.01203067]\n"," [-0.01199228]\n"," [ 0.01777799]]\n","Iter:  296 loss =  0.10681517534706651 learning rate =  0.5 update =  [[-0.01199727]\n"," [-0.01195946]\n"," [ 0.01772999]]\n","Iter:  297 loss =  0.10651492654135877 learning rate =  0.5 update =  [[-0.01196406]\n"," [-0.01192681]\n"," [ 0.01768224]]\n","Iter:  298 loss =  0.1062163076474072 learning rate =  0.5 update =  [[-0.01193104]\n"," [-0.01189434]\n"," [ 0.01763474]]\n","Iter:  299 loss =  0.10591930548101564 learning rate =  0.5 update =  [[-0.01189819]\n"," [-0.01186205]\n"," [ 0.0175875 ]]\n","Iter:  300 loss =  0.10562390700153498 learning rate =  0.5 update =  [[-0.01186553]\n"," [-0.01182993]\n"," [ 0.01754051]]\n","Iter:  301 loss =  0.10533009930987378 learning rate =  0.5 update =  [[-0.01183305]\n"," [-0.01179797]\n"," [ 0.01749376]]\n","Iter:  302 loss =  0.105037869646541 learning rate =  0.5 update =  [[-0.01180075]\n"," [-0.01176619]\n"," [ 0.01744726]]\n","Iter:  303 loss =  0.10474720538972337 learning rate =  0.5 update =  [[-0.01176863]\n"," [-0.01173458]\n"," [ 0.017401  ]]\n","Iter:  304 loss =  0.10445809405339461 learning rate =  0.5 update =  [[-0.01173668]\n"," [-0.01170313]\n"," [ 0.01735499]]\n","Iter:  305 loss =  0.1041705232854564 learning rate =  0.5 update =  [[-0.01170491]\n"," [-0.01167185]\n"," [ 0.01730921]]\n","Iter:  306 loss =  0.10388448086591093 learning rate =  0.5 update =  [[-0.01167331]\n"," [-0.01164074]\n"," [ 0.01726367]]\n","Iter:  307 loss =  0.10359995470506353 learning rate =  0.5 update =  [[-0.01164188]\n"," [-0.01160978]\n"," [ 0.01721837]]\n","Iter:  308 loss =  0.10331693284175589 learning rate =  0.5 update =  [[-0.01161062]\n"," [-0.01157899]\n"," [ 0.01717331]]\n","Iter:  309 loss =  0.10303540344162782 learning rate =  0.5 update =  [[-0.01157953]\n"," [-0.01154836]\n"," [ 0.01712847]]\n","Iter:  310 loss =  0.10275535479540873 learning rate =  0.5 update =  [[-0.0115486 ]\n"," [-0.01151789]\n"," [ 0.01708387]]\n","Iter:  311 loss =  0.10247677531723653 learning rate =  0.5 update =  [[-0.01151784]\n"," [-0.01148758]\n"," [ 0.01703949]]\n","Iter:  312 loss =  0.10219965354300481 learning rate =  0.5 update =  [[-0.01148725]\n"," [-0.01145742]\n"," [ 0.01699534]]\n","Iter:  313 loss =  0.10192397812873658 learning rate =  0.5 update =  [[-0.01145682]\n"," [-0.01142742]\n"," [ 0.01695142]]\n","Iter:  314 loss =  0.10164973784898498 learning rate =  0.5 update =  [[-0.01142655]\n"," [-0.01139757]\n"," [ 0.01690772]]\n","Iter:  315 loss =  0.10137692159526014 learning rate =  0.5 update =  [[-0.01139644]\n"," [-0.01136788]\n"," [ 0.01686424]]\n","Iter:  316 loss =  0.10110551837448128 learning rate =  0.5 update =  [[-0.01136649]\n"," [-0.01133834]\n"," [ 0.01682098]]\n","Iter:  317 loss =  0.10083551730745405 learning rate =  0.5 update =  [[-0.01133669]\n"," [-0.01130895]\n"," [ 0.01677794]]\n","Iter:  318 loss =  0.10056690762737247 learning rate =  0.5 update =  [[-0.01130706]\n"," [-0.01127971]\n"," [ 0.01673512]]\n","Iter:  319 loss =  0.10029967867834501 learning rate =  0.5 update =  [[-0.01127757]\n"," [-0.01125062]\n"," [ 0.01669251]]\n","Iter:  320 loss =  0.10003381991394422 learning rate =  0.5 update =  [[-0.01124824]\n"," [-0.01122167]\n"," [ 0.01665011]]\n","Iter:  321 loss =  0.09976932089577982 learning rate =  0.5 update =  [[-0.01121907]\n"," [-0.01119287]\n"," [ 0.01660793]]\n","Iter:  322 loss =  0.09950617129209416 learning rate =  0.5 update =  [[-0.01119004]\n"," [-0.01116422]\n"," [ 0.01656595]]\n","Iter:  323 loss =  0.0992443608763802 learning rate =  0.5 update =  [[-0.01116117]\n"," [-0.01113571]\n"," [ 0.01652419]]\n","Iter:  324 loss =  0.0989838795260217 learning rate =  0.5 update =  [[-0.01113244]\n"," [-0.01110735]\n"," [ 0.01648263]]\n","Iter:  325 loss =  0.09872471722095433 learning rate =  0.5 update =  [[-0.01110386]\n"," [-0.01107912]\n"," [ 0.01644128]]\n","Iter:  326 loss =  0.09846686404234845 learning rate =  0.5 update =  [[-0.01107543]\n"," [-0.01105104]\n"," [ 0.01640013]]\n","Iter:  327 loss =  0.09821031017131196 learning rate =  0.5 update =  [[-0.01104715]\n"," [-0.01102309]\n"," [ 0.01635918]]\n","Iter:  328 loss =  0.09795504588761414 learning rate =  0.5 update =  [[-0.011019  ]\n"," [-0.01099529]\n"," [ 0.01631843]]\n","Iter:  329 loss =  0.09770106156842881 learning rate =  0.5 update =  [[-0.01099101]\n"," [-0.01096762]\n"," [ 0.01627789]]\n","Iter:  330 loss =  0.0974483476870976 learning rate =  0.5 update =  [[-0.01096315]\n"," [-0.01094009]\n"," [ 0.01623754]]\n","Iter:  331 loss =  0.097196894811912 learning rate =  0.5 update =  [[-0.01093543]\n"," [-0.0109127 ]\n"," [ 0.01619738]]\n","Iter:  332 loss =  0.09694669360491424 learning rate =  0.5 update =  [[-0.01090785]\n"," [-0.01088543]\n"," [ 0.01615743]]\n","Iter:  333 loss =  0.09669773482071732 learning rate =  0.5 update =  [[-0.01088042]\n"," [-0.01085831]\n"," [ 0.01611766]]\n","Iter:  334 loss =  0.09645000930534206 learning rate =  0.5 update =  [[-0.01085312]\n"," [-0.01083131]\n"," [ 0.01607809]]\n","Iter:  335 loss =  0.09620350799507321 learning rate =  0.5 update =  [[-0.01082595]\n"," [-0.01080445]\n"," [ 0.01603871]]\n","Iter:  336 loss =  0.09595822191533221 learning rate =  0.5 update =  [[-0.01079892]\n"," [-0.01077772]\n"," [ 0.01599951]]\n","Iter:  337 loss =  0.09571414217956725 learning rate =  0.5 update =  [[-0.01077203]\n"," [-0.01075111]\n"," [ 0.01596051]]\n","Iter:  338 loss =  0.09547125998816082 learning rate =  0.5 update =  [[-0.01074527]\n"," [-0.01072464]\n"," [ 0.01592169]]\n","Iter:  339 loss =  0.09522956662735298 learning rate =  0.5 update =  [[-0.01071864]\n"," [-0.01069829]\n"," [ 0.01588306]]\n","Iter:  340 loss =  0.09498905346818129 learning rate =  0.5 update =  [[-0.01069214]\n"," [-0.01067207]\n"," [ 0.01584461]]\n","Iter:  341 loss =  0.09474971196543695 learning rate =  0.5 update =  [[-0.01066577]\n"," [-0.01064598]\n"," [ 0.01580634]]\n","Iter:  342 loss =  0.09451153365663617 learning rate =  0.5 update =  [[-0.01063954]\n"," [-0.01062001]\n"," [ 0.01576825]]\n","Iter:  343 loss =  0.09427451016100741 learning rate =  0.5 update =  [[-0.01061343]\n"," [-0.01059417]\n"," [ 0.01573035]]\n","Iter:  344 loss =  0.09403863317849323 learning rate =  0.5 update =  [[-0.01058744]\n"," [-0.01056845]\n"," [ 0.01569262]]\n","Iter:  345 loss =  0.09380389448876739 learning rate =  0.5 update =  [[-0.01056159]\n"," [-0.01054285]\n"," [ 0.01565507]]\n","Iter:  346 loss =  0.0935702859502667 learning rate =  0.5 update =  [[-0.01053586]\n"," [-0.01051737]\n"," [ 0.0156177 ]]\n","Iter:  347 loss =  0.09333779949923691 learning rate =  0.5 update =  [[-0.01051025]\n"," [-0.01049202]\n"," [ 0.0155805 ]]\n","Iter:  348 loss =  0.09310642714879219 learning rate =  0.5 update =  [[-0.01048477]\n"," [-0.01046678]\n"," [ 0.01554347]]\n","Iter:  349 loss =  0.09287616098798993 learning rate =  0.5 update =  [[-0.01045941]\n"," [-0.01044166]\n"," [ 0.01550662]]\n","Iter:  350 loss =  0.09264699318091763 learning rate =  0.5 update =  [[-0.01043417]\n"," [-0.01041666]\n"," [ 0.01546993]]\n","Iter:  351 loss =  0.09241891596579399 learning rate =  0.5 update =  [[-0.01040906]\n"," [-0.01039178]\n"," [ 0.01543342]]\n","Iter:  352 loss =  0.09219192165408291 learning rate =  0.5 update =  [[-0.01038406]\n"," [-0.01036701]\n"," [ 0.01539708]]\n","Iter:  353 loss =  0.09196600262962043 learning rate =  0.5 update =  [[-0.01035918]\n"," [-0.01034236]\n"," [ 0.0153609 ]]\n","Iter:  354 loss =  0.09174115134775418 learning rate =  0.5 update =  [[-0.01033442]\n"," [-0.01031783]\n"," [ 0.01532489]]\n","Iter:  355 loss =  0.09151736033449563 learning rate =  0.5 update =  [[-0.01030978]\n"," [-0.01029341]\n"," [ 0.01528905]]\n","Iter:  356 loss =  0.09129462218568433 learning rate =  0.5 update =  [[-0.01028525]\n"," [-0.0102691 ]\n"," [ 0.01525337]]\n","Iter:  357 loss =  0.09107292956616425 learning rate =  0.5 update =  [[-0.01026084]\n"," [-0.0102449 ]\n"," [ 0.01521785]]\n","Iter:  358 loss =  0.0908522752089722 learning rate =  0.5 update =  [[-0.01023655]\n"," [-0.01022082]\n"," [ 0.0151825 ]]\n","Iter:  359 loss =  0.09063265191453726 learning rate =  0.5 update =  [[-0.01021237]\n"," [-0.01019684]\n"," [ 0.0151473 ]]\n","Iter:  360 loss =  0.09041405254989224 learning rate =  0.5 update =  [[-0.0101883 ]\n"," [-0.01017298]\n"," [ 0.01511227]]\n","Iter:  361 loss =  0.09019647004789635 learning rate =  0.5 update =  [[-0.01016435]\n"," [-0.01014922]\n"," [ 0.01507739]]\n","Iter:  362 loss =  0.0899798974064685 learning rate =  0.5 update =  [[-0.0101405 ]\n"," [-0.01012558]\n"," [ 0.01504267]]\n","Iter:  363 loss =  0.08976432768783196 learning rate =  0.5 update =  [[-0.01011677]\n"," [-0.01010204]\n"," [ 0.01500811]]\n","Iter:  364 loss =  0.08954975401776916 learning rate =  0.5 update =  [[-0.01009315]\n"," [-0.01007861]\n"," [ 0.0149737 ]]\n","Iter:  365 loss =  0.08933616958488766 learning rate =  0.5 update =  [[-0.01006963]\n"," [-0.01005529]\n"," [ 0.01493945]]\n","Iter:  366 loss =  0.08912356763989582 learning rate =  0.5 update =  [[-0.01004623]\n"," [-0.01003207]\n"," [ 0.01490535]]\n","Iter:  367 loss =  0.08891194149488904 learning rate =  0.5 update =  [[-0.01002293]\n"," [-0.01000895]\n"," [ 0.0148714 ]]\n","Iter:  368 loss =  0.08870128452264564 learning rate =  0.5 update =  [[-0.00999974]\n"," [-0.00998594]\n"," [ 0.01483761]]\n","Iter:  369 loss =  0.08849159015593322 learning rate =  0.5 update =  [[-0.00997665]\n"," [-0.00996304]\n"," [ 0.01480396]]\n","Iter:  370 loss =  0.08828285188682354 learning rate =  0.5 update =  [[-0.00995367]\n"," [-0.00994023]\n"," [ 0.01477046]]\n","Iter:  371 loss =  0.08807506326601815 learning rate =  0.5 update =  [[-0.0099308 ]\n"," [-0.00991753]\n"," [ 0.01473711]]\n","Iter:  372 loss =  0.08786821790218269 learning rate =  0.5 update =  [[-0.00990803]\n"," [-0.00989493]\n"," [ 0.01470391]]\n","Iter:  373 loss =  0.08766230946129018 learning rate =  0.5 update =  [[-0.00988536]\n"," [-0.00987243]\n"," [ 0.01467086]]\n","Iter:  374 loss =  0.08745733166597396 learning rate =  0.5 update =  [[-0.0098628 ]\n"," [-0.00985003]\n"," [ 0.01463795]]\n","Iter:  375 loss =  0.087253278294889 learning rate =  0.5 update =  [[-0.00984033]\n"," [-0.00982773]\n"," [ 0.01460518]]\n","Iter:  376 loss =  0.08705014318208262 learning rate =  0.5 update =  [[-0.00981797]\n"," [-0.00980553]\n"," [ 0.01457256]]\n","Iter:  377 loss =  0.08684792021637261 learning rate =  0.5 update =  [[-0.00979571]\n"," [-0.00978343]\n"," [ 0.01454008]]\n","Iter:  378 loss =  0.08664660334073537 learning rate =  0.5 update =  [[-0.00977355]\n"," [-0.00976142]\n"," [ 0.01450774]]\n","Iter:  379 loss =  0.08644618655170089 learning rate =  0.5 update =  [[-0.00975148]\n"," [-0.00973951]\n"," [ 0.01447554]]\n","Iter:  380 loss =  0.08624666389875715 learning rate =  0.5 update =  [[-0.00972952]\n"," [-0.0097177 ]\n"," [ 0.01444348]]\n","Iter:  381 loss =  0.08604802948376178 learning rate =  0.5 update =  [[-0.00970765]\n"," [-0.00969598]\n"," [ 0.01441156]]\n","Iter:  382 loss =  0.08585027746036189 learning rate =  0.5 update =  [[-0.00968588]\n"," [-0.00967436]\n"," [ 0.01437978]]\n","Iter:  383 loss =  0.08565340203342202 learning rate =  0.5 update =  [[-0.00966421]\n"," [-0.00965283]\n"," [ 0.01434814]]\n","Iter:  384 loss =  0.08545739745845912 learning rate =  0.5 update =  [[-0.00964263]\n"," [-0.0096314 ]\n"," [ 0.01431663]]\n","Iter:  385 loss =  0.08526225804108575 learning rate =  0.5 update =  [[-0.00962115]\n"," [-0.00961005]\n"," [ 0.01428525]]\n","Iter:  386 loss =  0.0850679781364603 learning rate =  0.5 update =  [[-0.00959976]\n"," [-0.0095888 ]\n"," [ 0.01425401]]\n","Iter:  387 loss =  0.08487455214874487 learning rate =  0.5 update =  [[-0.00957847]\n"," [-0.00956765]\n"," [ 0.01422291]]\n","Iter:  388 loss =  0.08468197453056965 learning rate =  0.5 update =  [[-0.00955727]\n"," [-0.00954658]\n"," [ 0.01419193]]\n","Iter:  389 loss =  0.08449023978250572 learning rate =  0.5 update =  [[-0.00953616]\n"," [-0.00952561]\n"," [ 0.01416109]]\n","Iter:  390 loss =  0.08429934245254311 learning rate =  0.5 update =  [[-0.00951514]\n"," [-0.00950472]\n"," [ 0.01413038]]\n","Iter:  391 loss =  0.08410927713557742 learning rate =  0.5 update =  [[-0.00949422]\n"," [-0.00948392]\n"," [ 0.0140998 ]]\n","Iter:  392 loss =  0.08392003847290183 learning rate =  0.5 update =  [[-0.00947338]\n"," [-0.00946322]\n"," [ 0.01406935]]\n","Iter:  393 loss =  0.08373162115170671 learning rate =  0.5 update =  [[-0.00945264]\n"," [-0.0094426 ]\n"," [ 0.01403902]]\n","Iter:  394 loss =  0.08354401990458554 learning rate =  0.5 update =  [[-0.00943198]\n"," [-0.00942207]\n"," [ 0.01400883]]\n","Iter:  395 loss =  0.08335722950904678 learning rate =  0.5 update =  [[-0.00941142]\n"," [-0.00940162]\n"," [ 0.01397876]]\n","Iter:  396 loss =  0.08317124478703275 learning rate =  0.5 update =  [[-0.00939094]\n"," [-0.00938127]\n"," [ 0.01394881]]\n","Iter:  397 loss =  0.08298606060444466 learning rate =  0.5 update =  [[-0.00937055]\n"," [-0.009361  ]\n"," [ 0.013919  ]]\n","Iter:  398 loss =  0.08280167187067332 learning rate =  0.5 update =  [[-0.00935025]\n"," [-0.00934081]\n"," [ 0.0138893 ]]\n","Iter:  399 loss =  0.08261807353813674 learning rate =  0.5 update =  [[-0.00933003]\n"," [-0.00932071]\n"," [ 0.01385973]]\n","Iter:  400 loss =  0.08243526060182273 learning rate =  0.5 update =  [[-0.0093099 ]\n"," [-0.00930069]\n"," [ 0.01383028]]\n","Iter:  401 loss =  0.0822532280988382 learning rate =  0.5 update =  [[-0.00928986]\n"," [-0.00928076]\n"," [ 0.01380096]]\n","Iter:  402 loss =  0.0820719711079638 learning rate =  0.5 update =  [[-0.0092699 ]\n"," [-0.00926091]\n"," [ 0.01377176]]\n","Iter:  403 loss =  0.08189148474921451 learning rate =  0.5 update =  [[-0.00925002]\n"," [-0.00924115]\n"," [ 0.01374267]]\n","Iter:  404 loss =  0.08171176418340569 learning rate =  0.5 update =  [[-0.00923023]\n"," [-0.00922146]\n"," [ 0.01371371]]\n","Iter:  405 loss =  0.08153280461172466 learning rate =  0.5 update =  [[-0.00921052]\n"," [-0.00920186]\n"," [ 0.01368486]]\n","Iter:  406 loss =  0.08135460127530789 learning rate =  0.5 update =  [[-0.0091909 ]\n"," [-0.00918234]\n"," [ 0.01365614]]\n","Iter:  407 loss =  0.08117714945482332 learning rate =  0.5 update =  [[-0.00917136]\n"," [-0.0091629 ]\n"," [ 0.01362753]]\n","Iter:  408 loss =  0.08100044447005826 learning rate =  0.5 update =  [[-0.0091519 ]\n"," [-0.00914354]\n"," [ 0.01359904]]\n","Iter:  409 loss =  0.0808244816795125 learning rate =  0.5 update =  [[-0.00913252]\n"," [-0.00912427]\n"," [ 0.01357066]]\n","Iter:  410 loss =  0.08064925647999574 learning rate =  0.5 update =  [[-0.00911322]\n"," [-0.00910507]\n"," [ 0.0135424 ]]\n","Iter:  411 loss =  0.08047476430623142 learning rate =  0.5 update =  [[-0.009094  ]\n"," [-0.00908594]\n"," [ 0.01351426]]\n","Iter:  412 loss =  0.08030100063046468 learning rate =  0.5 update =  [[-0.00907486]\n"," [-0.0090669 ]\n"," [ 0.01348623]]\n","Iter:  413 loss =  0.08012796096207508 learning rate =  0.5 update =  [[-0.0090558 ]\n"," [-0.00904794]\n"," [ 0.01345831]]\n","Iter:  414 loss =  0.07995564084719484 learning rate =  0.5 update =  [[-0.00903682]\n"," [-0.00902905]\n"," [ 0.0134305 ]]\n","Iter:  415 loss =  0.0797840358683313 learning rate =  0.5 update =  [[-0.00901792]\n"," [-0.00901024]\n"," [ 0.01340281]]\n","Iter:  416 loss =  0.07961314164399422 learning rate =  0.5 update =  [[-0.0089991 ]\n"," [-0.00899151]\n"," [ 0.01337523]]\n","Iter:  417 loss =  0.07944295382832767 learning rate =  0.5 update =  [[-0.00898035]\n"," [-0.00897285]\n"," [ 0.01334776]]\n","Iter:  418 loss =  0.07927346811074665 learning rate =  0.5 update =  [[-0.00896168]\n"," [-0.00895427]\n"," [ 0.0133204 ]]\n","Iter:  419 loss =  0.07910468021557812 learning rate =  0.5 update =  [[-0.00894308]\n"," [-0.00893576]\n"," [ 0.01329315]]\n","Iter:  420 loss =  0.07893658590170619 learning rate =  0.5 update =  [[-0.00892456]\n"," [-0.00891733]\n"," [ 0.013266  ]]\n","Iter:  421 loss =  0.07876918096222199 learning rate =  0.5 update =  [[-0.00890612]\n"," [-0.00889897]\n"," [ 0.01323897]]\n","Iter:  422 loss =  0.078602461224078 learning rate =  0.5 update =  [[-0.00888775]\n"," [-0.00888069]\n"," [ 0.01321204]]\n","Iter:  423 loss =  0.07843642254774567 learning rate =  0.5 update =  [[-0.00886946]\n"," [-0.00886248]\n"," [ 0.01318522]]\n","Iter:  424 loss =  0.07827106082687882 learning rate =  0.5 update =  [[-0.00885124]\n"," [-0.00884434]\n"," [ 0.01315851]]\n","Iter:  425 loss =  0.07810637198797932 learning rate =  0.5 update =  [[-0.00883309]\n"," [-0.00882627]\n"," [ 0.0131319 ]]\n","Iter:  426 loss =  0.07794235199006841 learning rate =  0.5 update =  [[-0.00881502]\n"," [-0.00880828]\n"," [ 0.01310539]]\n","Iter:  427 loss =  0.07777899682436076 learning rate =  0.5 update =  [[-0.00879702]\n"," [-0.00879036]\n"," [ 0.01307899]]\n","Iter:  428 loss =  0.07761630251394347 learning rate =  0.5 update =  [[-0.00877909]\n"," [-0.0087725 ]\n"," [ 0.0130527 ]]\n","Iter:  429 loss =  0.07745426511345858 learning rate =  0.5 update =  [[-0.00876124]\n"," [-0.00875472]\n"," [ 0.0130265 ]]\n","Iter:  430 loss =  0.07729288070878879 learning rate =  0.5 update =  [[-0.00874345]\n"," [-0.00873701]\n"," [ 0.01300041]]\n","Iter:  431 loss =  0.07713214541674807 learning rate =  0.5 update =  [[-0.00872574]\n"," [-0.00871938]\n"," [ 0.01297443]]\n","Iter:  432 loss =  0.0769720553847753 learning rate =  0.5 update =  [[-0.00870809]\n"," [-0.0087018 ]\n"," [ 0.01294854]]\n","Iter:  433 loss =  0.07681260679063127 learning rate =  0.5 update =  [[-0.00869052]\n"," [-0.0086843 ]\n"," [ 0.01292275]]\n","Iter:  434 loss =  0.07665379584210046 learning rate =  0.5 update =  [[-0.00867302]\n"," [-0.00866687]\n"," [ 0.01289706]]\n","Iter:  435 loss =  0.07649561877669508 learning rate =  0.5 update =  [[-0.00865558]\n"," [-0.00864951]\n"," [ 0.01287148]]\n","Iter:  436 loss =  0.0763380718613634 learning rate =  0.5 update =  [[-0.00863821]\n"," [-0.00863221]\n"," [ 0.01284599]]\n","Iter:  437 loss =  0.07618115139220127 learning rate =  0.5 update =  [[-0.00862092]\n"," [-0.00861498]\n"," [ 0.0128206 ]]\n","Iter:  438 loss =  0.07602485369416744 learning rate =  0.5 update =  [[-0.00860369]\n"," [-0.00859782]\n"," [ 0.01279531]]\n","Iter:  439 loss =  0.07586917512080121 learning rate =  0.5 update =  [[-0.00858652]\n"," [-0.00858072]\n"," [ 0.01277011]]\n","Iter:  440 loss =  0.07571411205394499 learning rate =  0.5 update =  [[-0.00856943]\n"," [-0.00856369]\n"," [ 0.01274501]]\n","Iter:  441 loss =  0.07555966090346855 learning rate =  0.5 update =  [[-0.0085524 ]\n"," [-0.00854673]\n"," [ 0.01272001]]\n","Iter:  442 loss =  0.07540581810699784 learning rate =  0.5 update =  [[-0.00853544]\n"," [-0.00852983]\n"," [ 0.0126951 ]]\n","Iter:  443 loss =  0.07525258012964565 learning rate =  0.5 update =  [[-0.00851854]\n"," [-0.008513  ]\n"," [ 0.01267029]]\n","Iter:  444 loss =  0.0750999434637469 learning rate =  0.5 update =  [[-0.00850171]\n"," [-0.00849623]\n"," [ 0.01264557]]\n","Iter:  445 loss =  0.07494790462859556 learning rate =  0.5 update =  [[-0.00848495]\n"," [-0.00847953]\n"," [ 0.01262095]]\n","Iter:  446 loss =  0.0747964601701859 learning rate =  0.5 update =  [[-0.00846825]\n"," [-0.00846289]\n"," [ 0.01259642]]\n","Iter:  447 loss =  0.0746456066609558 learning rate =  0.5 update =  [[-0.00845161]\n"," [-0.00844632]\n"," [ 0.01257198]]\n","Iter:  448 loss =  0.07449534069953344 learning rate =  0.5 update =  [[-0.00843504]\n"," [-0.0084298 ]\n"," [ 0.01254764]]\n","Iter:  449 loss =  0.07434565891048715 learning rate =  0.5 update =  [[-0.00841853]\n"," [-0.00841336]\n"," [ 0.01252339]]\n","Iter:  450 loss =  0.07419655794407773 learning rate =  0.5 update =  [[-0.00840209]\n"," [-0.00839697]\n"," [ 0.01249922]]\n","Iter:  451 loss =  0.07404803447601352 learning rate =  0.5 update =  [[-0.00838571]\n"," [-0.00838064]\n"," [ 0.01247515]]\n","Iter:  452 loss =  0.07390008520720902 learning rate =  0.5 update =  [[-0.00836939]\n"," [-0.00836438]\n"," [ 0.01245117]]\n","Iter:  453 loss =  0.07375270686354553 learning rate =  0.5 update =  [[-0.00835313]\n"," [-0.00834818]\n"," [ 0.01242728]]\n","Iter:  454 loss =  0.0736058961956347 learning rate =  0.5 update =  [[-0.00833694]\n"," [-0.00833204]\n"," [ 0.01240348]]\n","Iter:  455 loss =  0.07345964997858526 learning rate =  0.5 update =  [[-0.00832081]\n"," [-0.00831596]\n"," [ 0.01237977]]\n","Iter:  456 loss =  0.07331396501177159 learning rate =  0.5 update =  [[-0.00830473]\n"," [-0.00829995]\n"," [ 0.01235614]]\n","Iter:  457 loss =  0.07316883811860574 learning rate =  0.5 update =  [[-0.00828872]\n"," [-0.00828399]\n"," [ 0.01233261]]\n","Iter:  458 loss =  0.07302426614631163 learning rate =  0.5 update =  [[-0.00827277]\n"," [-0.00826809]\n"," [ 0.01230916]]\n","Iter:  459 loss =  0.07288024596570136 learning rate =  0.5 update =  [[-0.00825688]\n"," [-0.00825225]\n"," [ 0.0122858 ]]\n","Iter:  460 loss =  0.07273677447095545 learning rate =  0.5 update =  [[-0.00824105]\n"," [-0.00823647]\n"," [ 0.01226252]]\n","Iter:  461 loss =  0.07259384857940365 learning rate =  0.5 update =  [[-0.00822528]\n"," [-0.00822075]\n"," [ 0.01223933]]\n","Iter:  462 loss =  0.07245146523131006 learning rate =  0.5 update =  [[-0.00820957]\n"," [-0.00820509]\n"," [ 0.01221622]]\n","Iter:  463 loss =  0.07230962138965917 learning rate =  0.5 update =  [[-0.00819392]\n"," [-0.00818948]\n"," [ 0.01219321]]\n","Iter:  464 loss =  0.07216831403994542 learning rate =  0.5 update =  [[-0.00817832]\n"," [-0.00817394]\n"," [ 0.01217027]]\n","Iter:  465 loss =  0.07202754018996449 learning rate =  0.5 update =  [[-0.00816279]\n"," [-0.00815845]\n"," [ 0.01214742]]\n","Iter:  466 loss =  0.07188729686960683 learning rate =  0.5 update =  [[-0.00814731]\n"," [-0.00814302]\n"," [ 0.01212465]]\n","Iter:  467 loss =  0.07174758113065427 learning rate =  0.5 update =  [[-0.00813189]\n"," [-0.00812764]\n"," [ 0.01210197]]\n","Iter:  468 loss =  0.07160839004657801 learning rate =  0.5 update =  [[-0.00811652]\n"," [-0.00811233]\n"," [ 0.01207936]]\n","Iter:  469 loss =  0.07146972071233917 learning rate =  0.5 update =  [[-0.00810121]\n"," [-0.00809706]\n"," [ 0.01205684]]\n","Iter:  470 loss =  0.07133157024419175 learning rate =  0.5 update =  [[-0.00808596]\n"," [-0.00808186]\n"," [ 0.01203441]]\n","Iter:  471 loss =  0.07119393577948765 learning rate =  0.5 update =  [[-0.00807077]\n"," [-0.00806671]\n"," [ 0.01201205]]\n","Iter:  472 loss =  0.07105681447648368 learning rate =  0.5 update =  [[-0.00805563]\n"," [-0.00805161]\n"," [ 0.01198977]]\n","Iter:  473 loss =  0.07092020351415076 learning rate =  0.5 update =  [[-0.00804055]\n"," [-0.00803657]\n"," [ 0.01196758]]\n","Iter:  474 loss =  0.07078410009198574 learning rate =  0.5 update =  [[-0.00802552]\n"," [-0.00802159]\n"," [ 0.01194546]]\n","Iter:  475 loss =  0.07064850142982408 learning rate =  0.5 update =  [[-0.00801055]\n"," [-0.00800666]\n"," [ 0.01192343]]\n","Iter:  476 loss =  0.070513404767656 learning rate =  0.5 update =  [[-0.00799563]\n"," [-0.00799178]\n"," [ 0.01190147]]\n","Iter:  477 loss =  0.07037880736544348 learning rate =  0.5 update =  [[-0.00798077]\n"," [-0.00797696]\n"," [ 0.01187959]]\n","Iter:  478 loss =  0.07024470650294017 learning rate =  0.5 update =  [[-0.00796596]\n"," [-0.00796219]\n"," [ 0.01185779]]\n","Iter:  479 loss =  0.0701110994795125 learning rate =  0.5 update =  [[-0.0079512 ]\n"," [-0.00794748]\n"," [ 0.01183607]]\n","Iter:  480 loss =  0.06997798361396337 learning rate =  0.5 update =  [[-0.0079365 ]\n"," [-0.00793282]\n"," [ 0.01181443]]\n","Iter:  481 loss =  0.06984535624435721 learning rate =  0.5 update =  [[-0.00792185]\n"," [-0.00791821]\n"," [ 0.01179286]]\n","Iter:  482 loss =  0.06971321472784714 learning rate =  0.5 update =  [[-0.00790726]\n"," [-0.00790365]\n"," [ 0.01177137]]\n","Iter:  483 loss =  0.06958155644050439 learning rate =  0.5 update =  [[-0.00789271]\n"," [-0.00788914]\n"," [ 0.01174995]]\n","Iter:  484 loss =  0.06945037877714878 learning rate =  0.5 update =  [[-0.00787822]\n"," [-0.00787469]\n"," [ 0.01172862]]\n","Iter:  485 loss =  0.06931967915118187 learning rate =  0.5 update =  [[-0.00786379]\n"," [-0.00786029]\n"," [ 0.01170735]]\n","Iter:  486 loss =  0.06918945499442107 learning rate =  0.5 update =  [[-0.0078494 ]\n"," [-0.00784594]\n"," [ 0.01168616]]\n","Iter:  487 loss =  0.06905970375693632 learning rate =  0.5 update =  [[-0.00783506]\n"," [-0.00783164]\n"," [ 0.01166505]]\n","Iter:  488 loss =  0.06893042290688783 learning rate =  0.5 update =  [[-0.00782078]\n"," [-0.00781739]\n"," [ 0.01164401]]\n","Iter:  489 loss =  0.0688016099303661 learning rate =  0.5 update =  [[-0.00780655]\n"," [-0.0078032 ]\n"," [ 0.01162305]]\n","Iter:  490 loss =  0.06867326233123339 learning rate =  0.5 update =  [[-0.00779236]\n"," [-0.00778905]\n"," [ 0.01160215]]\n","Iter:  491 loss =  0.06854537763096655 learning rate =  0.5 update =  [[-0.00777823]\n"," [-0.00777495]\n"," [ 0.01158134]]\n","Iter:  492 loss =  0.06841795336850266 learning rate =  0.5 update =  [[-0.00776415]\n"," [-0.0077609 ]\n"," [ 0.01156059]]\n","Iter:  493 loss =  0.0682909871000845 learning rate =  0.5 update =  [[-0.00775012]\n"," [-0.00774691]\n"," [ 0.01153992]]\n","Iter:  494 loss =  0.0681644763991095 learning rate =  0.5 update =  [[-0.00773614]\n"," [-0.00773296]\n"," [ 0.01151932]]\n","Iter:  495 loss =  0.06803841885597908 learning rate =  0.5 update =  [[-0.0077222 ]\n"," [-0.00771906]\n"," [ 0.01149879]]\n","Iter:  496 loss =  0.06791281207795012 learning rate =  0.5 update =  [[-0.00770832]\n"," [-0.00770521]\n"," [ 0.01147833]]\n","Iter:  497 loss =  0.0677876536889874 learning rate =  0.5 update =  [[-0.00769448]\n"," [-0.0076914 ]\n"," [ 0.01145794]]\n","Iter:  498 loss =  0.06766294132961895 learning rate =  0.5 update =  [[-0.0076807 ]\n"," [-0.00767765]\n"," [ 0.01143762]]\n","Iter:  499 loss =  0.06753867265679081 learning rate =  0.5 update =  [[-0.00766696]\n"," [-0.00766394]\n"," [ 0.01141737]]\n","Iter:  500 loss =  0.06741484534372552 learning rate =  0.5 update =  [[-0.00765327]\n"," [-0.00765028]\n"," [ 0.0113972 ]]\n","Iter:  501 loss =  0.06729145707978042 learning rate =  0.5 update =  [[-0.00763963]\n"," [-0.00763667]\n"," [ 0.01137709]]\n","Iter:  502 loss =  0.06716850557030828 learning rate =  0.5 update =  [[-0.00762603]\n"," [-0.00762311]\n"," [ 0.01135705]]\n","Iter:  503 loss =  0.06704598853651922 learning rate =  0.5 update =  [[-0.00761249]\n"," [-0.00760959]\n"," [ 0.01133708]]\n","Iter:  504 loss =  0.06692390371534401 learning rate =  0.5 update =  [[-0.00759899]\n"," [-0.00759612]\n"," [ 0.01131718]]\n","Iter:  505 loss =  0.06680224885929845 learning rate =  0.5 update =  [[-0.00758553]\n"," [-0.0075827 ]\n"," [ 0.01129734]]\n","Iter:  506 loss =  0.06668102173635013 learning rate =  0.5 update =  [[-0.00757213]\n"," [-0.00756932]\n"," [ 0.01127758]]\n","Iter:  507 loss =  0.06656022012978535 learning rate =  0.5 update =  [[-0.00755876]\n"," [-0.00755599]\n"," [ 0.01125788]]\n","Iter:  508 loss =  0.06643984183807834 learning rate =  0.5 update =  [[-0.00754545]\n"," [-0.0075427 ]\n"," [ 0.01123825]]\n","Iter:  509 loss =  0.06631988467476121 learning rate =  0.5 update =  [[-0.00753218]\n"," [-0.00752946]\n"," [ 0.01121868]]\n","Iter:  510 loss =  0.06620034646829585 learning rate =  0.5 update =  [[-0.00751896]\n"," [-0.00751626]\n"," [ 0.01119918]]\n","Iter:  511 loss =  0.06608122506194647 learning rate =  0.5 update =  [[-0.00750578]\n"," [-0.00750311]\n"," [ 0.01117975]]\n","Iter:  512 loss =  0.06596251831365381 learning rate =  0.5 update =  [[-0.00749265]\n"," [-0.00749001]\n"," [ 0.01116038]]\n","Iter:  513 loss =  0.06584422409591104 learning rate =  0.5 update =  [[-0.00747956]\n"," [-0.00747695]\n"," [ 0.01114108]]\n","Iter:  514 loss =  0.06572634029563981 learning rate =  0.5 update =  [[-0.00746652]\n"," [-0.00746393]\n"," [ 0.01112184]]\n","Iter:  515 loss =  0.06560886481406883 learning rate =  0.5 update =  [[-0.00745352]\n"," [-0.00745096]\n"," [ 0.01110267]]\n","Iter:  516 loss =  0.06549179556661261 learning rate =  0.5 update =  [[-0.00744057]\n"," [-0.00743803]\n"," [ 0.01108356]]\n","Iter:  517 loss =  0.06537513048275245 learning rate =  0.5 update =  [[-0.00742766]\n"," [-0.00742515]\n"," [ 0.01106452]]\n","Iter:  518 loss =  0.06525886750591757 learning rate =  0.5 update =  [[-0.00741479]\n"," [-0.00741231]\n"," [ 0.01104554]]\n","Iter:  519 loss =  0.06514300459336833 learning rate =  0.5 update =  [[-0.00740197]\n"," [-0.00739951]\n"," [ 0.01102662]]\n","Iter:  520 loss =  0.06502753971608002 learning rate =  0.5 update =  [[-0.00738919]\n"," [-0.00738675]\n"," [ 0.01100777]]\n","Iter:  521 loss =  0.06491247085862858 learning rate =  0.5 update =  [[-0.00737645]\n"," [-0.00737404]\n"," [ 0.01098898]]\n","Iter:  522 loss =  0.06479779601907601 learning rate =  0.5 update =  [[-0.00736376]\n"," [-0.00736137]\n"," [ 0.01097025]]\n","Iter:  523 loss =  0.0646835132088591 learning rate =  0.5 update =  [[-0.00735111]\n"," [-0.00734875]\n"," [ 0.01095158]]\n","Iter:  524 loss =  0.06456962045267695 learning rate =  0.5 update =  [[-0.0073385 ]\n"," [-0.00733616]\n"," [ 0.01093298]]\n","Iter:  525 loss =  0.06445611578838148 learning rate =  0.5 update =  [[-0.00732594]\n"," [-0.00732362]\n"," [ 0.01091443]]\n","Iter:  526 loss =  0.06434299726686786 learning rate =  0.5 update =  [[-0.00731341]\n"," [-0.00731112]\n"," [ 0.01089595]]\n","Iter:  527 loss =  0.06423026295196685 learning rate =  0.5 update =  [[-0.00730093]\n"," [-0.00729866]\n"," [ 0.01087753]]\n","Iter:  528 loss =  0.06411791092033726 learning rate =  0.5 update =  [[-0.00728849]\n"," [-0.00728624]\n"," [ 0.01085917]]\n","Iter:  529 loss =  0.06400593926136056 learning rate =  0.5 update =  [[-0.00727609]\n"," [-0.00727387]\n"," [ 0.01084087]]\n","Iter:  530 loss =  0.06389434607703569 learning rate =  0.5 update =  [[-0.00726374]\n"," [-0.00726153]\n"," [ 0.01082263]]\n","Iter:  531 loss =  0.06378312948187542 learning rate =  0.5 update =  [[-0.00725142]\n"," [-0.00724924]\n"," [ 0.01080445]]\n","Iter:  532 loss =  0.06367228760280341 learning rate =  0.5 update =  [[-0.00723914]\n"," [-0.00723698]\n"," [ 0.01078633]]\n","Iter:  533 loss =  0.06356181857905252 learning rate =  0.5 update =  [[-0.00722691]\n"," [-0.00722477]\n"," [ 0.01076827]]\n","Iter:  534 loss =  0.06345172056206393 learning rate =  0.5 update =  [[-0.00721472]\n"," [-0.0072126 ]\n"," [ 0.01075026]]\n","Iter:  535 loss =  0.06334199171538751 learning rate =  0.5 update =  [[-0.00720256]\n"," [-0.00720046]\n"," [ 0.01073232]]\n","Iter:  536 loss =  0.06323263021458261 learning rate =  0.5 update =  [[-0.00719045]\n"," [-0.00718837]\n"," [ 0.01071443]]\n","Iter:  537 loss =  0.06312363424712086 learning rate =  0.5 update =  [[-0.00717837]\n"," [-0.00717631]\n"," [ 0.0106966 ]]\n","Iter:  538 loss =  0.06301500201228886 learning rate =  0.5 update =  [[-0.00716634]\n"," [-0.0071643 ]\n"," [ 0.01067883]]\n","Iter:  539 loss =  0.06290673172109194 learning rate =  0.5 update =  [[-0.00715434]\n"," [-0.00715232]\n"," [ 0.01066112]]\n","Iter:  540 loss =  0.06279882159616003 learning rate =  0.5 update =  [[-0.00714239]\n"," [-0.00714039]\n"," [ 0.01064347]]\n","Iter:  541 loss =  0.06269126987165267 learning rate =  0.5 update =  [[-0.00713047]\n"," [-0.00712849]\n"," [ 0.01062587]]\n","Iter:  542 loss =  0.06258407479316655 learning rate =  0.5 update =  [[-0.00711859]\n"," [-0.00711663]\n"," [ 0.01060833]]\n","Iter:  543 loss =  0.062477234617642596 learning rate =  0.5 update =  [[-0.00710675]\n"," [-0.00710481]\n"," [ 0.01059084]]\n","Iter:  544 loss =  0.062370747613275254 learning rate =  0.5 update =  [[-0.00709495]\n"," [-0.00709303]\n"," [ 0.01057341]]\n","Iter:  545 loss =  0.062264612059421624 learning rate =  0.5 update =  [[-0.00708319]\n"," [-0.00708128]\n"," [ 0.01055604]]\n","Iter:  546 loss =  0.0621588262465118 learning rate =  0.5 update =  [[-0.00707146]\n"," [-0.00706958]\n"," [ 0.01053872]]\n","Iter:  547 loss =  0.062053388475960244 learning rate =  0.5 update =  [[-0.00705978]\n"," [-0.00705791]\n"," [ 0.01052146]]\n","Iter:  548 loss =  0.06194829706007813 learning rate =  0.5 update =  [[-0.00704813]\n"," [-0.00704628]\n"," [ 0.01050425]]\n","Iter:  549 loss =  0.061843550321985824 learning rate =  0.5 update =  [[-0.00703652]\n"," [-0.00703468]\n"," [ 0.0104871 ]]\n","Iter:  550 loss =  0.061739146595527 learning rate =  0.5 update =  [[-0.00702494]\n"," [-0.00702313]\n"," [ 0.01047   ]]\n","Iter:  551 loss =  0.061635084225183184 learning rate =  0.5 update =  [[-0.00701341]\n"," [-0.00701161]\n"," [ 0.01045295]]\n","Iter:  552 loss =  0.061531361565989054 learning rate =  0.5 update =  [[-0.00700191]\n"," [-0.00700013]\n"," [ 0.01043596]]\n","Iter:  553 loss =  0.061427976983448984 learning rate =  0.5 update =  [[-0.00699044]\n"," [-0.00698868]\n"," [ 0.01041903]]\n","Iter:  554 loss =  0.06132492885345362 learning rate =  0.5 update =  [[-0.00697902]\n"," [-0.00697727]\n"," [ 0.01040214]]\n","Iter:  555 loss =  0.061222215562198125 learning rate =  0.5 update =  [[-0.00696763]\n"," [-0.0069659 ]\n"," [ 0.01038531]]\n","Iter:  556 loss =  0.06111983550610063 learning rate =  0.5 update =  [[-0.00695627]\n"," [-0.00695456]\n"," [ 0.01036854]]\n","Iter:  557 loss =  0.061017787091721454 learning rate =  0.5 update =  [[-0.00694496]\n"," [-0.00694326]\n"," [ 0.01035181]]\n","Iter:  558 loss =  0.06091606873568338 learning rate =  0.5 update =  [[-0.00693368]\n"," [-0.00693199]\n"," [ 0.01033514]]\n","Iter:  559 loss =  0.06081467886459285 learning rate =  0.5 update =  [[-0.00692243]\n"," [-0.00692076]\n"," [ 0.01031852]]\n","Iter:  560 loss =  0.060713615914960904 learning rate =  0.5 update =  [[-0.00691122]\n"," [-0.00690957]\n"," [ 0.01030196]]\n","Iter:  561 loss =  0.060612878333126105 learning rate =  0.5 update =  [[-0.00690005]\n"," [-0.00689841]\n"," [ 0.01028544]]\n","Iter:  562 loss =  0.06051246457517766 learning rate =  0.5 update =  [[-0.00688891]\n"," [-0.00688729]\n"," [ 0.01026898]]\n","Iter:  563 loss =  0.06041237310687885 learning rate =  0.5 update =  [[-0.0068778 ]\n"," [-0.0068762 ]\n"," [ 0.01025257]]\n","Iter:  564 loss =  0.060312602403592 learning rate =  0.5 update =  [[-0.00686673]\n"," [-0.00686515]\n"," [ 0.0102362 ]]\n","Iter:  565 loss =  0.060213150950203354 learning rate =  0.5 update =  [[-0.0068557 ]\n"," [-0.00685413]\n"," [ 0.01021989]]\n","Iter:  566 loss =  0.06011401724104926 learning rate =  0.5 update =  [[-0.0068447 ]\n"," [-0.00684314]\n"," [ 0.01020363]]\n","Iter:  567 loss =  0.060015199779842925 learning rate =  0.5 update =  [[-0.00683374]\n"," [-0.00683219]\n"," [ 0.01018742]]\n","Iter:  568 loss =  0.05991669707960133 learning rate =  0.5 update =  [[-0.0068228 ]\n"," [-0.00682127]\n"," [ 0.01017127]]\n","Iter:  569 loss =  0.059818507662573656 learning rate =  0.5 update =  [[-0.00681191]\n"," [-0.00681039]\n"," [ 0.01015516]]\n","Iter:  570 loss =  0.059720630060169616 learning rate =  0.5 update =  [[-0.00680105]\n"," [-0.00679954]\n"," [ 0.0101391 ]]\n","Iter:  571 loss =  0.05962306281288901 learning rate =  0.5 update =  [[-0.00679022]\n"," [-0.00678873]\n"," [ 0.01012309]]\n","Iter:  572 loss =  0.05952580447025152 learning rate =  0.5 update =  [[-0.00677942]\n"," [-0.00677795]\n"," [ 0.01010712]]\n","Iter:  573 loss =  0.05942885359072754 learning rate =  0.5 update =  [[-0.00676866]\n"," [-0.0067672 ]\n"," [ 0.01009121]]\n","Iter:  574 loss =  0.05933220874166939 learning rate =  0.5 update =  [[-0.00675793]\n"," [-0.00675649]\n"," [ 0.01007535]]\n","Iter:  575 loss =  0.05923586849924303 learning rate =  0.5 update =  [[-0.00674724]\n"," [-0.0067458 ]\n"," [ 0.01005954]]\n","Iter:  576 loss =  0.059139831448360804 learning rate =  0.5 update =  [[-0.00673658]\n"," [-0.00673515]\n"," [ 0.01004377]]\n","Iter:  577 loss =  0.05904409618261458 learning rate =  0.5 update =  [[-0.00672595]\n"," [-0.00672454]\n"," [ 0.01002805]]\n","Iter:  578 loss =  0.0589486613042093 learning rate =  0.5 update =  [[-0.00671535]\n"," [-0.00671396]\n"," [ 0.01001238]]\n","Iter:  579 loss =  0.05885352542389756 learning rate =  0.5 update =  [[-0.00670479]\n"," [-0.00670341]\n"," [ 0.00999676]]\n","Iter:  580 loss =  0.05875868716091449 learning rate =  0.5 update =  [[-0.00669426]\n"," [-0.00669289]\n"," [ 0.00998118]]\n","Iter:  581 loss =  0.058664145142913314 learning rate =  0.5 update =  [[-0.00668376]\n"," [-0.0066824 ]\n"," [ 0.00996565]]\n","Iter:  582 loss =  0.05856989800590162 learning rate =  0.5 update =  [[-0.00667329]\n"," [-0.00667195]\n"," [ 0.00995017]]\n","Iter:  583 loss =  0.05847594439417784 learning rate =  0.5 update =  [[-0.00666286]\n"," [-0.00666152]\n"," [ 0.00993474]]\n","Iter:  584 loss =  0.05838228296026876 learning rate =  0.5 update =  [[-0.00665245]\n"," [-0.00665113]\n"," [ 0.00991935]]\n","Iter:  585 loss =  0.05828891236486729 learning rate =  0.5 update =  [[-0.00664208]\n"," [-0.00664077]\n"," [ 0.00990401]]\n","Iter:  586 loss =  0.058195831276770985 learning rate =  0.5 update =  [[-0.00663174]\n"," [-0.00663045]\n"," [ 0.00988872]]\n","Iter:  587 loss =  0.05810303837282077 learning rate =  0.5 update =  [[-0.00662144]\n"," [-0.00662015]\n"," [ 0.00987347]]\n","Iter:  588 loss =  0.058010532337840995 learning rate =  0.5 update =  [[-0.00661116]\n"," [-0.00660989]\n"," [ 0.00985826]]\n","Iter:  589 loss =  0.05791831186457913 learning rate =  0.5 update =  [[-0.00660091]\n"," [-0.00659965]\n"," [ 0.00984311]]\n","Iter:  590 loss =  0.05782637565364619 learning rate =  0.5 update =  [[-0.0065907 ]\n"," [-0.00658945]\n"," [ 0.00982799]]\n","Iter:  591 loss =  0.05773472241345858 learning rate =  0.5 update =  [[-0.00658052]\n"," [-0.00657928]\n"," [ 0.00981293]]\n","Iter:  592 loss =  0.057643350860179056 learning rate =  0.5 update =  [[-0.00657036]\n"," [-0.00656913]\n"," [ 0.00979791]]\n","Iter:  593 loss =  0.057552259717659295 learning rate =  0.5 update =  [[-0.00656024]\n"," [-0.00655902]\n"," [ 0.00978293]]\n","Iter:  594 loss =  0.05746144771738283 learning rate =  0.5 update =  [[-0.00655015]\n"," [-0.00654894]\n"," [ 0.009768  ]]\n","Iter:  595 loss =  0.05737091359840778 learning rate =  0.5 update =  [[-0.00654009]\n"," [-0.00653889]\n"," [ 0.00975311]]\n","Iter:  596 loss =  0.057280656107310854 learning rate =  0.5 update =  [[-0.00653006]\n"," [-0.00652887]\n"," [ 0.00973826]]\n","Iter:  597 loss =  0.057190673998131575 learning rate =  0.5 update =  [[-0.00652005]\n"," [-0.00651888]\n"," [ 0.00972346]]\n","Iter:  598 loss =  0.057100966032317256 learning rate =  0.5 update =  [[-0.00651008]\n"," [-0.00650892]\n"," [ 0.00970871]]\n","Iter:  599 loss =  0.05701153097866765 learning rate =  0.5 update =  [[-0.00650014]\n"," [-0.00649899]\n"," [ 0.009694  ]]\n","Iter:  600 loss =  0.056922367613281226 learning rate =  0.5 update =  [[-0.00649023]\n"," [-0.00648909]\n"," [ 0.00967933]]\n","Iter:  601 loss =  0.05683347471950117 learning rate =  0.5 update =  [[-0.00648035]\n"," [-0.00647922]\n"," [ 0.0096647 ]]\n","Iter:  602 loss =  0.056744851087861814 learning rate =  0.5 update =  [[-0.00647049]\n"," [-0.00646937]\n"," [ 0.00965012]]\n","Iter:  603 loss =  0.05665649551603623 learning rate =  0.5 update =  [[-0.00646067]\n"," [-0.00645956]\n"," [ 0.00963558]]\n","Iter:  604 loss =  0.056568406808783465 learning rate =  0.5 update =  [[-0.00645088]\n"," [-0.00644977]\n"," [ 0.00962108]]\n","Iter:  605 loss =  0.056480583777896776 learning rate =  0.5 update =  [[-0.00644111]\n"," [-0.00644002]\n"," [ 0.00960663]]\n","Iter:  606 loss =  0.05639302524215217 learning rate =  0.5 update =  [[-0.00643137]\n"," [-0.00643029]\n"," [ 0.00959221]]\n","Iter:  607 loss =  0.05630573002725725 learning rate =  0.5 update =  [[-0.00642167]\n"," [-0.00642059]\n"," [ 0.00957784]]\n","Iter:  608 loss =  0.056218696965800785 learning rate =  0.5 update =  [[-0.00641199]\n"," [-0.00641092]\n"," [ 0.00956351]]\n","Iter:  609 loss =  0.056131924897202354 learning rate =  0.5 update =  [[-0.00640234]\n"," [-0.00640128]\n"," [ 0.00954923]]\n","Iter:  610 loss =  0.056045412667663004 learning rate =  0.5 update =  [[-0.00639271]\n"," [-0.00639167]\n"," [ 0.00953498]]\n","Iter:  611 loss =  0.05595915913011554 learning rate =  0.5 update =  [[-0.00638312]\n"," [-0.00638208]\n"," [ 0.00952078]]\n","Iter:  612 loss =  0.055873163144176154 learning rate =  0.5 update =  [[-0.00637355]\n"," [-0.00637253]\n"," [ 0.00950662]]\n","Iter:  613 loss =  0.055787423576095646 learning rate =  0.5 update =  [[-0.00636402]\n"," [-0.006363  ]\n"," [ 0.0094925 ]]\n","Iter:  614 loss =  0.05570193929871155 learning rate =  0.5 update =  [[-0.00635451]\n"," [-0.0063535 ]\n"," [ 0.00947841]]\n","Iter:  615 loss =  0.05561670919140049 learning rate =  0.5 update =  [[-0.00634502]\n"," [-0.00634402]\n"," [ 0.00946437]]\n","Iter:  616 loss =  0.05553173214003145 learning rate =  0.5 update =  [[-0.00633557]\n"," [-0.00633458]\n"," [ 0.00945038]]\n","Iter:  617 loss =  0.05544700703691827 learning rate =  0.5 update =  [[-0.00632614]\n"," [-0.00632516]\n"," [ 0.00943642]]\n","Iter:  618 loss =  0.055362532780773854 learning rate =  0.5 update =  [[-0.00631674]\n"," [-0.00631577]\n"," [ 0.0094225 ]]\n","Iter:  619 loss =  0.055278308276664044 learning rate =  0.5 update =  [[-0.00630737]\n"," [-0.0063064 ]\n"," [ 0.00940862]]\n","Iter:  620 loss =  0.05519433243596206 learning rate =  0.5 update =  [[-0.00629802]\n"," [-0.00629707]\n"," [ 0.00939478]]\n","Iter:  621 loss =  0.05511060417630319 learning rate =  0.5 update =  [[-0.00628871]\n"," [-0.00628776]\n"," [ 0.00938098]]\n","Iter:  622 loss =  0.05502712242154048 learning rate =  0.5 update =  [[-0.00627941]\n"," [-0.00627847]\n"," [ 0.00936722]]\n","Iter:  623 loss =  0.054943886101699625 learning rate =  0.5 update =  [[-0.00627015]\n"," [-0.00626922]\n"," [ 0.0093535 ]]\n","Iter:  624 loss =  0.05486089415293563 learning rate =  0.5 update =  [[-0.00626091]\n"," [-0.00625999]\n"," [ 0.00933982]]\n","Iter:  625 loss =  0.05477814551748872 learning rate =  0.5 update =  [[-0.0062517 ]\n"," [-0.00625078]\n"," [ 0.00932618]]\n","Iter:  626 loss =  0.054695639143641256 learning rate =  0.5 update =  [[-0.00624252]\n"," [-0.00624161]\n"," [ 0.00931257]]\n","Iter:  627 loss =  0.05461337398567477 learning rate =  0.5 update =  [[-0.00623336]\n"," [-0.00623246]\n"," [ 0.00929901]]\n","Iter:  628 loss =  0.05453134900382759 learning rate =  0.5 update =  [[-0.00622423]\n"," [-0.00622333]\n"," [ 0.00928548]]\n","Iter:  629 loss =  0.05444956316425238 learning rate =  0.5 update =  [[-0.00621512]\n"," [-0.00621423]\n"," [ 0.00927199]]\n","Iter:  630 loss =  0.05436801543897457 learning rate =  0.5 update =  [[-0.00620604]\n"," [-0.00620516]\n"," [ 0.00925854]]\n","Iter:  631 loss =  0.05428670480585071 learning rate =  0.5 update =  [[-0.00619699]\n"," [-0.00619611]\n"," [ 0.00924513]]\n","Iter:  632 loss =  0.05420563024852765 learning rate =  0.5 update =  [[-0.00618796]\n"," [-0.00618709]\n"," [ 0.00923176]]\n","Iter:  633 loss =  0.0541247907564013 learning rate =  0.5 update =  [[-0.00617895]\n"," [-0.0061781 ]\n"," [ 0.00921842]]\n","Iter:  634 loss =  0.05404418532457665 learning rate =  0.5 update =  [[-0.00616998]\n"," [-0.00616913]\n"," [ 0.00920512]]\n","Iter:  635 loss =  0.05396381295382749 learning rate =  0.5 update =  [[-0.00616103]\n"," [-0.00616018]\n"," [ 0.00919186]]\n","Iter:  636 loss =  0.05388367265055671 learning rate =  0.5 update =  [[-0.0061521 ]\n"," [-0.00615126]\n"," [ 0.00917864]]\n","Iter:  637 loss =  0.0538037634267568 learning rate =  0.5 update =  [[-0.0061432 ]\n"," [-0.00614237]\n"," [ 0.00916545]]\n","Iter:  638 loss =  0.05372408429997089 learning rate =  0.5 update =  [[-0.00613432]\n"," [-0.0061335 ]\n"," [ 0.0091523 ]]\n","Iter:  639 loss =  0.053644634293253904 learning rate =  0.5 update =  [[-0.00612547]\n"," [-0.00612466]\n"," [ 0.00913918]]\n","Iter:  640 loss =  0.053565412435134296 learning rate =  0.5 update =  [[-0.00611665]\n"," [-0.00611584]\n"," [ 0.00912611]]\n","Iter:  641 loss =  0.05348641775957569 learning rate =  0.5 update =  [[-0.00610785]\n"," [-0.00610705]\n"," [ 0.00911307]]\n","Iter:  642 loss =  0.05340764930593928 learning rate =  0.5 update =  [[-0.00609907]\n"," [-0.00609828]\n"," [ 0.00910006]]\n","Iter:  643 loss =  0.05332910611894624 learning rate =  0.5 update =  [[-0.00609032]\n"," [-0.00608953]\n"," [ 0.0090871 ]]\n","Iter:  644 loss =  0.053250787248640664 learning rate =  0.5 update =  [[-0.00608159]\n"," [-0.00608081]\n"," [ 0.00907416]]\n","Iter:  645 loss =  0.05317269175035265 learning rate =  0.5 update =  [[-0.00607289]\n"," [-0.00607211]\n"," [ 0.00906127]]\n","Iter:  646 loss =  0.05309481868466169 learning rate =  0.5 update =  [[-0.00606421]\n"," [-0.00606344]\n"," [ 0.00904841]]\n","Iter:  647 loss =  0.05301716711736053 learning rate =  0.5 update =  [[-0.00605556]\n"," [-0.00605479]\n"," [ 0.00903558]]\n","Iter:  648 loss =  0.05293973611941916 learning rate =  0.5 update =  [[-0.00604693]\n"," [-0.00604617]\n"," [ 0.00902279]]\n","Iter:  649 loss =  0.05286252476694926 learning rate =  0.5 update =  [[-0.00603832]\n"," [-0.00603757]\n"," [ 0.00901004]]\n","Iter:  650 loss =  0.05278553214116856 learning rate =  0.5 update =  [[-0.00602974]\n"," [-0.006029  ]\n"," [ 0.00899732]]\n","Iter:  651 loss =  0.052708757328366204 learning rate =  0.5 update =  [[-0.00602118]\n"," [-0.00602044]\n"," [ 0.00898464]]\n","Iter:  652 loss =  0.052632199419867604 learning rate =  0.5 update =  [[-0.00601265]\n"," [-0.00601192]\n"," [ 0.00897199]]\n","Iter:  653 loss =  0.05255585751199997 learning rate =  0.5 update =  [[-0.00600413]\n"," [-0.00600341]\n"," [ 0.00895937]]\n","Iter:  654 loss =  0.05247973070605852 learning rate =  0.5 update =  [[-0.00599565]\n"," [-0.00599493]\n"," [ 0.00894679]]\n","Iter:  655 loss =  0.05240381810827198 learning rate =  0.5 update =  [[-0.00598718]\n"," [-0.00598647]\n"," [ 0.00893425]]\n","Iter:  656 loss =  0.05232811882976923 learning rate =  0.5 update =  [[-0.00597874]\n"," [-0.00597804]\n"," [ 0.00892174]]\n","Iter:  657 loss =  0.05225263198654602 learning rate =  0.5 update =  [[-0.00597033]\n"," [-0.00596963]\n"," [ 0.00890926]]\n","Iter:  658 loss =  0.052177356699431465 learning rate =  0.5 update =  [[-0.00596193]\n"," [-0.00596124]\n"," [ 0.00889682]]\n","Iter:  659 loss =  0.05210229209405576 learning rate =  0.5 update =  [[-0.00595356]\n"," [-0.00595287]\n"," [ 0.00888441]]\n","Iter:  660 loss =  0.0520274373008171 learning rate =  0.5 update =  [[-0.00594521]\n"," [-0.00594453]\n"," [ 0.00887203]]\n","Iter:  661 loss =  0.05195279145484976 learning rate =  0.5 update =  [[-0.00593689]\n"," [-0.00593621]\n"," [ 0.00885969]]\n","Iter:  662 loss =  0.05187835369599168 learning rate =  0.5 update =  [[-0.00592859]\n"," [-0.00592791]\n"," [ 0.00884738]]\n","Iter:  663 loss =  0.051804123168753075 learning rate =  0.5 update =  [[-0.00592031]\n"," [-0.00591964]\n"," [ 0.00883511]]\n","Iter:  664 loss =  0.05173009902228466 learning rate =  0.5 update =  [[-0.00591205]\n"," [-0.00591139]\n"," [ 0.00882287]]\n","Iter:  665 loss =  0.05165628041034637 learning rate =  0.5 update =  [[-0.00590381]\n"," [-0.00590316]\n"," [ 0.00881066]]\n","Iter:  666 loss =  0.05158266649127649 learning rate =  0.5 update =  [[-0.0058956 ]\n"," [-0.00589495]\n"," [ 0.00879848]]\n","Iter:  667 loss =  0.051509256427960726 learning rate =  0.5 update =  [[-0.00588741]\n"," [-0.00588677]\n"," [ 0.00878634]]\n","Iter:  668 loss =  0.051436049387801795 learning rate =  0.5 update =  [[-0.00587924]\n"," [-0.0058786 ]\n"," [ 0.00877423]]\n","Iter:  669 loss =  0.051363044542689 learning rate =  0.5 update =  [[-0.0058711 ]\n"," [-0.00587046]\n"," [ 0.00876215]]\n","Iter:  670 loss =  0.05129024106896854 learning rate =  0.5 update =  [[-0.00586297]\n"," [-0.00586235]\n"," [ 0.0087501 ]]\n","Iter:  671 loss =  0.05121763814741323 learning rate =  0.5 update =  [[-0.00585487]\n"," [-0.00585425]\n"," [ 0.00873809]]\n","Iter:  672 loss =  0.05114523496319329 learning rate =  0.5 update =  [[-0.00584679]\n"," [-0.00584617]\n"," [ 0.00872611]]\n","Iter:  673 loss =  0.05107303070584695 learning rate =  0.5 update =  [[-0.00583874]\n"," [-0.00583812]\n"," [ 0.00871416]]\n","Iter:  674 loss =  0.05100102456925133 learning rate =  0.5 update =  [[-0.0058307 ]\n"," [-0.00583009]\n"," [ 0.00870224]]\n","Iter:  675 loss =  0.05092921575159344 learning rate =  0.5 update =  [[-0.00582269]\n"," [-0.00582208]\n"," [ 0.00869036]]\n","Iter:  676 loss =  0.05085760345534193 learning rate =  0.5 update =  [[-0.00581469]\n"," [-0.00581409]\n"," [ 0.00867851]]\n","Iter:  677 loss =  0.05078618688721834 learning rate =  0.5 update =  [[-0.00580672]\n"," [-0.00580613]\n"," [ 0.00866668]]\n","Iter:  678 loss =  0.050714965258168945 learning rate =  0.5 update =  [[-0.00579877]\n"," [-0.00579818]\n"," [ 0.00865489]]\n","Iter:  679 loss =  0.05064393778333727 learning rate =  0.5 update =  [[-0.00579084]\n"," [-0.00579026]\n"," [ 0.00864313]]\n","Iter:  680 loss =  0.050573103682035714 learning rate =  0.5 update =  [[-0.00578293]\n"," [-0.00578235]\n"," [ 0.00863141]]\n","Iter:  681 loss =  0.05050246217771835 learning rate =  0.5 update =  [[-0.00577505]\n"," [-0.00577447]\n"," [ 0.00861971]]\n","Iter:  682 loss =  0.05043201249795372 learning rate =  0.5 update =  [[-0.00576718]\n"," [-0.00576661]\n"," [ 0.00860804]]\n","Iter:  683 loss =  0.05036175387439761 learning rate =  0.5 update =  [[-0.00575934]\n"," [-0.00575877]\n"," [ 0.00859641]]\n","Iter:  684 loss =  0.050291685542766175 learning rate =  0.5 update =  [[-0.00575151]\n"," [-0.00575095]\n"," [ 0.0085848 ]]\n","Iter:  685 loss =  0.05022180674280954 learning rate =  0.5 update =  [[-0.00574371]\n"," [-0.00574315]\n"," [ 0.00857323]]\n","Iter:  686 loss =  0.05015211671828491 learning rate =  0.5 update =  [[-0.00573593]\n"," [-0.00573537]\n"," [ 0.00856168]]\n","Iter:  687 loss =  0.05008261471693083 learning rate =  0.5 update =  [[-0.00572817]\n"," [-0.00572762]\n"," [ 0.00855017]]\n","Iter:  688 loss =  0.05001329999044059 learning rate =  0.5 update =  [[-0.00572042]\n"," [-0.00571988]\n"," [ 0.00853869]]\n","Iter:  689 loss =  0.04994417179443702 learning rate =  0.5 update =  [[-0.0057127 ]\n"," [-0.00571216]\n"," [ 0.00852723]]\n","Iter:  690 loss =  0.04987522938844656 learning rate =  0.5 update =  [[-0.005705  ]\n"," [-0.00570447]\n"," [ 0.00851581]]\n","Iter:  691 loss =  0.049806472035873586 learning rate =  0.5 update =  [[-0.00569732]\n"," [-0.00569679]\n"," [ 0.00850442]]\n","Iter:  692 loss =  0.049737899003975704 learning rate =  0.5 update =  [[-0.00568966]\n"," [-0.00568914]\n"," [ 0.00849305]]\n","Iter:  693 loss =  0.04966950956383823 learning rate =  0.5 update =  [[-0.00568202]\n"," [-0.0056815 ]\n"," [ 0.00848172]]\n","Iter:  694 loss =  0.04960130299034965 learning rate =  0.5 update =  [[-0.0056744 ]\n"," [-0.00567388]\n"," [ 0.00847042]]\n","Iter:  695 loss =  0.0495332785621771 learning rate =  0.5 update =  [[-0.0056668 ]\n"," [-0.00566629]\n"," [ 0.00845914]]\n","Iter:  696 loss =  0.04946543556174139 learning rate =  0.5 update =  [[-0.00565922]\n"," [-0.00565871]\n"," [ 0.00844789]]\n","Iter:  697 loss =  0.04939777327519333 learning rate =  0.5 update =  [[-0.00565166]\n"," [-0.00565116]\n"," [ 0.00843668]]\n","Iter:  698 loss =  0.04933029099238922 learning rate =  0.5 update =  [[-0.00564412]\n"," [-0.00564362]\n"," [ 0.00842549]]\n","Iter:  699 loss =  0.04926298800686707 learning rate =  0.5 update =  [[-0.0056366 ]\n"," [-0.0056361 ]\n"," [ 0.00841433]]\n","Iter:  700 loss =  0.04919586361582326 learning rate =  0.5 update =  [[-0.0056291 ]\n"," [-0.00562861]\n"," [ 0.0084032 ]]\n","Iter:  701 loss =  0.0491289171200885 learning rate =  0.5 update =  [[-0.00562162]\n"," [-0.00562113]\n"," [ 0.0083921 ]]\n","Iter:  702 loss =  0.04906214782410459 learning rate =  0.5 update =  [[-0.00561416]\n"," [-0.00561367]\n"," [ 0.00838103]]\n","Iter:  703 loss =  0.04899555503590167 learning rate =  0.5 update =  [[-0.00560671]\n"," [-0.00560623]\n"," [ 0.00836998]]\n","Iter:  704 loss =  0.04892913806707462 learning rate =  0.5 update =  [[-0.00559929]\n"," [-0.00559881]\n"," [ 0.00835897]]\n","Iter:  705 loss =  0.04886289623276076 learning rate =  0.5 update =  [[-0.00559189]\n"," [-0.00559141]\n"," [ 0.00834798]]\n","Iter:  706 loss =  0.04879682885161688 learning rate =  0.5 update =  [[-0.0055845 ]\n"," [-0.00558403]\n"," [ 0.00833702]]\n","Iter:  707 loss =  0.04873093524579705 learning rate =  0.5 update =  [[-0.00557713]\n"," [-0.00557667]\n"," [ 0.00832609]]\n","Iter:  708 loss =  0.04866521474093011 learning rate =  0.5 update =  [[-0.00556979]\n"," [-0.00556932]\n"," [ 0.00831519]]\n","Iter:  709 loss =  0.048599666666097635 learning rate =  0.5 update =  [[-0.00556246]\n"," [-0.005562  ]\n"," [ 0.00830431]]\n","Iter:  710 loss =  0.048534290353812085 learning rate =  0.5 update =  [[-0.00555515]\n"," [-0.00555469]\n"," [ 0.00829346]]\n","Iter:  711 loss =  0.048469085139994664 learning rate =  0.5 update =  [[-0.00554786]\n"," [-0.0055474 ]\n"," [ 0.00828264]]\n","Iter:  712 loss =  0.04840405036395426 learning rate =  0.5 update =  [[-0.00554059]\n"," [-0.00554014]\n"," [ 0.00827185]]\n","Iter:  713 loss =  0.04833918536836544 learning rate =  0.5 update =  [[-0.00553333]\n"," [-0.00553289]\n"," [ 0.00826109]]\n","Iter:  714 loss =  0.04827448949924754 learning rate =  0.5 update =  [[-0.0055261 ]\n"," [-0.00552565]\n"," [ 0.00825035]]\n","Iter:  715 loss =  0.04820996210594322 learning rate =  0.5 update =  [[-0.00551888]\n"," [-0.00551844]\n"," [ 0.00823964]]\n","Iter:  716 loss =  0.04814560254109802 learning rate =  0.5 update =  [[-0.00551168]\n"," [-0.00551125]\n"," [ 0.00822895]]\n","Iter:  717 loss =  0.04808141016063878 learning rate =  0.5 update =  [[-0.0055045 ]\n"," [-0.00550407]\n"," [ 0.0082183 ]]\n","Iter:  718 loss =  0.04801738432375356 learning rate =  0.5 update =  [[-0.00549734]\n"," [-0.00549691]\n"," [ 0.00820767]]\n","Iter:  719 loss =  0.04795352439287094 learning rate =  0.5 update =  [[-0.0054902 ]\n"," [-0.00548977]\n"," [ 0.00819707]]\n","Iter:  720 loss =  0.047889829733639824 learning rate =  0.5 update =  [[-0.00548307]\n"," [-0.00548265]\n"," [ 0.00818649]]\n","Iter:  721 loss =  0.047826299714908906 learning rate =  0.5 update =  [[-0.00547597]\n"," [-0.00547555]\n"," [ 0.00817594]]\n","Iter:  722 loss =  0.04776293370870709 learning rate =  0.5 update =  [[-0.00546888]\n"," [-0.00546846]\n"," [ 0.00816542]]\n","Iter:  723 loss =  0.04769973109022298 learning rate =  0.5 update =  [[-0.00546181]\n"," [-0.00546139]\n"," [ 0.00815492]]\n","Iter:  724 loss =  0.04763669123778581 learning rate =  0.5 update =  [[-0.00545476]\n"," [-0.00545434]\n"," [ 0.00814445]]\n","Iter:  725 loss =  0.04757381353284558 learning rate =  0.5 update =  [[-0.00544772]\n"," [-0.00544731]\n"," [ 0.00813401]]\n","Iter:  726 loss =  0.04751109735995329 learning rate =  0.5 update =  [[-0.0054407 ]\n"," [-0.0054403 ]\n"," [ 0.00812359]]\n","Iter:  727 loss =  0.04744854210674207 learning rate =  0.5 update =  [[-0.0054337]\n"," [-0.0054333]\n"," [ 0.0081132]]\n","Iter:  728 loss =  0.047386147163907794 learning rate =  0.5 update =  [[-0.00542672]\n"," [-0.00542632]\n"," [ 0.00810283]]\n","Iter:  729 loss =  0.047323911925190254 learning rate =  0.5 update =  [[-0.00541976]\n"," [-0.00541936]\n"," [ 0.00809249]]\n","Iter:  730 loss =  0.047261835787353795 learning rate =  0.5 update =  [[-0.00541281]\n"," [-0.00541242]\n"," [ 0.00808218]]\n","Iter:  731 loss =  0.04719991815016944 learning rate =  0.5 update =  [[-0.00540588]\n"," [-0.00540549]\n"," [ 0.00807189]]\n","Iter:  732 loss =  0.04713815841639546 learning rate =  0.5 update =  [[-0.00539897]\n"," [-0.00539858]\n"," [ 0.00806163]]\n","Iter:  733 loss =  0.04707655599175911 learning rate =  0.5 update =  [[-0.00539207]\n"," [-0.00539169]\n"," [ 0.00805139]]\n","Iter:  734 loss =  0.047015110284938634 learning rate =  0.5 update =  [[-0.00538519]\n"," [-0.00538481]\n"," [ 0.00804118]]\n","Iter:  735 loss =  0.04695382070754494 learning rate =  0.5 update =  [[-0.00537833]\n"," [-0.00537795]\n"," [ 0.00803099]]\n","Iter:  736 loss =  0.04689268667410311 learning rate =  0.5 update =  [[-0.00537149]\n"," [-0.00537111]\n"," [ 0.00802083]]\n","Iter:  737 loss =  0.046831707602035144 learning rate =  0.5 update =  [[-0.00536466]\n"," [-0.00536429]\n"," [ 0.00801069]]\n","Iter:  738 loss =  0.04677088291164176 learning rate =  0.5 update =  [[-0.00535785]\n"," [-0.00535748]\n"," [ 0.00800058]]\n","Iter:  739 loss =  0.04671021202608483 learning rate =  0.5 update =  [[-0.00535106]\n"," [-0.00535069]\n"," [ 0.0079905 ]]\n","Iter:  740 loss =  0.04664969437136972 learning rate =  0.5 update =  [[-0.00534428]\n"," [-0.00534392]\n"," [ 0.00798043]]\n","Iter:  741 loss =  0.04658932937632809 learning rate =  0.5 update =  [[-0.00533752]\n"," [-0.00533716]\n"," [ 0.0079704 ]]\n","Iter:  742 loss =  0.04652911647260066 learning rate =  0.5 update =  [[-0.00533078]\n"," [-0.00533042]\n"," [ 0.00796039]]\n","Iter:  743 loss =  0.04646905509461974 learning rate =  0.5 update =  [[-0.00532406]\n"," [-0.0053237 ]\n"," [ 0.0079504 ]]\n","Iter:  744 loss =  0.04640914467959249 learning rate =  0.5 update =  [[-0.00531735]\n"," [-0.00531699]\n"," [ 0.00794043]]\n","Iter:  745 loss =  0.046349384667483944 learning rate =  0.5 update =  [[-0.00531065]\n"," [-0.0053103 ]\n"," [ 0.0079305 ]]\n","Iter:  746 loss =  0.0462897745010002 learning rate =  0.5 update =  [[-0.00530398]\n"," [-0.00530363]\n"," [ 0.00792058]]\n","Iter:  747 loss =  0.04623031362557159 learning rate =  0.5 update =  [[-0.00529732]\n"," [-0.00529697]\n"," [ 0.00791069]]\n","Iter:  748 loss =  0.04617100148933667 learning rate =  0.5 update =  [[-0.00529067]\n"," [-0.00529033]\n"," [ 0.00790082]]\n","Iter:  749 loss =  0.04611183754312527 learning rate =  0.5 update =  [[-0.00528405]\n"," [-0.00528371]\n"," [ 0.00789098]]\n","Iter:  750 loss =  0.04605282124044252 learning rate =  0.5 update =  [[-0.00527744]\n"," [-0.0052771 ]\n"," [ 0.00788116]]\n","Iter:  751 loss =  0.045993952037452226 learning rate =  0.5 update =  [[-0.00527084]\n"," [-0.00527051]\n"," [ 0.00787137]]\n","Iter:  752 loss =  0.04593522939296153 learning rate =  0.5 update =  [[-0.00526426]\n"," [-0.00526393]\n"," [ 0.0078616 ]]\n","Iter:  753 loss =  0.045876652768404386 learning rate =  0.5 update =  [[-0.0052577 ]\n"," [-0.00525737]\n"," [ 0.00785185]]\n","Iter:  754 loss =  0.045818221627825784 learning rate =  0.5 update =  [[-0.00525115]\n"," [-0.00525082]\n"," [ 0.00784213]]\n","Iter:  755 loss =  0.04575993543786645 learning rate =  0.5 update =  [[-0.00524462]\n"," [-0.0052443 ]\n"," [ 0.00783243]]\n","Iter:  756 loss =  0.04570179366774671 learning rate =  0.5 update =  [[-0.00523811]\n"," [-0.00523778]\n"," [ 0.00782275]]\n","Iter:  757 loss =  0.04564379578925136 learning rate =  0.5 update =  [[-0.00523161]\n"," [-0.00523129]\n"," [ 0.0078131 ]]\n","Iter:  758 loss =  0.04558594127671402 learning rate =  0.5 update =  [[-0.00522512]\n"," [-0.00522481]\n"," [ 0.00780347]]\n","Iter:  759 loss =  0.04552822960700214 learning rate =  0.5 update =  [[-0.00521866]\n"," [-0.00521834]\n"," [ 0.00779386]]\n","Iter:  760 loss =  0.04547066025950186 learning rate =  0.5 update =  [[-0.00521221]\n"," [-0.00521189]\n"," [ 0.00778428]]\n","Iter:  761 loss =  0.04541323271610237 learning rate =  0.5 update =  [[-0.00520577]\n"," [-0.00520546]\n"," [ 0.00777472]]\n","Iter:  762 loss =  0.04535594646118174 learning rate =  0.5 update =  [[-0.00519935]\n"," [-0.00519904]\n"," [ 0.00776518]]\n","Iter:  763 loss =  0.04529880098159173 learning rate =  0.5 update =  [[-0.00519294]\n"," [-0.00519264]\n"," [ 0.00775566]]\n","Iter:  764 loss =  0.04524179576664299 learning rate =  0.5 update =  [[-0.00518655]\n"," [-0.00518625]\n"," [ 0.00774617]]\n","Iter:  765 loss =  0.045184930308090294 learning rate =  0.5 update =  [[-0.00518018]\n"," [-0.00517988]\n"," [ 0.0077367 ]]\n","Iter:  766 loss =  0.045128204100118766 learning rate =  0.5 update =  [[-0.00517382]\n"," [-0.00517352]\n"," [ 0.00772725]]\n","Iter:  767 loss =  0.04507161663932845 learning rate =  0.5 update =  [[-0.00516748]\n"," [-0.00516718]\n"," [ 0.00771783]]\n","Iter:  768 loss =  0.04501516742472064 learning rate =  0.5 update =  [[-0.00516115]\n"," [-0.00516085]\n"," [ 0.00770843]]\n","Iter:  769 loss =  0.044958855957683584 learning rate =  0.5 update =  [[-0.00515483]\n"," [-0.00515454]\n"," [ 0.00769905]]\n","Iter:  770 loss =  0.044902681741978076 learning rate =  0.5 update =  [[-0.00514854]\n"," [-0.00514824]\n"," [ 0.00768969]]\n","Iter:  771 loss =  0.04484664428372387 learning rate =  0.5 update =  [[-0.00514225]\n"," [-0.00514196]\n"," [ 0.00768036]]\n","Iter:  772 loss =  0.044790743091385324 learning rate =  0.5 update =  [[-0.00513598]\n"," [-0.0051357 ]\n"," [ 0.00767104]]\n","Iter:  773 loss =  0.04473497767575789 learning rate =  0.5 update =  [[-0.00512973]\n"," [-0.00512944]\n"," [ 0.00766175]]\n","Iter:  774 loss =  0.044679347549954354 learning rate =  0.5 update =  [[-0.00512349]\n"," [-0.00512321]\n"," [ 0.00765248]]\n","Iter:  775 loss =  0.044623852229391005 learning rate =  0.5 update =  [[-0.00511727]\n"," [-0.00511699]\n"," [ 0.00764324]]\n","Iter:  776 loss =  0.04456849123177431 learning rate =  0.5 update =  [[-0.00511106]\n"," [-0.00511078]\n"," [ 0.00763401]]\n","Iter:  777 loss =  0.04451326407708747 learning rate =  0.5 update =  [[-0.00510486]\n"," [-0.00510459]\n"," [ 0.00762481]]\n","Iter:  778 loss =  0.04445817028757702 learning rate =  0.5 update =  [[-0.00509869]\n"," [-0.00509841]\n"," [ 0.00761563]]\n","Iter:  779 loss =  0.044403209387739506 learning rate =  0.5 update =  [[-0.00509252]\n"," [-0.00509225]\n"," [ 0.00760647]]\n","Iter:  780 loss =  0.044348380904308426 learning rate =  0.5 update =  [[-0.00508637]\n"," [-0.0050861 ]\n"," [ 0.00759733]]\n","Iter:  781 loss =  0.04429368436624126 learning rate =  0.5 update =  [[-0.00508023]\n"," [-0.00507996]\n"," [ 0.00758821]]\n","Iter:  782 loss =  0.04423911930470621 learning rate =  0.5 update =  [[-0.00507411]\n"," [-0.00507385]\n"," [ 0.00757911]]\n","Iter:  783 loss =  0.044184685253069705 learning rate =  0.5 update =  [[-0.00506801]\n"," [-0.00506774]\n"," [ 0.00757004]]\n","Iter:  784 loss =  0.04413038174688333 learning rate =  0.5 update =  [[-0.00506191]\n"," [-0.00506165]\n"," [ 0.00756099]]\n","Iter:  785 loss =  0.044076208323871116 learning rate =  0.5 update =  [[-0.00505584]\n"," [-0.00505557]\n"," [ 0.00755195]]\n","Iter:  786 loss =  0.044022164523917065 learning rate =  0.5 update =  [[-0.00504977]\n"," [-0.00504951]\n"," [ 0.00754294]]\n","Iter:  787 loss =  0.04396824988905267 learning rate =  0.5 update =  [[-0.00504372]\n"," [-0.00504346]\n"," [ 0.00753395]]\n","Iter:  788 loss =  0.04391446396344418 learning rate =  0.5 update =  [[-0.00503769]\n"," [-0.00503743]\n"," [ 0.00752498]]\n","Iter:  789 loss =  0.04386080629338057 learning rate =  0.5 update =  [[-0.00503167]\n"," [-0.00503141]\n"," [ 0.00751603]]\n","Iter:  790 loss =  0.04380727642726108 learning rate =  0.5 update =  [[-0.00502566]\n"," [-0.0050254 ]\n"," [ 0.00750711]]\n","Iter:  791 loss =  0.04375387391558304 learning rate =  0.5 update =  [[-0.00501966]\n"," [-0.00501941]\n"," [ 0.0074982 ]]\n","Iter:  792 loss =  0.04370059831093005 learning rate =  0.5 update =  [[-0.00501369]\n"," [-0.00501344]\n"," [ 0.00748931]]\n","Iter:  793 loss =  0.043647449167959484 learning rate =  0.5 update =  [[-0.00500772]\n"," [-0.00500747]\n"," [ 0.00748045]]\n","Iter:  794 loss =  0.04359442604339095 learning rate =  0.5 update =  [[-0.00500177]\n"," [-0.00500152]\n"," [ 0.0074716 ]]\n","Iter:  795 loss =  0.043541528495994186 learning rate =  0.5 update =  [[-0.00499583]\n"," [-0.00499559]\n"," [ 0.00746278]]\n","Iter:  796 loss =  0.04348875608657747 learning rate =  0.5 update =  [[-0.00498991]\n"," [-0.00498966]\n"," [ 0.00745397]]\n","Iter:  797 loss =  0.043436108377975916 learning rate =  0.5 update =  [[-0.004984  ]\n"," [-0.00498376]\n"," [ 0.00744519]]\n","Iter:  798 loss =  0.04338358493503959 learning rate =  0.5 update =  [[-0.0049781 ]\n"," [-0.00497786]\n"," [ 0.00743643]]\n","Iter:  799 loss =  0.04333118532462242 learning rate =  0.5 update =  [[-0.00497222]\n"," [-0.00497198]\n"," [ 0.00742768]]\n","Iter:  800 loss =  0.04327890911557049 learning rate =  0.5 update =  [[-0.00496635]\n"," [-0.00496611]\n"," [ 0.00741896]]\n","Iter:  801 loss =  0.043226755878710554 learning rate =  0.5 update =  [[-0.00496049]\n"," [-0.00496026]\n"," [ 0.00741026]]\n","Iter:  802 loss =  0.04317472518683904 learning rate =  0.5 update =  [[-0.00495465]\n"," [-0.00495442]\n"," [ 0.00740157]]\n","Iter:  803 loss =  0.04312281661471063 learning rate =  0.5 update =  [[-0.00494882]\n"," [-0.00494859]\n"," [ 0.00739291]]\n","Iter:  804 loss =  0.043071029739027156 learning rate =  0.5 update =  [[-0.00494301]\n"," [-0.00494278]\n"," [ 0.00738426]]\n","Iter:  805 loss =  0.04301936413842653 learning rate =  0.5 update =  [[-0.00493721]\n"," [-0.00493698]\n"," [ 0.00737564]]\n","Iter:  806 loss =  0.04296781939347175 learning rate =  0.5 update =  [[-0.00493142]\n"," [-0.00493119]\n"," [ 0.00736704]]\n","Iter:  807 loss =  0.042916395086639955 learning rate =  0.5 update =  [[-0.00492564]\n"," [-0.00492542]\n"," [ 0.00735845]]\n","Iter:  808 loss =  0.04286509080231178 learning rate =  0.5 update =  [[-0.00491988]\n"," [-0.00491966]\n"," [ 0.00734988]]\n","Iter:  809 loss =  0.042813906126760365 learning rate =  0.5 update =  [[-0.00491413]\n"," [-0.00491391]\n"," [ 0.00734134]]\n","Iter:  810 loss =  0.042762840648140586 learning rate =  0.5 update =  [[-0.0049084 ]\n"," [-0.00490818]\n"," [ 0.00733281]]\n","Iter:  811 loss =  0.0427118939564788 learning rate =  0.5 update =  [[-0.00490267]\n"," [-0.00490245]\n"," [ 0.00732431]]\n","Iter:  812 loss =  0.042661065643661816 learning rate =  0.5 update =  [[-0.00489696]\n"," [-0.00489675]\n"," [ 0.00731582]]\n","Iter:  813 loss =  0.042610355303426814 learning rate =  0.5 update =  [[-0.00489127]\n"," [-0.00489105]\n"," [ 0.00730735]]\n","Iter:  814 loss =  0.04255976253135075 learning rate =  0.5 update =  [[-0.00488558]\n"," [-0.00488537]\n"," [ 0.0072989 ]]\n","Iter:  815 loss =  0.04250928692484006 learning rate =  0.5 update =  [[-0.00487991]\n"," [-0.0048797 ]\n"," [ 0.00729047]]\n","Iter:  816 loss =  0.04245892808312034 learning rate =  0.5 update =  [[-0.00487425]\n"," [-0.00487404]\n"," [ 0.00728206]]\n","Iter:  817 loss =  0.042408685607226104 learning rate =  0.5 update =  [[-0.00486861]\n"," [-0.0048684 ]\n"," [ 0.00727367]]\n","Iter:  818 loss =  0.04235855909999082 learning rate =  0.5 update =  [[-0.00486298]\n"," [-0.00486277]\n"," [ 0.00726529]]\n","Iter:  819 loss =  0.04230854816603673 learning rate =  0.5 update =  [[-0.00485736]\n"," [-0.00485715]\n"," [ 0.00725694]]\n","Iter:  820 loss =  0.04225865241176484 learning rate =  0.5 update =  [[-0.00485175]\n"," [-0.00485155]\n"," [ 0.0072486 ]]\n","Iter:  821 loss =  0.042208871445344985 learning rate =  0.5 update =  [[-0.00484616]\n"," [-0.00484595]\n"," [ 0.00724028]]\n","Iter:  822 loss =  0.042159204876706005 learning rate =  0.5 update =  [[-0.00484058]\n"," [-0.00484037]\n"," [ 0.00723199]]\n","Iter:  823 loss =  0.04210965231752595 learning rate =  0.5 update =  [[-0.00483501]\n"," [-0.00483481]\n"," [ 0.00722371]]\n","Iter:  824 loss =  0.04206021338122225 learning rate =  0.5 update =  [[-0.00482945]\n"," [-0.00482925]\n"," [ 0.00721545]]\n","Iter:  825 loss =  0.04201088768294225 learning rate =  0.5 update =  [[-0.00482391]\n"," [-0.00482371]\n"," [ 0.0072072 ]]\n","Iter:  826 loss =  0.041961674839553334 learning rate =  0.5 update =  [[-0.00481838]\n"," [-0.00481818]\n"," [ 0.00719898]]\n","Iter:  827 loss =  0.04191257446963355 learning rate =  0.5 update =  [[-0.00481286]\n"," [-0.00481266]\n"," [ 0.00719077]]\n","Iter:  828 loss =  0.041863586193462046 learning rate =  0.5 update =  [[-0.00480735]\n"," [-0.00480716]\n"," [ 0.00718258]]\n","Iter:  829 loss =  0.041814709633009886 learning rate =  0.5 update =  [[-0.00480186]\n"," [-0.00480166]\n"," [ 0.00717441]]\n","Iter:  830 loss =  0.04176594441193033 learning rate =  0.5 update =  [[-0.00479637]\n"," [-0.00479618]\n"," [ 0.00716626]]\n","Iter:  831 loss =  0.04171729015554985 learning rate =  0.5 update =  [[-0.0047909 ]\n"," [-0.00479071]\n"," [ 0.00715813]]\n","Iter:  832 loss =  0.041668746490858534 learning rate =  0.5 update =  [[-0.00478545]\n"," [-0.00478526]\n"," [ 0.00715001]]\n","Iter:  833 loss =  0.04162031304650142 learning rate =  0.5 update =  [[-0.00478   ]\n"," [-0.00477981]\n"," [ 0.00714192]]\n","Iter:  834 loss =  0.041571989452769034 learning rate =  0.5 update =  [[-0.00477457]\n"," [-0.00477438]\n"," [ 0.00713384]]\n","Iter:  835 loss =  0.04152377534158849 learning rate =  0.5 update =  [[-0.00476915]\n"," [-0.00476896]\n"," [ 0.00712578]]\n","Iter:  836 loss =  0.041475670346514226 learning rate =  0.5 update =  [[-0.00476374]\n"," [-0.00476355]\n"," [ 0.00711773]]\n","Iter:  837 loss =  0.04142767410271934 learning rate =  0.5 update =  [[-0.00475834]\n"," [-0.00475816]\n"," [ 0.00710971]]\n","Iter:  838 loss =  0.041379786246986926 learning rate =  0.5 update =  [[-0.00475296]\n"," [-0.00475277]\n"," [ 0.0071017 ]]\n","Iter:  839 loss =  0.04133200641770067 learning rate =  0.5 update =  [[-0.00474758]\n"," [-0.0047474 ]\n"," [ 0.00709371]]\n","Iter:  840 loss =  0.04128433425483654 learning rate =  0.5 update =  [[-0.00474222]\n"," [-0.00474204]\n"," [ 0.00708573]]\n","Iter:  841 loss =  0.04123676939995387 learning rate =  0.5 update =  [[-0.00473687]\n"," [-0.00473669]\n"," [ 0.00707778]]\n","Iter:  842 loss =  0.04118931149618698 learning rate =  0.5 update =  [[-0.00473154]\n"," [-0.00473136]\n"," [ 0.00706984]]\n","Iter:  843 loss =  0.041141960188236175 learning rate =  0.5 update =  [[-0.00472621]\n"," [-0.00472603]\n"," [ 0.00706192]]\n","Iter:  844 loss =  0.041094715122359765 learning rate =  0.5 update =  [[-0.0047209 ]\n"," [-0.00472072]\n"," [ 0.00705402]]\n","Iter:  845 loss =  0.0410475759463649 learning rate =  0.5 update =  [[-0.00471559]\n"," [-0.00471542]\n"," [ 0.00704613]]\n","Iter:  846 loss =  0.0410005423095998 learning rate =  0.5 update =  [[-0.0047103 ]\n"," [-0.00471013]\n"," [ 0.00703826]]\n","Iter:  847 loss =  0.04095361386294506 learning rate =  0.5 update =  [[-0.00470502]\n"," [-0.00470485]\n"," [ 0.00703041]]\n","Iter:  848 loss =  0.040906790258805156 learning rate =  0.5 update =  [[-0.00469976]\n"," [-0.00469959]\n"," [ 0.00702258]]\n","Iter:  849 loss =  0.04086007115110084 learning rate =  0.5 update =  [[-0.0046945 ]\n"," [-0.00469433]\n"," [ 0.00701476]]\n","Iter:  850 loss =  0.04081345619526018 learning rate =  0.5 update =  [[-0.00468926]\n"," [-0.00468909]\n"," [ 0.00700696]]\n","Iter:  851 loss =  0.040766945048210816 learning rate =  0.5 update =  [[-0.00468402]\n"," [-0.00468386]\n"," [ 0.00699917]]\n","Iter:  852 loss =  0.040720537368371876 learning rate =  0.5 update =  [[-0.0046788 ]\n"," [-0.00467863]\n"," [ 0.00699141]]\n","Iter:  853 loss =  0.04067423281564577 learning rate =  0.5 update =  [[-0.00467359]\n"," [-0.00467343]\n"," [ 0.00698366]]\n","Iter:  854 loss =  0.04062803105141029 learning rate =  0.5 update =  [[-0.00466839]\n"," [-0.00466823]\n"," [ 0.00697592]]\n","Iter:  855 loss =  0.040581931738510615 learning rate =  0.5 update =  [[-0.0046632 ]\n"," [-0.00466304]\n"," [ 0.00696821]]\n","Iter:  856 loss =  0.040535934541251484 learning rate =  0.5 update =  [[-0.00465803]\n"," [-0.00465787]\n"," [ 0.00696051]]\n","Iter:  857 loss =  0.04049003912538938 learning rate =  0.5 update =  [[-0.00465286]\n"," [-0.0046527 ]\n"," [ 0.00695283]]\n","Iter:  858 loss =  0.04044424515812451 learning rate =  0.5 update =  [[-0.00464771]\n"," [-0.00464755]\n"," [ 0.00694516]]\n","Iter:  859 loss =  0.040398552308093374 learning rate =  0.5 update =  [[-0.00464257]\n"," [-0.00464241]\n"," [ 0.00693751]]\n","Iter:  860 loss =  0.040352960245360794 learning rate =  0.5 update =  [[-0.00463744]\n"," [-0.00463728]\n"," [ 0.00692988]]\n","Iter:  861 loss =  0.04030746864141264 learning rate =  0.5 update =  [[-0.00463231]\n"," [-0.00463216]\n"," [ 0.00692226]]\n","Iter:  862 loss =  0.04026207716914767 learning rate =  0.5 update =  [[-0.00462721]\n"," [-0.00462705]\n"," [ 0.00691466]]\n","Iter:  863 loss =  0.0402167855028706 learning rate =  0.5 update =  [[-0.00462211]\n"," [-0.00462195]\n"," [ 0.00690708]]\n","Iter:  864 loss =  0.04017159331828421 learning rate =  0.5 update =  [[-0.00461702]\n"," [-0.00461687]\n"," [ 0.00689951]]\n","Iter:  865 loss =  0.04012650029248199 learning rate =  0.5 update =  [[-0.00461194]\n"," [-0.00461179]\n"," [ 0.00689196]]\n","Iter:  866 loss =  0.040081506103940824 learning rate =  0.5 update =  [[-0.00460688]\n"," [-0.00460673]\n"," [ 0.00688442]]\n","Iter:  867 loss =  0.040036610432513595 learning rate =  0.5 update =  [[-0.00460182]\n"," [-0.00460167]\n"," [ 0.0068769 ]]\n","Iter:  868 loss =  0.039991812959421565 learning rate =  0.5 update =  [[-0.00459678]\n"," [-0.00459663]\n"," [ 0.0068694 ]]\n","Iter:  869 loss =  0.03994711336724789 learning rate =  0.5 update =  [[-0.00459175]\n"," [-0.0045916 ]\n"," [ 0.00686191]]\n","Iter:  870 loss =  0.03990251133992937 learning rate =  0.5 update =  [[-0.00458673]\n"," [-0.00458658]\n"," [ 0.00685444]]\n","Iter:  871 loss =  0.03985800656275031 learning rate =  0.5 update =  [[-0.00458171]\n"," [-0.00458157]\n"," [ 0.00684698]]\n","Iter:  872 loss =  0.03981359872233456 learning rate =  0.5 update =  [[-0.00457671]\n"," [-0.00457657]\n"," [ 0.00683954]]\n","Iter:  873 loss =  0.039769287506638856 learning rate =  0.5 update =  [[-0.00457172]\n"," [-0.00457158]\n"," [ 0.00683212]]\n","Iter:  874 loss =  0.0397250726049459 learning rate =  0.5 update =  [[-0.00456674]\n"," [-0.0045666 ]\n"," [ 0.00682471]]\n","Iter:  875 loss =  0.03968095370785686 learning rate =  0.5 update =  [[-0.00456178]\n"," [-0.00456163]\n"," [ 0.00681732]]\n","Iter:  876 loss =  0.039636930507285006 learning rate =  0.5 update =  [[-0.00455682]\n"," [-0.00455668]\n"," [ 0.00680994]]\n","Iter:  877 loss =  0.039593002696448464 learning rate =  0.5 update =  [[-0.00455187]\n"," [-0.00455173]\n"," [ 0.00680258]]\n","Iter:  878 loss =  0.03954916996986334 learning rate =  0.5 update =  [[-0.00454693]\n"," [-0.00454679]\n"," [ 0.00679524]]\n","Iter:  879 loss =  0.03950543202333709 learning rate =  0.5 update =  [[-0.00454201]\n"," [-0.00454187]\n"," [ 0.00678791]]\n","Iter:  880 loss =  0.03946178855396169 learning rate =  0.5 update =  [[-0.00453709]\n"," [-0.00453695]\n"," [ 0.00678059]]\n","Iter:  881 loss =  0.03941823926010666 learning rate =  0.5 update =  [[-0.00453219]\n"," [-0.00453205]\n"," [ 0.00677329]]\n","Iter:  882 loss =  0.03937478384141285 learning rate =  0.5 update =  [[-0.00452729]\n"," [-0.00452715]\n"," [ 0.00676601]]\n","Iter:  883 loss =  0.03933142199878538 learning rate =  0.5 update =  [[-0.00452241]\n"," [-0.00452227]\n"," [ 0.00675874]]\n","Iter:  884 loss =  0.03928815343438716 learning rate =  0.5 update =  [[-0.00451753]\n"," [-0.0045174 ]\n"," [ 0.00675149]]\n","Iter:  885 loss =  0.03924497785163239 learning rate =  0.5 update =  [[-0.00451267]\n"," [-0.00451253]\n"," [ 0.00674425]]\n","Iter:  886 loss =  0.03920189495518005 learning rate =  0.5 update =  [[-0.00450781]\n"," [-0.00450768]\n"," [ 0.00673703]]\n","Iter:  887 loss =  0.03915890445092712 learning rate =  0.5 update =  [[-0.00450297]\n"," [-0.00450284]\n"," [ 0.00672982]]\n","Iter:  888 loss =  0.03911600604600273 learning rate =  0.5 update =  [[-0.00449814]\n"," [-0.00449801]\n"," [ 0.00672263]]\n","Iter:  889 loss =  0.03907319944876111 learning rate =  0.5 update =  [[-0.00449331]\n"," [-0.00449318]\n"," [ 0.00671545]]\n","Iter:  890 loss =  0.039030484368775556 learning rate =  0.5 update =  [[-0.0044885 ]\n"," [-0.00448837]\n"," [ 0.00670829]]\n","Iter:  891 loss =  0.03898786051683212 learning rate =  0.5 update =  [[-0.0044837 ]\n"," [-0.00448357]\n"," [ 0.00670114]]\n","Iter:  892 loss =  0.03894532760492339 learning rate =  0.5 update =  [[-0.0044789 ]\n"," [-0.00447878]\n"," [ 0.00669401]]\n","Iter:  893 loss =  0.03890288534624182 learning rate =  0.5 update =  [[-0.00447412]\n"," [-0.00447399]\n"," [ 0.00668689]]\n","Iter:  894 loss =  0.03886053345517421 learning rate =  0.5 update =  [[-0.00446935]\n"," [-0.00446922]\n"," [ 0.00667979]]\n","Iter:  895 loss =  0.03881827164729475 learning rate =  0.5 update =  [[-0.00446458]\n"," [-0.00446446]\n"," [ 0.0066727 ]]\n","Iter:  896 loss =  0.03877609963935966 learning rate =  0.5 update =  [[-0.00445983]\n"," [-0.00445971]\n"," [ 0.00666563]]\n","Iter:  897 loss =  0.038734017149300526 learning rate =  0.5 update =  [[-0.00445509]\n"," [-0.00445496]\n"," [ 0.00665857]]\n","Iter:  898 loss =  0.03869202389621844 learning rate =  0.5 update =  [[-0.00445035]\n"," [-0.00445023]\n"," [ 0.00665152]]\n","Iter:  899 loss =  0.03865011960037809 learning rate =  0.5 update =  [[-0.00444563]\n"," [-0.00444551]\n"," [ 0.00664449]]\n","Iter:  900 loss =  0.0386083039832016 learning rate =  0.5 update =  [[-0.00444092]\n"," [-0.0044408 ]\n"," [ 0.00663748]]\n","Iter:  901 loss =  0.03856657676726275 learning rate =  0.5 update =  [[-0.00443621]\n"," [-0.00443609]\n"," [ 0.00663048]]\n","Iter:  902 loss =  0.0385249376762809 learning rate =  0.5 update =  [[-0.00443152]\n"," [-0.0044314 ]\n"," [ 0.00662349]]\n","Iter:  903 loss =  0.03848338643511527 learning rate =  0.5 update =  [[-0.00442684]\n"," [-0.00442672]\n"," [ 0.00661652]]\n","Iter:  904 loss =  0.03844192276975888 learning rate =  0.5 update =  [[-0.00442216]\n"," [-0.00442204]\n"," [ 0.00660956]]\n","Iter:  905 loss =  0.038400546407333105 learning rate =  0.5 update =  [[-0.0044175 ]\n"," [-0.00441738]\n"," [ 0.00660262]]\n","Iter:  906 loss =  0.03835925707608169 learning rate =  0.5 update =  [[-0.00441284]\n"," [-0.00441272]\n"," [ 0.00659569]]\n","Iter:  907 loss =  0.03831805450536481 learning rate =  0.5 update =  [[-0.00440819]\n"," [-0.00440808]\n"," [ 0.00658878]]\n","Iter:  908 loss =  0.03827693842565397 learning rate =  0.5 update =  [[-0.00440356]\n"," [-0.00440344]\n"," [ 0.00658188]]\n","Iter:  909 loss =  0.03823590856852564 learning rate =  0.5 update =  [[-0.00439893]\n"," [-0.00439882]\n"," [ 0.00657499]]\n","Iter:  910 loss =  0.03819496466665619 learning rate =  0.5 update =  [[-0.00439431]\n"," [-0.0043942 ]\n"," [ 0.00656812]]\n","Iter:  911 loss =  0.038154106453815964 learning rate =  0.5 update =  [[-0.00438971]\n"," [-0.00438959]\n"," [ 0.00656126]]\n","Iter:  912 loss =  0.0381133336648639 learning rate =  0.5 update =  [[-0.00438511]\n"," [-0.004385  ]\n"," [ 0.00655442]]\n","Iter:  913 loss =  0.0380726460357419 learning rate =  0.5 update =  [[-0.00438052]\n"," [-0.00438041]\n"," [ 0.00654759]]\n","Iter:  914 loss =  0.03803204330346934 learning rate =  0.5 update =  [[-0.00437594]\n"," [-0.00437583]\n"," [ 0.00654077]]\n","Iter:  915 loss =  0.037991525206137924 learning rate =  0.5 update =  [[-0.00437137]\n"," [-0.00437126]\n"," [ 0.00653397]]\n","Iter:  916 loss =  0.037951091482905486 learning rate =  0.5 update =  [[-0.00436681]\n"," [-0.0043667 ]\n"," [ 0.00652718]]\n","Iter:  917 loss =  0.03791074187399156 learning rate =  0.5 update =  [[-0.00436226]\n"," [-0.00436215]\n"," [ 0.00652041]]\n","Iter:  918 loss =  0.03787047612067147 learning rate =  0.5 update =  [[-0.00435772]\n"," [-0.00435761]\n"," [ 0.00651365]]\n","Iter:  919 loss =  0.03783029396527092 learning rate =  0.5 update =  [[-0.00435318]\n"," [-0.00435308]\n"," [ 0.0065069 ]]\n","Iter:  920 loss =  0.037790195151161055 learning rate =  0.5 update =  [[-0.00434866]\n"," [-0.00434855]\n"," [ 0.00650017]]\n","Iter:  921 loss =  0.03775017942275294 learning rate =  0.5 update =  [[-0.00434415]\n"," [-0.00434404]\n"," [ 0.00649345]]\n","Iter:  922 loss =  0.03771024652549237 learning rate =  0.5 update =  [[-0.00433964]\n"," [-0.00433954]\n"," [ 0.00648674]]\n","Iter:  923 loss =  0.037670396205854814 learning rate =  0.5 update =  [[-0.00433515]\n"," [-0.00433504]\n"," [ 0.00648005]]\n","Iter:  924 loss =  0.037630628211340104 learning rate =  0.5 update =  [[-0.00433066]\n"," [-0.00433055]\n"," [ 0.00647337]]\n","Iter:  925 loss =  0.03759094229046741 learning rate =  0.5 update =  [[-0.00432618]\n"," [-0.00432608]\n"," [ 0.0064667 ]]\n","Iter:  926 loss =  0.03755133819276981 learning rate =  0.5 update =  [[-0.00432171]\n"," [-0.00432161]\n"," [ 0.00646005]]\n","Iter:  927 loss =  0.03751181566878972 learning rate =  0.5 update =  [[-0.00431725]\n"," [-0.00431715]\n"," [ 0.00645341]]\n","Iter:  928 loss =  0.037472374470073504 learning rate =  0.5 update =  [[-0.0043128 ]\n"," [-0.0043127 ]\n"," [ 0.00644679]]\n","Iter:  929 loss =  0.03743301434916664 learning rate =  0.5 update =  [[-0.00430836]\n"," [-0.00430826]\n"," [ 0.00644017]]\n","Iter:  930 loss =  0.03739373505960839 learning rate =  0.5 update =  [[-0.00430393]\n"," [-0.00430383]\n"," [ 0.00643358]]\n","Iter:  931 loss =  0.03735453635592747 learning rate =  0.5 update =  [[-0.00429951]\n"," [-0.0042994 ]\n"," [ 0.00642699]]\n","Iter:  932 loss =  0.03731541799363629 learning rate =  0.5 update =  [[-0.00429509]\n"," [-0.00429499]\n"," [ 0.00642042]]\n","Iter:  933 loss =  0.037276379729226955 learning rate =  0.5 update =  [[-0.00429068]\n"," [-0.00429059]\n"," [ 0.00641386]]\n","Iter:  934 loss =  0.03723742132016566 learning rate =  0.5 update =  [[-0.00428629]\n"," [-0.00428619]\n"," [ 0.00640731]]\n","Iter:  935 loss =  0.037198542524888245 learning rate =  0.5 update =  [[-0.0042819 ]\n"," [-0.0042818 ]\n"," [ 0.00640078]]\n","Iter:  936 loss =  0.037159743102795476 learning rate =  0.5 update =  [[-0.00427752]\n"," [-0.00427742]\n"," [ 0.00639426]]\n","Iter:  937 loss =  0.037121022814247645 learning rate =  0.5 update =  [[-0.00427315]\n"," [-0.00427305]\n"," [ 0.00638775]]\n","Iter:  938 loss =  0.037082381420560695 learning rate =  0.5 update =  [[-0.00426879]\n"," [-0.00426869]\n"," [ 0.00638126]]\n","Iter:  939 loss =  0.03704381868400067 learning rate =  0.5 update =  [[-0.00426443]\n"," [-0.00426434]\n"," [ 0.00637478]]\n","Iter:  940 loss =  0.037005334367779724 learning rate =  0.5 update =  [[-0.00426009]\n"," [-0.00426   ]\n"," [ 0.00636831]]\n","Iter:  941 loss =  0.036966928236050856 learning rate =  0.5 update =  [[-0.00425575]\n"," [-0.00425566]\n"," [ 0.00636185]]\n","Iter:  942 loss =  0.036928600053903735 learning rate =  0.5 update =  [[-0.00425143]\n"," [-0.00425133]\n"," [ 0.00635541]]\n","Iter:  943 loss =  0.03689034958735978 learning rate =  0.5 update =  [[-0.00424711]\n"," [-0.00424702]\n"," [ 0.00634898]]\n","Iter:  944 loss =  0.03685217660336761 learning rate =  0.5 update =  [[-0.0042428 ]\n"," [-0.00424271]\n"," [ 0.00634257]]\n","Iter:  945 loss =  0.036814080869798614 learning rate =  0.5 update =  [[-0.0042385 ]\n"," [-0.00423841]\n"," [ 0.00633616]]\n","Iter:  946 loss =  0.036776062155442456 learning rate =  0.5 update =  [[-0.0042342 ]\n"," [-0.00423411]\n"," [ 0.00632977]]\n","Iter:  947 loss =  0.036738120230002336 learning rate =  0.5 update =  [[-0.00422992]\n"," [-0.00422983]\n"," [ 0.00632339]]\n","Iter:  948 loss =  0.03670025486409047 learning rate =  0.5 update =  [[-0.00422564]\n"," [-0.00422555]\n"," [ 0.00631702]]\n","Iter:  949 loss =  0.0366624658292242 learning rate =  0.5 update =  [[-0.00422138]\n"," [-0.00422129]\n"," [ 0.00631067]]\n","Iter:  950 loss =  0.03662475289782087 learning rate =  0.5 update =  [[-0.00421712]\n"," [-0.00421703]\n"," [ 0.00630433]]\n","Iter:  951 loss =  0.03658711584319381 learning rate =  0.5 update =  [[-0.00421287]\n"," [-0.00421278]\n"," [ 0.006298  ]]\n","Iter:  952 loss =  0.03654955443954784 learning rate =  0.5 update =  [[-0.00420863]\n"," [-0.00420854]\n"," [ 0.00629168]]\n","Iter:  953 loss =  0.03651206846197492 learning rate =  0.5 update =  [[-0.00420439]\n"," [-0.0042043 ]\n"," [ 0.00628538]]\n","Iter:  954 loss =  0.03647465768644987 learning rate =  0.5 update =  [[-0.00420017]\n"," [-0.00420008]\n"," [ 0.00627909]]\n","Iter:  955 loss =  0.03643732188982616 learning rate =  0.5 update =  [[-0.00419595]\n"," [-0.00419586]\n"," [ 0.00627281]]\n","Iter:  956 loss =  0.03640006084983126 learning rate =  0.5 update =  [[-0.00419174]\n"," [-0.00419166]\n"," [ 0.00626654]]\n","Iter:  957 loss =  0.036362874345062995 learning rate =  0.5 update =  [[-0.00418754]\n"," [-0.00418746]\n"," [ 0.00626029]]\n","Iter:  958 loss =  0.03632576215498458 learning rate =  0.5 update =  [[-0.00418335]\n"," [-0.00418326]\n"," [ 0.00625404]]\n","Iter:  959 loss =  0.0362887240599212 learning rate =  0.5 update =  [[-0.00417917]\n"," [-0.00417908]\n"," [ 0.00624781]]\n","Iter:  960 loss =  0.036251759841055 learning rate =  0.5 update =  [[-0.00417499]\n"," [-0.00417491]\n"," [ 0.0062416 ]]\n","Iter:  961 loss =  0.03621486928042188 learning rate =  0.5 update =  [[-0.00417082]\n"," [-0.00417074]\n"," [ 0.00623539]]\n","Iter:  962 loss =  0.03617805216090645 learning rate =  0.5 update =  [[-0.00416666]\n"," [-0.00416658]\n"," [ 0.0062292 ]]\n","Iter:  963 loss =  0.03614130826623855 learning rate =  0.5 update =  [[-0.00416251]\n"," [-0.00416243]\n"," [ 0.00622301]]\n","Iter:  964 loss =  0.036104637380988874 learning rate =  0.5 update =  [[-0.00415837]\n"," [-0.00415829]\n"," [ 0.00621684]]\n","Iter:  965 loss =  0.03606803929056506 learning rate =  0.5 update =  [[-0.00415423]\n"," [-0.00415415]\n"," [ 0.00621069]]\n","Iter:  966 loss =  0.036031513781207364 learning rate =  0.5 update =  [[-0.00415011]\n"," [-0.00415003]\n"," [ 0.00620454]]\n","Iter:  967 loss =  0.035995060639985144 learning rate =  0.5 update =  [[-0.00414599]\n"," [-0.00414591]\n"," [ 0.00619841]]\n","Iter:  968 loss =  0.03595867965479253 learning rate =  0.5 update =  [[-0.00414188]\n"," [-0.0041418 ]\n"," [ 0.00619228]]\n","Iter:  969 loss =  0.035922370614344265 learning rate =  0.5 update =  [[-0.00413777]\n"," [-0.00413769]\n"," [ 0.00618617]]\n","Iter:  970 loss =  0.03588613330817244 learning rate =  0.5 update =  [[-0.00413368]\n"," [-0.0041336 ]\n"," [ 0.00618008]]\n","Iter:  971 loss =  0.035849967526621895 learning rate =  0.5 update =  [[-0.00412959]\n"," [-0.00412951]\n"," [ 0.00617399]]\n","Iter:  972 loss =  0.03581387306084656 learning rate =  0.5 update =  [[-0.00412551]\n"," [-0.00412543]\n"," [ 0.00616791]]\n","Iter:  973 loss =  0.035777849702805634 learning rate =  0.5 update =  [[-0.00412144]\n"," [-0.00412136]\n"," [ 0.00616185]]\n","Iter:  974 loss =  0.03574189724525977 learning rate =  0.5 update =  [[-0.00411738]\n"," [-0.0041173 ]\n"," [ 0.0061558 ]]\n","Iter:  975 loss =  0.03570601548176722 learning rate =  0.5 update =  [[-0.00411332]\n"," [-0.00411325]\n"," [ 0.00614976]]\n","Iter:  976 loss =  0.03567020420667974 learning rate =  0.5 update =  [[-0.00410927]\n"," [-0.0041092 ]\n"," [ 0.00614373]]\n","Iter:  977 loss =  0.035634463215139196 learning rate =  0.5 update =  [[-0.00410523]\n"," [-0.00410516]\n"," [ 0.00613771]]\n","Iter:  978 loss =  0.035598792303073674 learning rate =  0.5 update =  [[-0.0041012 ]\n"," [-0.00410113]\n"," [ 0.00613171]]\n","Iter:  979 loss =  0.03556319126719362 learning rate =  0.5 update =  [[-0.00409718]\n"," [-0.0040971 ]\n"," [ 0.00612571]]\n","Iter:  980 loss =  0.03552765990498818 learning rate =  0.5 update =  [[-0.00409316]\n"," [-0.00409309]\n"," [ 0.00611973]]\n","Iter:  981 loss =  0.03549219801472159 learning rate =  0.5 update =  [[-0.00408915]\n"," [-0.00408908]\n"," [ 0.00611376]]\n","Iter:  982 loss =  0.03545680539542932 learning rate =  0.5 update =  [[-0.00408515]\n"," [-0.00408508]\n"," [ 0.0061078 ]]\n","Iter:  983 loss =  0.0354214818469146 learning rate =  0.5 update =  [[-0.00408116]\n"," [-0.00408109]\n"," [ 0.00610185]]\n","Iter:  984 loss =  0.03538622716974467 learning rate =  0.5 update =  [[-0.00407717]\n"," [-0.0040771 ]\n"," [ 0.00609592]]\n","Iter:  985 loss =  0.035351041165247246 learning rate =  0.5 update =  [[-0.00407319]\n"," [-0.00407312]\n"," [ 0.00608999]]\n","Iter:  986 loss =  0.0353159236355066 learning rate =  0.5 update =  [[-0.00406922]\n"," [-0.00406915]\n"," [ 0.00608408]]\n","Iter:  987 loss =  0.03528087438336059 learning rate =  0.5 update =  [[-0.00406526]\n"," [-0.00406519]\n"," [ 0.00607818]]\n","Iter:  988 loss =  0.0352458932123965 learning rate =  0.5 update =  [[-0.00406131]\n"," [-0.00406123]\n"," [ 0.00607228]]\n","Iter:  989 loss =  0.035210979926947855 learning rate =  0.5 update =  [[-0.00405736]\n"," [-0.00405729]\n"," [ 0.0060664 ]]\n","Iter:  990 loss =  0.03517613433209083 learning rate =  0.5 update =  [[-0.00405342]\n"," [-0.00405335]\n"," [ 0.00606053]]\n","Iter:  991 loss =  0.035141356233640564 learning rate =  0.5 update =  [[-0.00404948]\n"," [-0.00404941]\n"," [ 0.00605468]]\n","Iter:  992 loss =  0.03510664543814804 learning rate =  0.5 update =  [[-0.00404556]\n"," [-0.00404549]\n"," [ 0.00604883]]\n","Iter:  993 loss =  0.03507200175289631 learning rate =  0.5 update =  [[-0.00404164]\n"," [-0.00404157]\n"," [ 0.00604299]]\n","Iter:  994 loss =  0.03503742498589718 learning rate =  0.5 update =  [[-0.00403773]\n"," [-0.00403766]\n"," [ 0.00603717]]\n","Iter:  995 loss =  0.035002914945887795 learning rate =  0.5 update =  [[-0.00403383]\n"," [-0.00403376]\n"," [ 0.00603135]]\n","Iter:  996 loss =  0.03496847144232715 learning rate =  0.5 update =  [[-0.00402993]\n"," [-0.00402986]\n"," [ 0.00602555]]\n","Iter:  997 loss =  0.03493409428539298 learning rate =  0.5 update =  [[-0.00402604]\n"," [-0.00402598]\n"," [ 0.00601976]]\n","Iter:  998 loss =  0.03489978328597795 learning rate =  0.5 update =  [[-0.00402216]\n"," [-0.0040221 ]\n"," [ 0.00601398]]\n","Iter:  999 loss =  0.034865538255686765 learning rate =  0.5 update =  [[-0.00401829]\n"," [-0.00401822]\n"," [ 0.00600821]]\n","Iter:  1000 loss =  0.03483135900683246 learning rate =  0.5 update =  [[-0.00401442]\n"," [-0.00401436]\n"," [ 0.00600245]]\n","Iter:  1001 loss =  0.03479724535243334 learning rate =  0.5 update =  [[-0.00401056]\n"," [-0.0040105 ]\n"," [ 0.0059967 ]]\n","Iter:  1002 loss =  0.03476319710620976 learning rate =  0.5 update =  [[-0.00400671]\n"," [-0.00400665]\n"," [ 0.00599096]]\n","Iter:  1003 loss =  0.03472921408258041 learning rate =  0.5 update =  [[-0.00400287]\n"," [-0.0040028 ]\n"," [ 0.00598524]]\n","Iter:  1004 loss =  0.034695296096659736 learning rate =  0.5 update =  [[-0.00399903]\n"," [-0.00399897]\n"," [ 0.00597952]]\n","Iter:  1005 loss =  0.03466144296425394 learning rate =  0.5 update =  [[-0.0039952 ]\n"," [-0.00399514]\n"," [ 0.00597381]]\n","Iter:  1006 loss =  0.034627654501858514 learning rate =  0.5 update =  [[-0.00399138]\n"," [-0.00399131]\n"," [ 0.00596812]]\n","Iter:  1007 loss =  0.034593930526654465 learning rate =  0.5 update =  [[-0.00398756]\n"," [-0.0039875 ]\n"," [ 0.00596244]]\n","Iter:  1008 loss =  0.03456027085650544 learning rate =  0.5 update =  [[-0.00398375]\n"," [-0.00398369]\n"," [ 0.00595676]]\n","Iter:  1009 loss =  0.03452667530995444 learning rate =  0.5 update =  [[-0.00397995]\n"," [-0.00397989]\n"," [ 0.0059511 ]]\n","Iter:  1010 loss =  0.034493143706220766 learning rate =  0.5 update =  [[-0.00397616]\n"," [-0.0039761 ]\n"," [ 0.00594545]]\n","Iter:  1011 loss =  0.034459675865196826 learning rate =  0.5 update =  [[-0.00397237]\n"," [-0.00397231]\n"," [ 0.0059398 ]]\n","Iter:  1012 loss =  0.034426271607445 learning rate =  0.5 update =  [[-0.00396859]\n"," [-0.00396853]\n"," [ 0.00593417]]\n","Iter:  1013 loss =  0.034392930754194626 learning rate =  0.5 update =  [[-0.00396482]\n"," [-0.00396476]\n"," [ 0.00592855]]\n","Iter:  1014 loss =  0.034359653127338775 learning rate =  0.5 update =  [[-0.00396105]\n"," [-0.00396099]\n"," [ 0.00592294]]\n","Iter:  1015 loss =  0.03432643854943143 learning rate =  0.5 update =  [[-0.00395729]\n"," [-0.00395723]\n"," [ 0.00591734]]\n","Iter:  1016 loss =  0.03429328684368435 learning rate =  0.5 update =  [[-0.00395354]\n"," [-0.00395348]\n"," [ 0.00591175]]\n","Iter:  1017 loss =  0.03426019783396378 learning rate =  0.5 update =  [[-0.0039498 ]\n"," [-0.00394974]\n"," [ 0.00590617]]\n","Iter:  1018 loss =  0.03422717134478787 learning rate =  0.5 update =  [[-0.00394606]\n"," [-0.003946  ]\n"," [ 0.0059006 ]]\n","Iter:  1019 loss =  0.03419420720132331 learning rate =  0.5 update =  [[-0.00394233]\n"," [-0.00394227]\n"," [ 0.00589504]]\n","Iter:  1020 loss =  0.03416130522938272 learning rate =  0.5 update =  [[-0.0039386 ]\n"," [-0.00393855]\n"," [ 0.0058895 ]]\n","Iter:  1021 loss =  0.03412846525542125 learning rate =  0.5 update =  [[-0.00393489]\n"," [-0.00393483]\n"," [ 0.00588396]]\n","Iter:  1022 loss =  0.0340956871065341 learning rate =  0.5 update =  [[-0.00393118]\n"," [-0.00393112]\n"," [ 0.00587843]]\n","Iter:  1023 loss =  0.03406297061045317 learning rate =  0.5 update =  [[-0.00392747]\n"," [-0.00392742]\n"," [ 0.00587291]]\n","Iter:  1024 loss =  0.034030315595544626 learning rate =  0.5 update =  [[-0.00392378]\n"," [-0.00392372]\n"," [ 0.0058674 ]]\n","Iter:  1025 loss =  0.0339977218908053 learning rate =  0.5 update =  [[-0.00392009]\n"," [-0.00392003]\n"," [ 0.00586191]]\n","Iter:  1026 loss =  0.033965189325860665 learning rate =  0.5 update =  [[-0.0039164 ]\n"," [-0.00391635]\n"," [ 0.00585642]]\n","Iter:  1027 loss =  0.03393271773096113 learning rate =  0.5 update =  [[-0.00391273]\n"," [-0.00391267]\n"," [ 0.00585094]]\n","Iter:  1028 loss =  0.03390030693698003 learning rate =  0.5 update =  [[-0.00390906]\n"," [-0.003909  ]\n"," [ 0.00584547]]\n","Iter:  1029 loss =  0.033867956775409956 learning rate =  0.5 update =  [[-0.0039054 ]\n"," [-0.00390534]\n"," [ 0.00584002]]\n","Iter:  1030 loss =  0.03383566707836062 learning rate =  0.5 update =  [[-0.00390174]\n"," [-0.00390168]\n"," [ 0.00583457]]\n","Iter:  1031 loss =  0.03380343767855555 learning rate =  0.5 update =  [[-0.00389809]\n"," [-0.00389804]\n"," [ 0.00582913]]\n","Iter:  1032 loss =  0.033771268409329785 learning rate =  0.5 update =  [[-0.00389445]\n"," [-0.00389439]\n"," [ 0.0058237 ]]\n","Iter:  1033 loss =  0.03373915910462665 learning rate =  0.5 update =  [[-0.00389081]\n"," [-0.00389076]\n"," [ 0.00581829]]\n","Iter:  1034 loss =  0.03370710959899525 learning rate =  0.5 update =  [[-0.00388718]\n"," [-0.00388713]\n"," [ 0.00581288]]\n","Iter:  1035 loss =  0.033675119727587514 learning rate =  0.5 update =  [[-0.00388356]\n"," [-0.00388351]\n"," [ 0.00580748]]\n","Iter:  1036 loss =  0.03364318932615608 learning rate =  0.5 update =  [[-0.00387995]\n"," [-0.00387989]\n"," [ 0.00580209]]\n","Iter:  1037 loss =  0.033611318231050506 learning rate =  0.5 update =  [[-0.00387634]\n"," [-0.00387628]\n"," [ 0.00579671]]\n","Iter:  1038 loss =  0.033579506279215576 learning rate =  0.5 update =  [[-0.00387273]\n"," [-0.00387268]\n"," [ 0.00579135]]\n","Iter:  1039 loss =  0.03354775330818825 learning rate =  0.5 update =  [[-0.00386914]\n"," [-0.00386908]\n"," [ 0.00578599]]\n","Iter:  1040 loss =  0.033516059156094756 learning rate =  0.5 update =  [[-0.00386555]\n"," [-0.00386549]\n"," [ 0.00578064]]\n","Iter:  1041 loss =  0.0334844236616482 learning rate =  0.5 update =  [[-0.00386196]\n"," [-0.00386191]\n"," [ 0.0057753 ]]\n","Iter:  1042 loss =  0.03345284666414608 learning rate =  0.5 update =  [[-0.00385839]\n"," [-0.00385834]\n"," [ 0.00576997]]\n","Iter:  1043 loss =  0.03342132800346721 learning rate =  0.5 update =  [[-0.00385482]\n"," [-0.00385477]\n"," [ 0.00576465]]\n","Iter:  1044 loss =  0.03338986752006952 learning rate =  0.5 update =  [[-0.00385125]\n"," [-0.0038512 ]\n"," [ 0.00575934]]\n","Iter:  1045 loss =  0.033358465054987206 learning rate =  0.5 update =  [[-0.0038477 ]\n"," [-0.00384765]\n"," [ 0.00575404]]\n","Iter:  1046 loss =  0.033327120449828204 learning rate =  0.5 update =  [[-0.00384415]\n"," [-0.00384409]\n"," [ 0.00574875]]\n","Iter:  1047 loss =  0.03329583354677169 learning rate =  0.5 update =  [[-0.0038406 ]\n"," [-0.00384055]\n"," [ 0.00574347]]\n","Iter:  1048 loss =  0.03326460418856553 learning rate =  0.5 update =  [[-0.00383706]\n"," [-0.00383701]\n"," [ 0.00573819]]\n","Iter:  1049 loss =  0.03323343221852362 learning rate =  0.5 update =  [[-0.00383353]\n"," [-0.00383348]\n"," [ 0.00573293]]\n","Iter:  1050 loss =  0.03320231748052352 learning rate =  0.5 update =  [[-0.00383001]\n"," [-0.00382996]\n"," [ 0.00572768]]\n","Iter:  1051 loss =  0.03317125981900347 learning rate =  0.5 update =  [[-0.00382649]\n"," [-0.00382644]\n"," [ 0.00572243]]\n","Iter:  1052 loss =  0.03314025907896085 learning rate =  0.5 update =  [[-0.00382298]\n"," [-0.00382293]\n"," [ 0.0057172 ]]\n","Iter:  1053 loss =  0.033109315105948584 learning rate =  0.5 update =  [[-0.00381947]\n"," [-0.00381942]\n"," [ 0.00571197]]\n","Iter:  1054 loss =  0.03307842774607343 learning rate =  0.5 update =  [[-0.00381597]\n"," [-0.00381592]\n"," [ 0.00570676]]\n","Iter:  1055 loss =  0.03304759684599329 learning rate =  0.5 update =  [[-0.00381248]\n"," [-0.00381243]\n"," [ 0.00570155]]\n","Iter:  1056 loss =  0.03301682225291447 learning rate =  0.5 update =  [[-0.00380899]\n"," [-0.00380894]\n"," [ 0.00569636]]\n","Iter:  1057 loss =  0.032986103814589884 learning rate =  0.5 update =  [[-0.00380551]\n"," [-0.00380546]\n"," [ 0.00569117]]\n","Iter:  1058 loss =  0.0329554413793159 learning rate =  0.5 update =  [[-0.00380204]\n"," [-0.00380199]\n"," [ 0.00568599]]\n","Iter:  1059 loss =  0.03292483479593049 learning rate =  0.5 update =  [[-0.00379857]\n"," [-0.00379852]\n"," [ 0.00568082]]\n","Iter:  1060 loss =  0.032894283913810544 learning rate =  0.5 update =  [[-0.0037951 ]\n"," [-0.00379506]\n"," [ 0.00567566]]\n","Iter:  1061 loss =  0.03286378858286959 learning rate =  0.5 update =  [[-0.00379165]\n"," [-0.0037916 ]\n"," [ 0.00567051]]\n","Iter:  1062 loss =  0.03283334865355543 learning rate =  0.5 update =  [[-0.0037882 ]\n"," [-0.00378815]\n"," [ 0.00566537]]\n","Iter:  1063 loss =  0.0328029639768476 learning rate =  0.5 update =  [[-0.00378476]\n"," [-0.00378471]\n"," [ 0.00566024]]\n","Iter:  1064 loss =  0.032772634404255326 learning rate =  0.5 update =  [[-0.00378132]\n"," [-0.00378127]\n"," [ 0.00565511]]\n","Iter:  1065 loss =  0.03274235978781494 learning rate =  0.5 update =  [[-0.00377789]\n"," [-0.00377784]\n"," [ 0.00565   ]]\n","Iter:  1066 loss =  0.03271213998008761 learning rate =  0.5 update =  [[-0.00377446]\n"," [-0.00377441]\n"," [ 0.00564489]]\n","Iter:  1067 loss =  0.03268197483415724 learning rate =  0.5 update =  [[-0.00377104]\n"," [-0.003771  ]\n"," [ 0.0056398 ]]\n","Iter:  1068 loss =  0.0326518642036277 learning rate =  0.5 update =  [[-0.00376763]\n"," [-0.00376758]\n"," [ 0.00563471]]\n","Iter:  1069 loss =  0.03262180794262107 learning rate =  0.5 update =  [[-0.00376422]\n"," [-0.00376418]\n"," [ 0.00562963]]\n","Iter:  1070 loss =  0.032591805905775054 learning rate =  0.5 update =  [[-0.00376082]\n"," [-0.00376078]\n"," [ 0.00562456]]\n","Iter:  1071 loss =  0.032561857948240694 learning rate =  0.5 update =  [[-0.00375743]\n"," [-0.00375738]\n"," [ 0.0056195 ]]\n","Iter:  1072 loss =  0.032531963925680446 learning rate =  0.5 update =  [[-0.00375404]\n"," [-0.00375399]\n"," [ 0.00561445]]\n","Iter:  1073 loss =  0.03250212369426539 learning rate =  0.5 update =  [[-0.00375065]\n"," [-0.00375061]\n"," [ 0.00560941]]\n","Iter:  1074 loss =  0.03247233711067359 learning rate =  0.5 update =  [[-0.00374728]\n"," [-0.00374723]\n"," [ 0.00560438]]\n","Iter:  1075 loss =  0.03244260403208746 learning rate =  0.5 update =  [[-0.00374391]\n"," [-0.00374386]\n"," [ 0.00559935]]\n","Iter:  1076 loss =  0.03241292431619174 learning rate =  0.5 update =  [[-0.00374054]\n"," [-0.0037405 ]\n"," [ 0.00559434]]\n","Iter:  1077 loss =  0.032383297821171365 learning rate =  0.5 update =  [[-0.00373718]\n"," [-0.00373714]\n"," [ 0.00558933]]\n","Iter:  1078 loss =  0.032353724405709 learning rate =  0.5 update =  [[-0.00373383]\n"," [-0.00373379]\n"," [ 0.00558433]]\n","Iter:  1079 loss =  0.03232420392898322 learning rate =  0.5 update =  [[-0.00373048]\n"," [-0.00373044]\n"," [ 0.00557934]]\n","Iter:  1080 loss =  0.032294736250666006 learning rate =  0.5 update =  [[-0.00372714]\n"," [-0.0037271 ]\n"," [ 0.00557436]]\n","Iter:  1081 loss =  0.0322653212309211 learning rate =  0.5 update =  [[-0.0037238 ]\n"," [-0.00372376]\n"," [ 0.00556939]]\n","Iter:  1082 loss =  0.03223595873040116 learning rate =  0.5 update =  [[-0.00372047]\n"," [-0.00372043]\n"," [ 0.00556443]]\n","Iter:  1083 loss =  0.03220664861024626 learning rate =  0.5 update =  [[-0.00371715]\n"," [-0.00371711]\n"," [ 0.00555947]]\n","Iter:  1084 loss =  0.03217739073208156 learning rate =  0.5 update =  [[-0.00371383]\n"," [-0.00371379]\n"," [ 0.00555453]]\n","Iter:  1085 loss =  0.032148184958015025 learning rate =  0.5 update =  [[-0.00371052]\n"," [-0.00371048]\n"," [ 0.00554959]]\n","Iter:  1086 loss =  0.03211903115063551 learning rate =  0.5 update =  [[-0.00370721]\n"," [-0.00370717]\n"," [ 0.00554466]]\n","Iter:  1087 loss =  0.03208992917301079 learning rate =  0.5 update =  [[-0.00370391]\n"," [-0.00370387]\n"," [ 0.00553974]]\n","Iter:  1088 loss =  0.032060878888685355 learning rate =  0.5 update =  [[-0.00370062]\n"," [-0.00370058]\n"," [ 0.00553483]]\n","Iter:  1089 loss =  0.032031880161678114 learning rate =  0.5 update =  [[-0.00369733]\n"," [-0.00369729]\n"," [ 0.00552993]]\n","Iter:  1090 loss =  0.03200293285648076 learning rate =  0.5 update =  [[-0.00369405]\n"," [-0.00369401]\n"," [ 0.00552503]]\n","Iter:  1091 loss =  0.03197403683805562 learning rate =  0.5 update =  [[-0.00369077]\n"," [-0.00369073]\n"," [ 0.00552015]]\n","Iter:  1092 loss =  0.031945191971833334 learning rate =  0.5 update =  [[-0.0036875 ]\n"," [-0.00368746]\n"," [ 0.00551527]]\n","Iter:  1093 loss =  0.03191639812371125 learning rate =  0.5 update =  [[-0.00368423]\n"," [-0.00368419]\n"," [ 0.0055104 ]]\n","Iter:  1094 loss =  0.031887655160051265 learning rate =  0.5 update =  [[-0.00368097]\n"," [-0.00368093]\n"," [ 0.00550554]]\n","Iter:  1095 loss =  0.031858962947677406 learning rate =  0.5 update =  [[-0.00367772]\n"," [-0.00367768]\n"," [ 0.00550069]]\n","Iter:  1096 loss =  0.03183032135387487 learning rate =  0.5 update =  [[-0.00367447]\n"," [-0.00367443]\n"," [ 0.00549584]]\n","Iter:  1097 loss =  0.031801730246386675 learning rate =  0.5 update =  [[-0.00367122]\n"," [-0.00367118]\n"," [ 0.00549101]]\n","Iter:  1098 loss =  0.03177318949341297 learning rate =  0.5 update =  [[-0.00366799]\n"," [-0.00366795]\n"," [ 0.00548618]]\n","Iter:  1099 loss =  0.031744698963608375 learning rate =  0.5 update =  [[-0.00366475]\n"," [-0.00366471]\n"," [ 0.00548136]]\n","Iter:  1100 loss =  0.03171625852608003 learning rate =  0.5 update =  [[-0.00366153]\n"," [-0.00366149]\n"," [ 0.00547655]]\n","Iter:  1101 loss =  0.03168786805038601 learning rate =  0.5 update =  [[-0.00365831]\n"," [-0.00365827]\n"," [ 0.00547175]]\n","Iter:  1102 loss =  0.031659527406533036 learning rate =  0.5 update =  [[-0.00365509]\n"," [-0.00365505]\n"," [ 0.00546696]]\n","Iter:  1103 loss =  0.031631236464974954 learning rate =  0.5 update =  [[-0.00365188]\n"," [-0.00365184]\n"," [ 0.00546217]]\n","Iter:  1104 loss =  0.03160299509661035 learning rate =  0.5 update =  [[-0.00364868]\n"," [-0.00364864]\n"," [ 0.00545739]]\n","Iter:  1105 loss =  0.03157480317278103 learning rate =  0.5 update =  [[-0.00364548]\n"," [-0.00364544]\n"," [ 0.00545262]]\n","Iter:  1106 loss =  0.03154666056526996 learning rate =  0.5 update =  [[-0.00364228]\n"," [-0.00364225]\n"," [ 0.00544786]]\n","Iter:  1107 loss =  0.031518567146299525 learning rate =  0.5 update =  [[-0.00363909]\n"," [-0.00363906]\n"," [ 0.00544311]]\n","Iter:  1108 loss =  0.03149052278852945 learning rate =  0.5 update =  [[-0.00363591]\n"," [-0.00363588]\n"," [ 0.00543837]]\n","Iter:  1109 loss =  0.031462527365054976 learning rate =  0.5 update =  [[-0.00363274]\n"," [-0.0036327 ]\n"," [ 0.00543363]]\n","Iter:  1110 loss =  0.03143458074940548 learning rate =  0.5 update =  [[-0.00362956]\n"," [-0.00362953]\n"," [ 0.0054289 ]]\n","Iter:  1111 loss =  0.03140668281554164 learning rate =  0.5 update =  [[-0.0036264 ]\n"," [-0.00362636]\n"," [ 0.00542418]]\n","Iter:  1112 loss =  0.03137883343785484 learning rate =  0.5 update =  [[-0.00362324]\n"," [-0.0036232 ]\n"," [ 0.00541947]]\n","Iter:  1113 loss =  0.03135103249116426 learning rate =  0.5 update =  [[-0.00362008]\n"," [-0.00362005]\n"," [ 0.00541476]]\n","Iter:  1114 loss =  0.031323279850715736 learning rate =  0.5 update =  [[-0.00361693]\n"," [-0.0036169 ]\n"," [ 0.00541007]]\n","Iter:  1115 loss =  0.0312955753921795 learning rate =  0.5 update =  [[-0.00361379]\n"," [-0.00361375]\n"," [ 0.00540538]]\n","Iter:  1116 loss =  0.03126791899164901 learning rate =  0.5 update =  [[-0.00361065]\n"," [-0.00361061]\n"," [ 0.0054007 ]]\n","Iter:  1117 loss =  0.031240310525638526 learning rate =  0.5 update =  [[-0.00360751]\n"," [-0.00360748]\n"," [ 0.00539603]]\n","Iter:  1118 loss =  0.031212749871081587 learning rate =  0.5 update =  [[-0.00360439]\n"," [-0.00360435]\n"," [ 0.00539136]]\n","Iter:  1119 loss =  0.031185236905329324 learning rate =  0.5 update =  [[-0.00360126]\n"," [-0.00360123]\n"," [ 0.00538671]]\n","Iter:  1120 loss =  0.03115777150614854 learning rate =  0.5 update =  [[-0.00359814]\n"," [-0.00359811]\n"," [ 0.00538206]]\n","Iter:  1121 loss =  0.031130353551720134 learning rate =  0.5 update =  [[-0.00359503]\n"," [-0.003595  ]\n"," [ 0.00537742]]\n","Iter:  1122 loss =  0.031102982920637113 learning rate =  0.5 update =  [[-0.00359193]\n"," [-0.00359189]\n"," [ 0.00537279]]\n","Iter:  1123 loss =  0.031075659491903253 learning rate =  0.5 update =  [[-0.00358882]\n"," [-0.00358879]\n"," [ 0.00536816]]\n","Iter:  1124 loss =  0.031048383144930917 learning rate =  0.5 update =  [[-0.00358573]\n"," [-0.00358569]\n"," [ 0.00536354]]\n","Iter:  1125 loss =  0.03102115375953962 learning rate =  0.5 update =  [[-0.00358264]\n"," [-0.0035826 ]\n"," [ 0.00535893]]\n","Iter:  1126 loss =  0.030993971215954402 learning rate =  0.5 update =  [[-0.00357955]\n"," [-0.00357952]\n"," [ 0.00535433]]\n","Iter:  1127 loss =  0.03096683539480369 learning rate =  0.5 update =  [[-0.00357647]\n"," [-0.00357643]\n"," [ 0.00534974]]\n","Iter:  1128 loss =  0.03093974617711825 learning rate =  0.5 update =  [[-0.00357339]\n"," [-0.00357336]\n"," [ 0.00534515]]\n","Iter:  1129 loss =  0.030912703444328934 learning rate =  0.5 update =  [[-0.00357032]\n"," [-0.00357029]\n"," [ 0.00534057]]\n","Iter:  1130 loss =  0.030885707078265302 learning rate =  0.5 update =  [[-0.00356726]\n"," [-0.00356722]\n"," [ 0.005336  ]]\n","Iter:  1131 loss =  0.030858756961153914 learning rate =  0.5 update =  [[-0.0035642 ]\n"," [-0.00356416]\n"," [ 0.00533144]]\n","Iter:  1132 loss =  0.030831852975616608 learning rate =  0.5 update =  [[-0.00356114]\n"," [-0.00356111]\n"," [ 0.00532688]]\n","Iter:  1133 loss =  0.03080499500466899 learning rate =  0.5 update =  [[-0.00355809]\n"," [-0.00355806]\n"," [ 0.00532234]]\n","Iter:  1134 loss =  0.03077818293171852 learning rate =  0.5 update =  [[-0.00355505]\n"," [-0.00355501]\n"," [ 0.0053178 ]]\n","Iter:  1135 loss =  0.030751416640563342 learning rate =  0.5 update =  [[-0.00355201]\n"," [-0.00355197]\n"," [ 0.00531326]]\n","Iter:  1136 loss =  0.030724696015390122 learning rate =  0.5 update =  [[-0.00354897]\n"," [-0.00354894]\n"," [ 0.00530874]]\n","Iter:  1137 loss =  0.030698020940772786 learning rate =  0.5 update =  [[-0.00354594]\n"," [-0.00354591]\n"," [ 0.00530422]]\n","Iter:  1138 loss =  0.03067139130167096 learning rate =  0.5 update =  [[-0.00354292]\n"," [-0.00354289]\n"," [ 0.00529971]]\n","Iter:  1139 loss =  0.03064480698342794 learning rate =  0.5 update =  [[-0.0035399 ]\n"," [-0.00353987]\n"," [ 0.00529521]]\n","Iter:  1140 loss =  0.03061826787176964 learning rate =  0.5 update =  [[-0.00353688]\n"," [-0.00353685]\n"," [ 0.00529072]]\n","Iter:  1141 loss =  0.030591773852802644 learning rate =  0.5 update =  [[-0.00353388]\n"," [-0.00353384]\n"," [ 0.00528623]]\n","Iter:  1142 loss =  0.030565324813012766 learning rate =  0.5 update =  [[-0.00353087]\n"," [-0.00353084]\n"," [ 0.00528175]]\n","Iter:  1143 loss =  0.030538920639263556 learning rate =  0.5 update =  [[-0.00352787]\n"," [-0.00352784]\n"," [ 0.00527728]]\n","Iter:  1144 loss =  0.030512561218794476 learning rate =  0.5 update =  [[-0.00352488]\n"," [-0.00352485]\n"," [ 0.00527281]]\n","Iter:  1145 loss =  0.030486246439219718 learning rate =  0.5 update =  [[-0.00352189]\n"," [-0.00352186]\n"," [ 0.00526835]]\n","Iter:  1146 loss =  0.030459976188526187 learning rate =  0.5 update =  [[-0.0035189 ]\n"," [-0.00351887]\n"," [ 0.0052639 ]]\n","Iter:  1147 loss =  0.030433750355072543 learning rate =  0.5 update =  [[-0.00351593]\n"," [-0.0035159 ]\n"," [ 0.00525946]]\n","Iter:  1148 loss =  0.03040756882758716 learning rate =  0.5 update =  [[-0.00351295]\n"," [-0.00351292]\n"," [ 0.00525503]]\n","Iter:  1149 loss =  0.030381431495166887 learning rate =  0.5 update =  [[-0.00350998]\n"," [-0.00350995]\n"," [ 0.0052506 ]]\n","Iter:  1150 loss =  0.030355338247275508 learning rate =  0.5 update =  [[-0.00350702]\n"," [-0.00350699]\n"," [ 0.00524618]]\n","Iter:  1151 loss =  0.030329288973742034 learning rate =  0.5 update =  [[-0.00350406]\n"," [-0.00350403]\n"," [ 0.00524176]]\n","Iter:  1152 loss =  0.03030328356475943 learning rate =  0.5 update =  [[-0.0035011 ]\n"," [-0.00350107]\n"," [ 0.00523736]]\n","Iter:  1153 loss =  0.03027732191088319 learning rate =  0.5 update =  [[-0.00349815]\n"," [-0.00349812]\n"," [ 0.00523296]]\n","Iter:  1154 loss =  0.03025140390302939 learning rate =  0.5 update =  [[-0.00349521]\n"," [-0.00349518]\n"," [ 0.00522857]]\n","Iter:  1155 loss =  0.030225529432473877 learning rate =  0.5 update =  [[-0.00349227]\n"," [-0.00349224]\n"," [ 0.00522418]]\n","Iter:  1156 loss =  0.03019969839085019 learning rate =  0.5 update =  [[-0.00348933]\n"," [-0.0034893 ]\n"," [ 0.00521981]]\n","Iter:  1157 loss =  0.03017391067014839 learning rate =  0.5 update =  [[-0.0034864 ]\n"," [-0.00348637]\n"," [ 0.00521544]]\n","Iter:  1158 loss =  0.030148166162713808 learning rate =  0.5 update =  [[-0.00348348]\n"," [-0.00348345]\n"," [ 0.00521107]]\n","Iter:  1159 loss =  0.030122464761245027 learning rate =  0.5 update =  [[-0.00348056]\n"," [-0.00348053]\n"," [ 0.00520672]]\n","Iter:  1160 loss =  0.03009680635879293 learning rate =  0.5 update =  [[-0.00347764]\n"," [-0.00347761]\n"," [ 0.00520237]]\n","Iter:  1161 loss =  0.03007119084875918 learning rate =  0.5 update =  [[-0.00347473]\n"," [-0.0034747 ]\n"," [ 0.00519803]]\n","Iter:  1162 loss =  0.03004561812489457 learning rate =  0.5 update =  [[-0.00347182]\n"," [-0.0034718 ]\n"," [ 0.00519369]]\n","Iter:  1163 loss =  0.03002008808129783 learning rate =  0.5 update =  [[-0.00346892]\n"," [-0.0034689 ]\n"," [ 0.00518937]]\n","Iter:  1164 loss =  0.029994600612414154 learning rate =  0.5 update =  [[-0.00346603]\n"," [-0.003466  ]\n"," [ 0.00518505]]\n","Iter:  1165 loss =  0.029969155613033677 learning rate =  0.5 update =  [[-0.00346314]\n"," [-0.00346311]\n"," [ 0.00518073]]\n","Iter:  1166 loss =  0.02994375297829021 learning rate =  0.5 update =  [[-0.00346025]\n"," [-0.00346022]\n"," [ 0.00517643]]\n","Iter:  1167 loss =  0.029918392603659978 learning rate =  0.5 update =  [[-0.00345737]\n"," [-0.00345734]\n"," [ 0.00517213]]\n","Iter:  1168 loss =  0.029893074384959684 learning rate =  0.5 update =  [[-0.00345449]\n"," [-0.00345446]\n"," [ 0.00516784]]\n","Iter:  1169 loss =  0.02986779821834586 learning rate =  0.5 update =  [[-0.00345161]\n"," [-0.00345159]\n"," [ 0.00516355]]\n","Iter:  1170 loss =  0.029842564000312936 learning rate =  0.5 update =  [[-0.00344875]\n"," [-0.00344872]\n"," [ 0.00515928]]\n","Iter:  1171 loss =  0.029817371627692206 learning rate =  0.5 update =  [[-0.00344588]\n"," [-0.00344586]\n"," [ 0.005155  ]]\n","Iter:  1172 loss =  0.02979222099765013 learning rate =  0.5 update =  [[-0.00344302]\n"," [-0.003443  ]\n"," [ 0.00515074]]\n","Iter:  1173 loss =  0.029767112007687543 learning rate =  0.5 update =  [[-0.00344017]\n"," [-0.00344014]\n"," [ 0.00514648]]\n","Iter:  1174 loss =  0.029742044555637615 learning rate =  0.5 update =  [[-0.00343732]\n"," [-0.00343729]\n"," [ 0.00514223]]\n","Iter:  1175 loss =  0.029717018539665012 learning rate =  0.5 update =  [[-0.00343448]\n"," [-0.00343445]\n"," [ 0.00513799]]\n","Iter:  1176 loss =  0.02969203385826416 learning rate =  0.5 update =  [[-0.00343163]\n"," [-0.00343161]\n"," [ 0.00513375]]\n","Iter:  1177 loss =  0.02966709041025864 learning rate =  0.5 update =  [[-0.0034288 ]\n"," [-0.00342877]\n"," [ 0.00512952]]\n","Iter:  1178 loss =  0.02964218809479885 learning rate =  0.5 update =  [[-0.00342597]\n"," [-0.00342594]\n"," [ 0.0051253 ]]\n","Iter:  1179 loss =  0.029617326811361654 learning rate =  0.5 update =  [[-0.00342314]\n"," [-0.00342312]\n"," [ 0.00512109]]\n","Iter:  1180 loss =  0.02959250645974829 learning rate =  0.5 update =  [[-0.00342032]\n"," [-0.00342029]\n"," [ 0.00511688]]\n","Iter:  1181 loss =  0.029567726940083614 learning rate =  0.5 update =  [[-0.0034175 ]\n"," [-0.00341748]\n"," [ 0.00511268]]\n","Iter:  1182 loss =  0.029542988152814635 learning rate =  0.5 update =  [[-0.00341469]\n"," [-0.00341466]\n"," [ 0.00510848]]\n","Iter:  1183 loss =  0.029518289998709007 learning rate =  0.5 update =  [[-0.00341188]\n"," [-0.00341186]\n"," [ 0.00510429]]\n","Iter:  1184 loss =  0.029493632378854107 learning rate =  0.5 update =  [[-0.00340908]\n"," [-0.00340905]\n"," [ 0.00510011]]\n","Iter:  1185 loss =  0.02946901519465548 learning rate =  0.5 update =  [[-0.00340628]\n"," [-0.00340625]\n"," [ 0.00509593]]\n","Iter:  1186 loss =  0.029444438347835777 learning rate =  0.5 update =  [[-0.00340348]\n"," [-0.00340346]\n"," [ 0.00509177]]\n","Iter:  1187 loss =  0.029419901740433324 learning rate =  0.5 update =  [[-0.00340069]\n"," [-0.00340067]\n"," [ 0.0050876 ]]\n","Iter:  1188 loss =  0.0293954052748009 learning rate =  0.5 update =  [[-0.00339791]\n"," [-0.00339788]\n"," [ 0.00508345]]\n","Iter:  1189 loss =  0.029370948853604497 learning rate =  0.5 update =  [[-0.00339513]\n"," [-0.0033951 ]\n"," [ 0.0050793 ]]\n","Iter:  1190 loss =  0.029346532379822154 learning rate =  0.5 update =  [[-0.00339235]\n"," [-0.00339233]\n"," [ 0.00507516]]\n","Iter:  1191 loss =  0.02932215575674265 learning rate =  0.5 update =  [[-0.00338958]\n"," [-0.00338955]\n"," [ 0.00507102]]\n","Iter:  1192 loss =  0.029297818887964056 learning rate =  0.5 update =  [[-0.00338681]\n"," [-0.00338679]\n"," [ 0.0050669 ]]\n","Iter:  1193 loss =  0.029273521677393072 learning rate =  0.5 update =  [[-0.00338405]\n"," [-0.00338402]\n"," [ 0.00506277]]\n","Iter:  1194 loss =  0.029249264029243 learning rate =  0.5 update =  [[-0.00338129]\n"," [-0.00338126]\n"," [ 0.00505866]]\n","Iter:  1195 loss =  0.02922504584803332 learning rate =  0.5 update =  [[-0.00337853]\n"," [-0.00337851]\n"," [ 0.00505455]]\n","Iter:  1196 loss =  0.029200867038588103 learning rate =  0.5 update =  [[-0.00337578]\n"," [-0.00337576]\n"," [ 0.00505045]]\n","Iter:  1197 loss =  0.029176727506034547 learning rate =  0.5 update =  [[-0.00337304]\n"," [-0.00337301]\n"," [ 0.00504635]]\n","Iter:  1198 loss =  0.029152627155802323 learning rate =  0.5 update =  [[-0.0033703 ]\n"," [-0.00337027]\n"," [ 0.00504226]]\n","Iter:  1199 loss =  0.029128565893621942 learning rate =  0.5 update =  [[-0.00336756]\n"," [-0.00336754]\n"," [ 0.00503818]]\n","Iter:  1200 loss =  0.02910454362552392 learning rate =  0.5 update =  [[-0.00336483]\n"," [-0.0033648 ]\n"," [ 0.0050341 ]]\n","Iter:  1201 loss =  0.029080560257837125 learning rate =  0.5 update =  [[-0.0033621 ]\n"," [-0.00336208]\n"," [ 0.00503003]]\n","Iter:  1202 loss =  0.02905661569718823 learning rate =  0.5 update =  [[-0.00335937]\n"," [-0.00335935]\n"," [ 0.00502597]]\n","Iter:  1203 loss =  0.02903270985049989 learning rate =  0.5 update =  [[-0.00335666]\n"," [-0.00335663]\n"," [ 0.00502191]]\n","Iter:  1204 loss =  0.029008842624990053 learning rate =  0.5 update =  [[-0.00335394]\n"," [-0.00335392]\n"," [ 0.00501786]]\n","Iter:  1205 loss =  0.028985013928170458 learning rate =  0.5 update =  [[-0.00335123]\n"," [-0.00335121]\n"," [ 0.00501382]]\n","Iter:  1206 loss =  0.028961223667845974 learning rate =  0.5 update =  [[-0.00334852]\n"," [-0.0033485 ]\n"," [ 0.00500978]]\n","Iter:  1207 loss =  0.028937471752112608 learning rate =  0.5 update =  [[-0.00334582]\n"," [-0.0033458 ]\n"," [ 0.00500575]]\n","Iter:  1208 loss =  0.02891375808935729 learning rate =  0.5 update =  [[-0.00334312]\n"," [-0.0033431 ]\n"," [ 0.00500173]]\n","Iter:  1209 loss =  0.028890082588256115 learning rate =  0.5 update =  [[-0.00334043]\n"," [-0.00334041]\n"," [ 0.00499771]]\n","Iter:  1210 loss =  0.02886644515777346 learning rate =  0.5 update =  [[-0.00333774]\n"," [-0.00333772]\n"," [ 0.0049937 ]]\n","Iter:  1211 loss =  0.028842845707160836 learning rate =  0.5 update =  [[-0.00333505]\n"," [-0.00333503]\n"," [ 0.00498969]]\n","Iter:  1212 loss =  0.028819284145955595 learning rate =  0.5 update =  [[-0.00333237]\n"," [-0.00333235]\n"," [ 0.00498569]]\n","Iter:  1213 loss =  0.02879576038398004 learning rate =  0.5 update =  [[-0.0033297 ]\n"," [-0.00332967]\n"," [ 0.0049817 ]]\n","Iter:  1214 loss =  0.02877227433134015 learning rate =  0.5 update =  [[-0.00332702]\n"," [-0.003327  ]\n"," [ 0.00497771]]\n","Iter:  1215 loss =  0.02874882589842465 learning rate =  0.5 update =  [[-0.00332436]\n"," [-0.00332433]\n"," [ 0.00497373]]\n","Iter:  1216 loss =  0.02872541499590366 learning rate =  0.5 update =  [[-0.00332169]\n"," [-0.00332167]\n"," [ 0.00496976]]\n","Iter:  1217 loss =  0.028702041534727753 learning rate =  0.5 update =  [[-0.00331903]\n"," [-0.00331901]\n"," [ 0.00496579]]\n","Iter:  1218 loss =  0.028678705426126896 learning rate =  0.5 update =  [[-0.00331638]\n"," [-0.00331635]\n"," [ 0.00496183]]\n","Iter:  1219 loss =  0.028655406581609186 learning rate =  0.5 update =  [[-0.00331372]\n"," [-0.0033137 ]\n"," [ 0.00495787]]\n","Iter:  1220 loss =  0.028632144912960115 learning rate =  0.5 update =  [[-0.00331108]\n"," [-0.00331106]\n"," [ 0.00495392]]\n","Iter:  1221 loss =  0.028608920332240897 learning rate =  0.5 update =  [[-0.00330843]\n"," [-0.00330841]\n"," [ 0.00494998]]\n","Iter:  1222 loss =  0.02858573275178812 learning rate =  0.5 update =  [[-0.0033058 ]\n"," [-0.00330577]\n"," [ 0.00494604]]\n","Iter:  1223 loss =  0.02856258208421203 learning rate =  0.5 update =  [[-0.00330316]\n"," [-0.00330314]\n"," [ 0.00494211]]\n","Iter:  1224 loss =  0.02853946824239592 learning rate =  0.5 update =  [[-0.00330053]\n"," [-0.00330051]\n"," [ 0.00493819]]\n","Iter:  1225 loss =  0.02851639113949498 learning rate =  0.5 update =  [[-0.0032979 ]\n"," [-0.00329788]\n"," [ 0.00493427]]\n","Iter:  1226 loss =  0.0284933506889348 learning rate =  0.5 update =  [[-0.00329528]\n"," [-0.00329526]\n"," [ 0.00493036]]\n","Iter:  1227 loss =  0.02847034680441117 learning rate =  0.5 update =  [[-0.00329266]\n"," [-0.00329264]\n"," [ 0.00492645]]\n","Iter:  1228 loss =  0.028447379399888145 learning rate =  0.5 update =  [[-0.00329005]\n"," [-0.00329003]\n"," [ 0.00492255]]\n","Iter:  1229 loss =  0.028424448389597675 learning rate =  0.5 update =  [[-0.00328744]\n"," [-0.00328742]\n"," [ 0.00491866]]\n","Iter:  1230 loss =  0.028401553688038206 learning rate =  0.5 update =  [[-0.00328483]\n"," [-0.00328481]\n"," [ 0.00491477]]\n","Iter:  1231 loss =  0.028378695209973742 learning rate =  0.5 update =  [[-0.00328223]\n"," [-0.00328221]\n"," [ 0.00491089]]\n","Iter:  1232 loss =  0.028355872870432945 learning rate =  0.5 update =  [[-0.00327963]\n"," [-0.00327961]\n"," [ 0.00490701]]\n","Iter:  1233 loss =  0.02833308658470795 learning rate =  0.5 update =  [[-0.00327704]\n"," [-0.00327702]\n"," [ 0.00490314]]\n","Iter:  1234 loss =  0.02831033626835327 learning rate =  0.5 update =  [[-0.00327445]\n"," [-0.00327443]\n"," [ 0.00489928]]\n","Iter:  1235 loss =  0.02828762183718514 learning rate =  0.5 update =  [[-0.00327186]\n"," [-0.00327184]\n"," [ 0.00489542]]\n","Iter:  1236 loss =  0.028264943207280187 learning rate =  0.5 update =  [[-0.00326928]\n"," [-0.00326926]\n"," [ 0.00489157]]\n","Iter:  1237 loss =  0.028242300294974515 learning rate =  0.5 update =  [[-0.0032667 ]\n"," [-0.00326668]\n"," [ 0.00488772]]\n","Iter:  1238 loss =  0.028219693016862786 learning rate =  0.5 update =  [[-0.00326413]\n"," [-0.00326411]\n"," [ 0.00488388]]\n","Iter:  1239 loss =  0.028197121289797067 learning rate =  0.5 update =  [[-0.00326156]\n"," [-0.00326154]\n"," [ 0.00488004]]\n","Iter:  1240 loss =  0.028174585030886032 learning rate =  0.5 update =  [[-0.00325899]\n"," [-0.00325897]\n"," [ 0.00487622]]\n","Iter:  1241 loss =  0.028152084157493984 learning rate =  0.5 update =  [[-0.00325643]\n"," [-0.00325641]\n"," [ 0.00487239]]\n","Iter:  1242 loss =  0.02812961858723957 learning rate =  0.5 update =  [[-0.00325387]\n"," [-0.00325385]\n"," [ 0.00486858]]\n","Iter:  1243 loss =  0.02810718823799524 learning rate =  0.5 update =  [[-0.00325132]\n"," [-0.0032513 ]\n"," [ 0.00486477]]\n","Iter:  1244 loss =  0.028084793027885822 learning rate =  0.5 update =  [[-0.00324877]\n"," [-0.00324875]\n"," [ 0.00486096]]\n","Iter:  1245 loss =  0.02806243287528791 learning rate =  0.5 update =  [[-0.00324622]\n"," [-0.0032462 ]\n"," [ 0.00485716]]\n","Iter:  1246 loss =  0.02804010769882902 learning rate =  0.5 update =  [[-0.00324368]\n"," [-0.00324366]\n"," [ 0.00485337]]\n","Iter:  1247 loss =  0.02801781741738587 learning rate =  0.5 update =  [[-0.00324114]\n"," [-0.00324112]\n"," [ 0.00484958]]\n","Iter:  1248 loss =  0.027995561950084614 learning rate =  0.5 update =  [[-0.00323861]\n"," [-0.00323859]\n"," [ 0.0048458 ]]\n","Iter:  1249 loss =  0.027973341216298686 learning rate =  0.5 update =  [[-0.00323608]\n"," [-0.00323606]\n"," [ 0.00484202]]\n","Iter:  1250 loss =  0.027951155135648595 learning rate =  0.5 update =  [[-0.00323355]\n"," [-0.00323353]\n"," [ 0.00483826]]\n","Iter:  1251 loss =  0.027929003628000965 learning rate =  0.5 update =  [[-0.00323103]\n"," [-0.00323101]\n"," [ 0.00483449]]\n","Iter:  1252 loss =  0.027906886613467255 learning rate =  0.5 update =  [[-0.00322851]\n"," [-0.00322849]\n"," [ 0.00483073]]\n","Iter:  1253 loss =  0.027884804012403085 learning rate =  0.5 update =  [[-0.00322599]\n"," [-0.00322597]\n"," [ 0.00482698]]\n","Iter:  1254 loss =  0.02786275574540715 learning rate =  0.5 update =  [[-0.00322348]\n"," [-0.00322346]\n"," [ 0.00482323]]\n","Iter:  1255 loss =  0.027840741733320473 learning rate =  0.5 update =  [[-0.00322097]\n"," [-0.00322096]\n"," [ 0.00481949]]\n","Iter:  1256 loss =  0.027818761897225392 learning rate =  0.5 update =  [[-0.00321847]\n"," [-0.00321845]\n"," [ 0.00481576]]\n","Iter:  1257 loss =  0.027796816158444593 learning rate =  0.5 update =  [[-0.00321597]\n"," [-0.00321595]\n"," [ 0.00481203]]\n","Iter:  1258 loss =  0.02777490443854034 learning rate =  0.5 update =  [[-0.00321348]\n"," [-0.00321346]\n"," [ 0.0048083 ]]\n","Iter:  1259 loss =  0.027753026659313403 learning rate =  0.5 update =  [[-0.00321098]\n"," [-0.00321097]\n"," [ 0.00480458]]\n","Iter:  1260 loss =  0.02773118274280233 learning rate =  0.5 update =  [[-0.0032085 ]\n"," [-0.00320848]\n"," [ 0.00480087]]\n","Iter:  1261 loss =  0.02770937261128234 learning rate =  0.5 update =  [[-0.00320601]\n"," [-0.00320599]\n"," [ 0.00479716]]\n","Iter:  1262 loss =  0.027687596187264733 learning rate =  0.5 update =  [[-0.00320353]\n"," [-0.00320351]\n"," [ 0.00479346]]\n","Iter:  1263 loss =  0.027665853393495587 learning rate =  0.5 update =  [[-0.00320105]\n"," [-0.00320104]\n"," [ 0.00478977]]\n","Iter:  1264 loss =  0.027644144152955387 learning rate =  0.5 update =  [[-0.00319858]\n"," [-0.00319856]\n"," [ 0.00478608]]\n","Iter:  1265 loss =  0.02762246838885756 learning rate =  0.5 update =  [[-0.00319611]\n"," [-0.00319609]\n"," [ 0.00478239]]\n","Iter:  1266 loss =  0.027600826024648098 learning rate =  0.5 update =  [[-0.00319365]\n"," [-0.00319363]\n"," [ 0.00477871]]\n","Iter:  1267 loss =  0.02757921698400448 learning rate =  0.5 update =  [[-0.00319118]\n"," [-0.00319117]\n"," [ 0.00477504]]\n","Iter:  1268 loss =  0.027557641190834682 learning rate =  0.5 update =  [[-0.00318873]\n"," [-0.00318871]\n"," [ 0.00477137]]\n","Iter:  1269 loss =  0.027536098569276628 learning rate =  0.5 update =  [[-0.00318627]\n"," [-0.00318625]\n"," [ 0.00476771]]\n","Iter:  1270 loss =  0.027514589043696927 learning rate =  0.5 update =  [[-0.00318382]\n"," [-0.0031838 ]\n"," [ 0.00476405]]\n","Iter:  1271 loss =  0.02749311253869032 learning rate =  0.5 update =  [[-0.00318137]\n"," [-0.00318136]\n"," [ 0.0047604 ]]\n","Iter:  1272 loss =  0.027471668979078788 learning rate =  0.5 update =  [[-0.00317893]\n"," [-0.00317891]\n"," [ 0.00475675]]\n","Iter:  1273 loss =  0.02745025828991063 learning rate =  0.5 update =  [[-0.00317649]\n"," [-0.00317647]\n"," [ 0.00475311]]\n","Iter:  1274 loss =  0.027428880396459517 learning rate =  0.5 update =  [[-0.00317405]\n"," [-0.00317404]\n"," [ 0.00474948]]\n","Iter:  1275 loss =  0.02740753522422389 learning rate =  0.5 update =  [[-0.00317162]\n"," [-0.00317161]\n"," [ 0.00474585]]\n","Iter:  1276 loss =  0.02738622269892598 learning rate =  0.5 update =  [[-0.00316919]\n"," [-0.00316918]\n"," [ 0.00474223]]\n","Iter:  1277 loss =  0.027364942746510898 learning rate =  0.5 update =  [[-0.00316677]\n"," [-0.00316675]\n"," [ 0.00473861]]\n","Iter:  1278 loss =  0.027343695293146023 learning rate =  0.5 update =  [[-0.00316435]\n"," [-0.00316433]\n"," [ 0.00473499]]\n","Iter:  1279 loss =  0.02732248026521998 learning rate =  0.5 update =  [[-0.00316193]\n"," [-0.00316191]\n"," [ 0.00473139]]\n","Iter:  1280 loss =  0.027301297589341893 learning rate =  0.5 update =  [[-0.00315952]\n"," [-0.0031595 ]\n"," [ 0.00472778]]\n","Iter:  1281 loss =  0.02728014719234052 learning rate =  0.5 update =  [[-0.00315711]\n"," [-0.00315709]\n"," [ 0.00472419]]\n","Iter:  1282 loss =  0.02725902900126346 learning rate =  0.5 update =  [[-0.0031547 ]\n"," [-0.00315468]\n"," [ 0.00472059]]\n","Iter:  1283 loss =  0.027237942943376423 learning rate =  0.5 update =  [[-0.0031523 ]\n"," [-0.00315228]\n"," [ 0.00471701]]\n","Iter:  1284 loss =  0.027216888946162324 learning rate =  0.5 update =  [[-0.0031499 ]\n"," [-0.00314988]\n"," [ 0.00471343]]\n","Iter:  1285 loss =  0.02719586693732046 learning rate =  0.5 update =  [[-0.0031475 ]\n"," [-0.00314748]\n"," [ 0.00470985]]\n","Iter:  1286 loss =  0.027174876844765862 learning rate =  0.5 update =  [[-0.00314511]\n"," [-0.00314509]\n"," [ 0.00470628]]\n","Iter:  1287 loss =  0.027153918596628163 learning rate =  0.5 update =  [[-0.00314272]\n"," [-0.0031427 ]\n"," [ 0.00470272]]\n","Iter:  1288 loss =  0.027132992121251212 learning rate =  0.5 update =  [[-0.00314033]\n"," [-0.00314032]\n"," [ 0.00469916]]\n","Iter:  1289 loss =  0.027112097347192134 learning rate =  0.5 update =  [[-0.00313795]\n"," [-0.00313794]\n"," [ 0.0046956 ]]\n","Iter:  1290 loss =  0.027091234203220266 learning rate =  0.5 update =  [[-0.00313557]\n"," [-0.00313556]\n"," [ 0.00469205]]\n","Iter:  1291 loss =  0.027070402618316815 learning rate =  0.5 update =  [[-0.0031332 ]\n"," [-0.00313318]\n"," [ 0.00468851]]\n","Iter:  1292 loss =  0.027049602521673714 learning rate =  0.5 update =  [[-0.00313083]\n"," [-0.00313081]\n"," [ 0.00468497]]\n","Iter:  1293 loss =  0.02702883384269327 learning rate =  0.5 update =  [[-0.00312846]\n"," [-0.00312845]\n"," [ 0.00468144]]\n","Iter:  1294 loss =  0.02700809651098668 learning rate =  0.5 update =  [[-0.0031261 ]\n"," [-0.00312608]\n"," [ 0.00467791]]\n","Iter:  1295 loss =  0.02698739045637415 learning rate =  0.5 update =  [[-0.00312374]\n"," [-0.00312372]\n"," [ 0.00467439]]\n","Iter:  1296 loss =  0.026966715608883373 learning rate =  0.5 update =  [[-0.00312138]\n"," [-0.00312136]\n"," [ 0.00467087]]\n","Iter:  1297 loss =  0.02694607189874922 learning rate =  0.5 update =  [[-0.00311903]\n"," [-0.00311901]\n"," [ 0.00466736]]\n","Iter:  1298 loss =  0.026925459256412695 learning rate =  0.5 update =  [[-0.00311668]\n"," [-0.00311666]\n"," [ 0.00466385]]\n","Iter:  1299 loss =  0.026904877612520528 learning rate =  0.5 update =  [[-0.00311433]\n"," [-0.00311432]\n"," [ 0.00466035]]\n","Iter:  1300 loss =  0.026884326897923885 learning rate =  0.5 update =  [[-0.00311199]\n"," [-0.00311197]\n"," [ 0.00465685]]\n","Iter:  1301 loss =  0.02686380704367824 learning rate =  0.5 update =  [[-0.00310965]\n"," [-0.00310963]\n"," [ 0.00465336]]\n","Iter:  1302 loss =  0.02684331798104217 learning rate =  0.5 update =  [[-0.00310731]\n"," [-0.0031073 ]\n"," [ 0.00464988]]\n","Iter:  1303 loss =  0.026822859641476827 learning rate =  0.5 update =  [[-0.00310498]\n"," [-0.00310496]\n"," [ 0.00464639]]\n","Iter:  1304 loss =  0.026802431956644925 learning rate =  0.5 update =  [[-0.00310265]\n"," [-0.00310264]\n"," [ 0.00464292]]\n","Iter:  1305 loss =  0.02678203485841054 learning rate =  0.5 update =  [[-0.00310032]\n"," [-0.00310031]\n"," [ 0.00463945]]\n","Iter:  1306 loss =  0.0267616682788378 learning rate =  0.5 update =  [[-0.003098  ]\n"," [-0.00309799]\n"," [ 0.00463598]]\n","Iter:  1307 loss =  0.026741332150190512 learning rate =  0.5 update =  [[-0.00309568]\n"," [-0.00309567]\n"," [ 0.00463252]]\n","Iter:  1308 loss =  0.02672102640493126 learning rate =  0.5 update =  [[-0.00309337]\n"," [-0.00309335]\n"," [ 0.00462906]]\n","Iter:  1309 loss =  0.02670075097572082 learning rate =  0.5 update =  [[-0.00309106]\n"," [-0.00309104]\n"," [ 0.00462561]]\n","Iter:  1310 loss =  0.026680505795417222 learning rate =  0.5 update =  [[-0.00308875]\n"," [-0.00308873]\n"," [ 0.00462217]]\n","Iter:  1311 loss =  0.026660290797075208 learning rate =  0.5 update =  [[-0.00308644]\n"," [-0.00308643]\n"," [ 0.00461873]]\n","Iter:  1312 loss =  0.026640105913945673 learning rate =  0.5 update =  [[-0.00308414]\n"," [-0.00308413]\n"," [ 0.00461529]]\n","Iter:  1313 loss =  0.026619951079474485 learning rate =  0.5 update =  [[-0.00308184]\n"," [-0.00308183]\n"," [ 0.00461186]]\n","Iter:  1314 loss =  0.026599826227302108 learning rate =  0.5 update =  [[-0.00307955]\n"," [-0.00307953]\n"," [ 0.00460844]]\n","Iter:  1315 loss =  0.026579731291262988 learning rate =  0.5 update =  [[-0.00307725]\n"," [-0.00307724]\n"," [ 0.00460502]]\n","Iter:  1316 loss =  0.026559666205384466 learning rate =  0.5 update =  [[-0.00307497]\n"," [-0.00307495]\n"," [ 0.0046016 ]]\n","Iter:  1317 loss =  0.026539630903886648 learning rate =  0.5 update =  [[-0.00307268]\n"," [-0.00307267]\n"," [ 0.00459819]]\n","Iter:  1318 loss =  0.0265196253211809 learning rate =  0.5 update =  [[-0.0030704 ]\n"," [-0.00307039]\n"," [ 0.00459478]]\n","Iter:  1319 loss =  0.026499649391869984 learning rate =  0.5 update =  [[-0.00306812]\n"," [-0.00306811]\n"," [ 0.00459138]]\n","Iter:  1320 loss =  0.026479703050746865 learning rate =  0.5 update =  [[-0.00306585]\n"," [-0.00306583]\n"," [ 0.00458799]]\n","Iter:  1321 loss =  0.02645978623279421 learning rate =  0.5 update =  [[-0.00306357]\n"," [-0.00306356]\n"," [ 0.0045846 ]]\n","Iter:  1322 loss =  0.026439898873183525 learning rate =  0.5 update =  [[-0.00306131]\n"," [-0.00306129]\n"," [ 0.00458121]]\n","Iter:  1323 loss =  0.026420040907274844 learning rate =  0.5 update =  [[-0.00305904]\n"," [-0.00305903]\n"," [ 0.00457783]]\n","Iter:  1324 loss =  0.026400212270615536 learning rate =  0.5 update =  [[-0.00305678]\n"," [-0.00305677]\n"," [ 0.00457446]]\n","Iter:  1325 loss =  0.026380412898940116 learning rate =  0.5 update =  [[-0.00305452]\n"," [-0.00305451]\n"," [ 0.00457108]]\n","Iter:  1326 loss =  0.026360642728169157 learning rate =  0.5 update =  [[-0.00305227]\n"," [-0.00305225]\n"," [ 0.00456772]]\n","Iter:  1327 loss =  0.02634090169440905 learning rate =  0.5 update =  [[-0.00305001]\n"," [-0.00305   ]\n"," [ 0.00456436]]\n","Iter:  1328 loss =  0.02632118973395089 learning rate =  0.5 update =  [[-0.00304776]\n"," [-0.00304775]\n"," [ 0.004561  ]]\n","Iter:  1329 loss =  0.026301506783270104 learning rate =  0.5 update =  [[-0.00304552]\n"," [-0.00304551]\n"," [ 0.00455765]]\n","Iter:  1330 loss =  0.026281852779025852 learning rate =  0.5 update =  [[-0.00304328]\n"," [-0.00304326]\n"," [ 0.0045543 ]]\n","Iter:  1331 loss =  0.026262227658059842 learning rate =  0.5 update =  [[-0.00304104]\n"," [-0.00304103]\n"," [ 0.00455096]]\n","Iter:  1332 loss =  0.02624263135739656 learning rate =  0.5 update =  [[-0.0030388 ]\n"," [-0.00303879]\n"," [ 0.00454762]]\n","Iter:  1333 loss =  0.026223063814241797 learning rate =  0.5 update =  [[-0.00303657]\n"," [-0.00303656]\n"," [ 0.00454429]]\n","Iter:  1334 loss =  0.026203524965982238 learning rate =  0.5 update =  [[-0.00303434]\n"," [-0.00303433]\n"," [ 0.00454096]]\n","Iter:  1335 loss =  0.026184014750185176 learning rate =  0.5 update =  [[-0.00303212]\n"," [-0.0030321 ]\n"," [ 0.00453764]]\n","Iter:  1336 loss =  0.026164533104597383 learning rate =  0.5 update =  [[-0.00302989]\n"," [-0.00302988]\n"," [ 0.00453432]]\n","Iter:  1337 loss =  0.026145079967144776 learning rate =  0.5 update =  [[-0.00302767]\n"," [-0.00302766]\n"," [ 0.00453101]]\n","Iter:  1338 loss =  0.02612565527593152 learning rate =  0.5 update =  [[-0.00302546]\n"," [-0.00302544]\n"," [ 0.0045277 ]]\n","Iter:  1339 loss =  0.026106258969239633 learning rate =  0.5 update =  [[-0.00302324]\n"," [-0.00302323]\n"," [ 0.0045244 ]]\n","Iter:  1340 loss =  0.026086890985528313 learning rate =  0.5 update =  [[-0.00302103]\n"," [-0.00302102]\n"," [ 0.0045211 ]]\n","Iter:  1341 loss =  0.026067551263433235 learning rate =  0.5 update =  [[-0.00301883]\n"," [-0.00301881]\n"," [ 0.00451781]]\n","Iter:  1342 loss =  0.026048239741765775 learning rate =  0.5 update =  [[-0.00301662]\n"," [-0.00301661]\n"," [ 0.00451452]]\n","Iter:  1343 loss =  0.02602895635951282 learning rate =  0.5 update =  [[-0.00301442]\n"," [-0.00301441]\n"," [ 0.00451123]]\n","Iter:  1344 loss =  0.02600970105583559 learning rate =  0.5 update =  [[-0.00301223]\n"," [-0.00301221]\n"," [ 0.00450795]]\n","Iter:  1345 loss =  0.02599047377006939 learning rate =  0.5 update =  [[-0.00301003]\n"," [-0.00301002]\n"," [ 0.00450468]]\n","Iter:  1346 loss =  0.025971274441723143 learning rate =  0.5 update =  [[-0.00300784]\n"," [-0.00300783]\n"," [ 0.00450141]]\n","Iter:  1347 loss =  0.02595210301047818 learning rate =  0.5 update =  [[-0.00300565]\n"," [-0.00300564]\n"," [ 0.00449814]]\n","Iter:  1348 loss =  0.025932959416188124 learning rate =  0.5 update =  [[-0.00300347]\n"," [-0.00300346]\n"," [ 0.00449488]]\n","Iter:  1349 loss =  0.025913843598878175 learning rate =  0.5 update =  [[-0.00300129]\n"," [-0.00300127]\n"," [ 0.00449162]]\n","Iter:  1350 loss =  0.025894755498744454 learning rate =  0.5 update =  [[-0.00299911]\n"," [-0.0029991 ]\n"," [ 0.00448837]]\n","Iter:  1351 loss =  0.02587569505615324 learning rate =  0.5 update =  [[-0.00299693]\n"," [-0.00299692]\n"," [ 0.00448512]]\n","Iter:  1352 loss =  0.025856662211640888 learning rate =  0.5 update =  [[-0.00299476]\n"," [-0.00299475]\n"," [ 0.00448188]]\n","Iter:  1353 loss =  0.025837656905912505 learning rate =  0.5 update =  [[-0.00299259]\n"," [-0.00299258]\n"," [ 0.00447864]]\n","Iter:  1354 loss =  0.025818679079841866 learning rate =  0.5 update =  [[-0.00299042]\n"," [-0.00299041]\n"," [ 0.00447541]]\n","Iter:  1355 loss =  0.02579972867447073 learning rate =  0.5 update =  [[-0.00298826]\n"," [-0.00298825]\n"," [ 0.00447218]]\n","Iter:  1356 loss =  0.02578080563100811 learning rate =  0.5 update =  [[-0.0029861 ]\n"," [-0.00298609]\n"," [ 0.00446896]]\n","Iter:  1357 loss =  0.025761909890829875 learning rate =  0.5 update =  [[-0.00298395]\n"," [-0.00298393]\n"," [ 0.00446574]]\n","Iter:  1358 loss =  0.025743041395477853 learning rate =  0.5 update =  [[-0.00298179]\n"," [-0.00298178]\n"," [ 0.00446252]]\n","Iter:  1359 loss =  0.025724200086659675 learning rate =  0.5 update =  [[-0.00297964]\n"," [-0.00297963]\n"," [ 0.00445931]]\n","Iter:  1360 loss =  0.02570538590624767 learning rate =  0.5 update =  [[-0.00297749]\n"," [-0.00297748]\n"," [ 0.00445611]]\n","Iter:  1361 loss =  0.025686598796279006 learning rate =  0.5 update =  [[-0.00297535]\n"," [-0.00297534]\n"," [ 0.0044529 ]]\n","Iter:  1362 loss =  0.025667838698954313 learning rate =  0.5 update =  [[-0.00297321]\n"," [-0.0029732 ]\n"," [ 0.00444971]]\n","Iter:  1363 loss =  0.025649105556637568 learning rate =  0.5 update =  [[-0.00297107]\n"," [-0.00297106]\n"," [ 0.00444651]]\n","Iter:  1364 loss =  0.025630399311855442 learning rate =  0.5 update =  [[-0.00296893]\n"," [-0.00296892]\n"," [ 0.00444333]]\n","Iter:  1365 loss =  0.025611719907296854 learning rate =  0.5 update =  [[-0.0029668 ]\n"," [-0.00296679]\n"," [ 0.00444014]]\n","Iter:  1366 loss =  0.025593067285812142 learning rate =  0.5 update =  [[-0.00296467]\n"," [-0.00296466]\n"," [ 0.00443696]]\n","Iter:  1367 loss =  0.02557444139041256 learning rate =  0.5 update =  [[-0.00296254]\n"," [-0.00296253]\n"," [ 0.00443379]]\n","Iter:  1368 loss =  0.025555842164269878 learning rate =  0.5 update =  [[-0.00296042]\n"," [-0.00296041]\n"," [ 0.00443062]]\n","Iter:  1369 loss =  0.025537269550715724 learning rate =  0.5 update =  [[-0.0029583 ]\n"," [-0.00295829]\n"," [ 0.00442745]]\n","Iter:  1370 loss =  0.025518723493241018 learning rate =  0.5 update =  [[-0.00295618]\n"," [-0.00295617]\n"," [ 0.00442429]]\n","Iter:  1371 loss =  0.025500203935495305 learning rate =  0.5 update =  [[-0.00295407]\n"," [-0.00295406]\n"," [ 0.00442114]]\n","Iter:  1372 loss =  0.025481710821286523 learning rate =  0.5 update =  [[-0.00295196]\n"," [-0.00295195]\n"," [ 0.00441798]]\n","Iter:  1373 loss =  0.02546324409458004 learning rate =  0.5 update =  [[-0.00294985]\n"," [-0.00294984]\n"," [ 0.00441484]]\n","Iter:  1374 loss =  0.02544480369949849 learning rate =  0.5 update =  [[-0.00294774]\n"," [-0.00294773]\n"," [ 0.00441169]]\n","Iter:  1375 loss =  0.025426389580320938 learning rate =  0.5 update =  [[-0.00294564]\n"," [-0.00294563]\n"," [ 0.00440855]]\n","Iter:  1376 loss =  0.02540800168148239 learning rate =  0.5 update =  [[-0.00294354]\n"," [-0.00294353]\n"," [ 0.00440542]]\n","Iter:  1377 loss =  0.02538963994757342 learning rate =  0.5 update =  [[-0.00294144]\n"," [-0.00294143]\n"," [ 0.00440229]]\n","Iter:  1378 loss =  0.025371304323339477 learning rate =  0.5 update =  [[-0.00293935]\n"," [-0.00293934]\n"," [ 0.00439916]]\n","Iter:  1379 loss =  0.025352994753680438 learning rate =  0.5 update =  [[-0.00293726]\n"," [-0.00293725]\n"," [ 0.00439604]]\n","Iter:  1380 loss =  0.025334711183649765 learning rate =  0.5 update =  [[-0.00293517]\n"," [-0.00293516]\n"," [ 0.00439293]]\n","Iter:  1381 loss =  0.02531645355845451 learning rate =  0.5 update =  [[-0.00293309]\n"," [-0.00293308]\n"," [ 0.00438981]]\n","Iter:  1382 loss =  0.02529822182345446 learning rate =  0.5 update =  [[-0.00293101]\n"," [-0.00293099]\n"," [ 0.0043867 ]]\n","Iter:  1383 loss =  0.025280015924161466 learning rate =  0.5 update =  [[-0.00292893]\n"," [-0.00292892]\n"," [ 0.0043836 ]]\n","Iter:  1384 loss =  0.02526183580623926 learning rate =  0.5 update =  [[-0.00292685]\n"," [-0.00292684]\n"," [ 0.0043805 ]]\n","Iter:  1385 loss =  0.025243681415502664 learning rate =  0.5 update =  [[-0.00292478]\n"," [-0.00292477]\n"," [ 0.00437741]]\n","Iter:  1386 loss =  0.02522555269791729 learning rate =  0.5 update =  [[-0.00292271]\n"," [-0.0029227 ]\n"," [ 0.00437431]]\n","Iter:  1387 loss =  0.025207449599598704 learning rate =  0.5 update =  [[-0.00292064]\n"," [-0.00292063]\n"," [ 0.00437123]]\n","Iter:  1388 loss =  0.0251893720668123 learning rate =  0.5 update =  [[-0.00291857]\n"," [-0.00291856]\n"," [ 0.00436815]]\n","Iter:  1389 loss =  0.0251713200459725 learning rate =  0.5 update =  [[-0.00291651]\n"," [-0.0029165 ]\n"," [ 0.00436507]]\n","Iter:  1390 loss =  0.025153293483642255 learning rate =  0.5 update =  [[-0.00291445]\n"," [-0.00291444]\n"," [ 0.00436199]]\n","Iter:  1391 loss =  0.025135292326532728 learning rate =  0.5 update =  [[-0.0029124 ]\n"," [-0.00291239]\n"," [ 0.00435892]]\n","Iter:  1392 loss =  0.02511731652150253 learning rate =  0.5 update =  [[-0.00291034]\n"," [-0.00291033]\n"," [ 0.00435586]]\n","Iter:  1393 loss =  0.0250993660155575 learning rate =  0.5 update =  [[-0.00290829]\n"," [-0.00290828]\n"," [ 0.0043528 ]]\n","Iter:  1394 loss =  0.02508144075584986 learning rate =  0.5 update =  [[-0.00290625]\n"," [-0.00290624]\n"," [ 0.00434974]]\n","Iter:  1395 loss =  0.025063540689678088 learning rate =  0.5 update =  [[-0.0029042 ]\n"," [-0.00290419]\n"," [ 0.00434669]]\n","Iter:  1396 loss =  0.025045665764485952 learning rate =  0.5 update =  [[-0.00290216]\n"," [-0.00290215]\n"," [ 0.00434364]]\n","Iter:  1397 loss =  0.025027815927862573 learning rate =  0.5 update =  [[-0.00290012]\n"," [-0.00290011]\n"," [ 0.00434059]]\n","Iter:  1398 loss =  0.025009991127541473 learning rate =  0.5 update =  [[-0.00289808]\n"," [-0.00289807]\n"," [ 0.00433755]]\n","Iter:  1399 loss =  0.024992191311400194 learning rate =  0.5 update =  [[-0.00289605]\n"," [-0.00289604]\n"," [ 0.00433452]]\n","Iter:  1400 loss =  0.024974416427459928 learning rate =  0.5 update =  [[-0.00289402]\n"," [-0.00289401]\n"," [ 0.00433149]]\n","Iter:  1401 loss =  0.024956666423885027 learning rate =  0.5 update =  [[-0.00289199]\n"," [-0.00289198]\n"," [ 0.00432846]]\n","Iter:  1402 loss =  0.024938941248982223 learning rate =  0.5 update =  [[-0.00288997]\n"," [-0.00288996]\n"," [ 0.00432544]]\n","Iter:  1403 loss =  0.02492124085120056 learning rate =  0.5 update =  [[-0.00288795]\n"," [-0.00288794]\n"," [ 0.00432242]]\n","Iter:  1404 loss =  0.024903565179130522 learning rate =  0.5 update =  [[-0.00288593]\n"," [-0.00288592]\n"," [ 0.0043194 ]]\n","Iter:  1405 loss =  0.024885914181503904 learning rate =  0.5 update =  [[-0.00288391]\n"," [-0.0028839 ]\n"," [ 0.00431639]]\n","Iter:  1406 loss =  0.024868287807192982 learning rate =  0.5 update =  [[-0.0028819 ]\n"," [-0.00288189]\n"," [ 0.00431338]]\n","Iter:  1407 loss =  0.024850686005210322 learning rate =  0.5 update =  [[-0.00287988]\n"," [-0.00287988]\n"," [ 0.00431038]]\n","Iter:  1408 loss =  0.02483310872470819 learning rate =  0.5 update =  [[-0.00287788]\n"," [-0.00287787]\n"," [ 0.00430738]]\n","Iter:  1409 loss =  0.024815555914977968 learning rate =  0.5 update =  [[-0.00287587]\n"," [-0.00287586]\n"," [ 0.00430439]]\n","Iter:  1410 loss =  0.024798027525449955 learning rate =  0.5 update =  [[-0.00287387]\n"," [-0.00287386]\n"," [ 0.0043014 ]]\n","Iter:  1411 loss =  0.02478052350569249 learning rate =  0.5 update =  [[-0.00287187]\n"," [-0.00287186]\n"," [ 0.00429841]]\n","Iter:  1412 loss =  0.024763043805411927 learning rate =  0.5 update =  [[-0.00286987]\n"," [-0.00286986]\n"," [ 0.00429543]]\n","Iter:  1413 loss =  0.02474558837445185 learning rate =  0.5 update =  [[-0.00286788]\n"," [-0.00286787]\n"," [ 0.00429245]]\n","Iter:  1414 loss =  0.02472815716279282 learning rate =  0.5 update =  [[-0.00286588]\n"," [-0.00286588]\n"," [ 0.00428948]]\n","Iter:  1415 loss =  0.02471075012055153 learning rate =  0.5 update =  [[-0.0028639 ]\n"," [-0.00286389]\n"," [ 0.00428651]]\n","Iter:  1416 loss =  0.02469336719798107 learning rate =  0.5 update =  [[-0.00286191]\n"," [-0.0028619 ]\n"," [ 0.00428354]]\n","Iter:  1417 loss =  0.02467600834546959 learning rate =  0.5 update =  [[-0.00285993]\n"," [-0.00285992]\n"," [ 0.00428058]]\n","Iter:  1418 loss =  0.024658673513540477 learning rate =  0.5 update =  [[-0.00285794]\n"," [-0.00285794]\n"," [ 0.00427762]]\n","Iter:  1419 loss =  0.024641362652851634 learning rate =  0.5 update =  [[-0.00285597]\n"," [-0.00285596]\n"," [ 0.00427467]]\n","Iter:  1420 loss =  0.02462407571419514 learning rate =  0.5 update =  [[-0.00285399]\n"," [-0.00285398]\n"," [ 0.00427172]]\n","Iter:  1421 loss =  0.024606812648496706 learning rate =  0.5 update =  [[-0.00285202]\n"," [-0.00285201]\n"," [ 0.00426877]]\n","Iter:  1422 loss =  0.02458957340681517 learning rate =  0.5 update =  [[-0.00285005]\n"," [-0.00285004]\n"," [ 0.00426583]]\n","Iter:  1423 loss =  0.024572357940342147 learning rate =  0.5 update =  [[-0.00284808]\n"," [-0.00284807]\n"," [ 0.00426289]]\n","Iter:  1424 loss =  0.0245551662004017 learning rate =  0.5 update =  [[-0.00284612]\n"," [-0.00284611]\n"," [ 0.00425996]]\n","Iter:  1425 loss =  0.02453799813844974 learning rate =  0.5 update =  [[-0.00284415]\n"," [-0.00284414]\n"," [ 0.00425703]]\n","Iter:  1426 loss =  0.024520853706073376 learning rate =  0.5 update =  [[-0.00284219]\n"," [-0.00284219]\n"," [ 0.0042541 ]]\n","Iter:  1427 loss =  0.024503732854990967 learning rate =  0.5 update =  [[-0.00284024]\n"," [-0.00284023]\n"," [ 0.00425118]]\n","Iter:  1428 loss =  0.02448663553705119 learning rate =  0.5 update =  [[-0.00283828]\n"," [-0.00283827]\n"," [ 0.00424826]]\n","Iter:  1429 loss =  0.024469561704233134 learning rate =  0.5 update =  [[-0.00283633]\n"," [-0.00283632]\n"," [ 0.00424535]]\n","Iter:  1430 loss =  0.024452511308645168 learning rate =  0.5 update =  [[-0.00283438]\n"," [-0.00283437]\n"," [ 0.00424244]]\n","Iter:  1431 loss =  0.024435484302525227 learning rate =  0.5 update =  [[-0.00283244]\n"," [-0.00283243]\n"," [ 0.00423953]]\n","Iter:  1432 loss =  0.02441848063823987 learning rate =  0.5 update =  [[-0.00283049]\n"," [-0.00283048]\n"," [ 0.00423663]]\n","Iter:  1433 loss =  0.024401500268283995 learning rate =  0.5 update =  [[-0.00282855]\n"," [-0.00282854]\n"," [ 0.00423373]]\n","Iter:  1434 loss =  0.02438454314528045 learning rate =  0.5 update =  [[-0.00282661]\n"," [-0.0028266 ]\n"," [ 0.00423084]]\n","Iter:  1435 loss =  0.024367609221979815 learning rate =  0.5 update =  [[-0.00282468]\n"," [-0.00282467]\n"," [ 0.00422795]]\n","Iter:  1436 loss =  0.02435069845125943 learning rate =  0.5 update =  [[-0.00282274]\n"," [-0.00282274]\n"," [ 0.00422506]]\n","Iter:  1437 loss =  0.024333810786123518 learning rate =  0.5 update =  [[-0.00282081]\n"," [-0.00282081]\n"," [ 0.00422218]]\n","Iter:  1438 loss =  0.02431694617970232 learning rate =  0.5 update =  [[-0.00281889]\n"," [-0.00281888]\n"," [ 0.0042193 ]]\n","Iter:  1439 loss =  0.02430010458525217 learning rate =  0.5 update =  [[-0.00281696]\n"," [-0.00281695]\n"," [ 0.00421643]]\n","Iter:  1440 loss =  0.024283285956154605 learning rate =  0.5 update =  [[-0.00281504]\n"," [-0.00281503]\n"," [ 0.00421355]]\n","Iter:  1441 loss =  0.024266490245916116 learning rate =  0.5 update =  [[-0.00281312]\n"," [-0.00281311]\n"," [ 0.00421069]]\n","Iter:  1442 loss =  0.02424971740816783 learning rate =  0.5 update =  [[-0.0028112 ]\n"," [-0.00281119]\n"," [ 0.00420782]]\n","Iter:  1443 loss =  0.024232967396665098 learning rate =  0.5 update =  [[-0.00280929]\n"," [-0.00280928]\n"," [ 0.00420496]]\n","Iter:  1444 loss =  0.024216240165286753 learning rate =  0.5 update =  [[-0.00280737]\n"," [-0.00280736]\n"," [ 0.00420211]]\n","Iter:  1445 loss =  0.02419953566803526 learning rate =  0.5 update =  [[-0.00280546]\n"," [-0.00280546]\n"," [ 0.00419926]]\n","Iter:  1446 loss =  0.024182853859035756 learning rate =  0.5 update =  [[-0.00280356]\n"," [-0.00280355]\n"," [ 0.00419641]]\n","Iter:  1447 loss =  0.024166194692535895 learning rate =  0.5 update =  [[-0.00280165]\n"," [-0.00280164]\n"," [ 0.00419356]]\n","Iter:  1448 loss =  0.024149558122905562 learning rate =  0.5 update =  [[-0.00279975]\n"," [-0.00279974]\n"," [ 0.00419072]]\n","Iter:  1449 loss =  0.024132944104636282 learning rate =  0.5 update =  [[-0.00279785]\n"," [-0.00279784]\n"," [ 0.00418789]]\n","Iter:  1450 loss =  0.02411635259234084 learning rate =  0.5 update =  [[-0.00279595]\n"," [-0.00279594]\n"," [ 0.00418505]]\n","Iter:  1451 loss =  0.024099783540752838 learning rate =  0.5 update =  [[-0.00279406]\n"," [-0.00279405]\n"," [ 0.00418222]]\n","Iter:  1452 loss =  0.02408323690472651 learning rate =  0.5 update =  [[-0.00279217]\n"," [-0.00279216]\n"," [ 0.0041794 ]]\n","Iter:  1453 loss =  0.024066712639235988 learning rate =  0.5 update =  [[-0.00279028]\n"," [-0.00279027]\n"," [ 0.00417658]]\n","Iter:  1454 loss =  0.024050210699375314 learning rate =  0.5 update =  [[-0.00278839]\n"," [-0.00278838]\n"," [ 0.00417376]]\n","Iter:  1455 loss =  0.024033731040357713 learning rate =  0.5 update =  [[-0.0027865 ]\n"," [-0.0027865 ]\n"," [ 0.00417094]]\n","Iter:  1456 loss =  0.024017273617515246 learning rate =  0.5 update =  [[-0.00278462]\n"," [-0.00278461]\n"," [ 0.00416813]]\n","Iter:  1457 loss =  0.024000838386298604 learning rate =  0.5 update =  [[-0.00278274]\n"," [-0.00278274]\n"," [ 0.00416533]]\n","Iter:  1458 loss =  0.023984425302276423 learning rate =  0.5 update =  [[-0.00278087]\n"," [-0.00278086]\n"," [ 0.00416252]]\n","Iter:  1459 loss =  0.02396803432113542 learning rate =  0.5 update =  [[-0.00277899]\n"," [-0.00277898]\n"," [ 0.00415973]]\n","Iter:  1460 loss =  0.023951665398679253 learning rate =  0.5 update =  [[-0.00277712]\n"," [-0.00277711]\n"," [ 0.00415693]]\n","Iter:  1461 loss =  0.02393531849082861 learning rate =  0.5 update =  [[-0.00277525]\n"," [-0.00277524]\n"," [ 0.00415414]]\n","Iter:  1462 loss =  0.023918993553621072 learning rate =  0.5 update =  [[-0.00277338]\n"," [-0.00277338]\n"," [ 0.00415135]]\n","Iter:  1463 loss =  0.02390269054320996 learning rate =  0.5 update =  [[-0.00277152]\n"," [-0.00277151]\n"," [ 0.00414857]]\n","Iter:  1464 loss =  0.02388640941586477 learning rate =  0.5 update =  [[-0.00276966]\n"," [-0.00276965]\n"," [ 0.00414578]]\n","Iter:  1465 loss =  0.02387015012797011 learning rate =  0.5 update =  [[-0.0027678 ]\n"," [-0.00276779]\n"," [ 0.00414301]]\n","Iter:  1466 loss =  0.02385391263602588 learning rate =  0.5 update =  [[-0.00276594]\n"," [-0.00276593]\n"," [ 0.00414023]]\n","Iter:  1467 loss =  0.023837696896646565 learning rate =  0.5 update =  [[-0.00276409]\n"," [-0.00276408]\n"," [ 0.00413746]]\n","Iter:  1468 loss =  0.023821502866560894 learning rate =  0.5 update =  [[-0.00276223]\n"," [-0.00276223]\n"," [ 0.0041347 ]]\n","Iter:  1469 loss =  0.02380533050261152 learning rate =  0.5 update =  [[-0.00276038]\n"," [-0.00276038]\n"," [ 0.00413194]]\n","Iter:  1470 loss =  0.023789179761754813 learning rate =  0.5 update =  [[-0.00275854]\n"," [-0.00275853]\n"," [ 0.00412918]]\n","Iter:  1471 loss =  0.023773050601060088 learning rate =  0.5 update =  [[-0.00275669]\n"," [-0.00275668]\n"," [ 0.00412642]]\n","Iter:  1472 loss =  0.02375694297770947 learning rate =  0.5 update =  [[-0.00275485]\n"," [-0.00275484]\n"," [ 0.00412367]]\n","Iter:  1473 loss =  0.02374085684899778 learning rate =  0.5 update =  [[-0.00275301]\n"," [-0.002753  ]\n"," [ 0.00412092]]\n","Iter:  1474 loss =  0.023724792172331637 learning rate =  0.5 update =  [[-0.00275117]\n"," [-0.00275116]\n"," [ 0.00411818]]\n","Iter:  1475 loss =  0.023708748905229546 learning rate =  0.5 update =  [[-0.00274934]\n"," [-0.00274933]\n"," [ 0.00411544]]\n","Iter:  1476 loss =  0.023692727005321153 learning rate =  0.5 update =  [[-0.0027475]\n"," [-0.0027475]\n"," [ 0.0041127]]\n","Iter:  1477 loss =  0.023676726430347367 learning rate =  0.5 update =  [[-0.00274567]\n"," [-0.00274567]\n"," [ 0.00410997]]\n","Iter:  1478 loss =  0.02366074713815937 learning rate =  0.5 update =  [[-0.00274384]\n"," [-0.00274384]\n"," [ 0.00410724]]\n","Iter:  1479 loss =  0.02364478908671889 learning rate =  0.5 update =  [[-0.00274202]\n"," [-0.00274201]\n"," [ 0.00410451]]\n","Iter:  1480 loss =  0.023628852234097407 learning rate =  0.5 update =  [[-0.0027402 ]\n"," [-0.00274019]\n"," [ 0.00410179]]\n","Iter:  1481 loss =  0.023612936538476043 learning rate =  0.5 update =  [[-0.00273837]\n"," [-0.00273837]\n"," [ 0.00409907]]\n","Iter:  1482 loss =  0.023597041958144986 learning rate =  0.5 update =  [[-0.00273656]\n"," [-0.00273655]\n"," [ 0.00409635]]\n","Iter:  1483 loss =  0.023581168451503272 learning rate =  0.5 update =  [[-0.00273474]\n"," [-0.00273473]\n"," [ 0.00409364]]\n","Iter:  1484 loss =  0.02356531597705847 learning rate =  0.5 update =  [[-0.00273293]\n"," [-0.00273292]\n"," [ 0.00409093]]\n","Iter:  1485 loss =  0.023549484493426323 learning rate =  0.5 update =  [[-0.00273112]\n"," [-0.00273111]\n"," [ 0.00408823]]\n","Iter:  1486 loss =  0.02353367395933026 learning rate =  0.5 update =  [[-0.00272931]\n"," [-0.0027293 ]\n"," [ 0.00408552]]\n","Iter:  1487 loss =  0.023517884333601115 learning rate =  0.5 update =  [[-0.0027275 ]\n"," [-0.00272749]\n"," [ 0.00408283]]\n","Iter:  1488 loss =  0.023502115575176907 learning rate =  0.5 update =  [[-0.0027257 ]\n"," [-0.00272569]\n"," [ 0.00408013]]\n","Iter:  1489 loss =  0.023486367643102298 learning rate =  0.5 update =  [[-0.00272389]\n"," [-0.00272389]\n"," [ 0.00407744]]\n","Iter:  1490 loss =  0.023470640496528368 learning rate =  0.5 update =  [[-0.00272209]\n"," [-0.00272209]\n"," [ 0.00407475]]\n","Iter:  1491 loss =  0.023454934094712228 learning rate =  0.5 update =  [[-0.0027203 ]\n"," [-0.00272029]\n"," [ 0.00407207]]\n","Iter:  1492 loss =  0.023439248397016842 learning rate =  0.5 update =  [[-0.0027185 ]\n"," [-0.0027185 ]\n"," [ 0.00406939]]\n","Iter:  1493 loss =  0.023423583362910273 learning rate =  0.5 update =  [[-0.00271671]\n"," [-0.0027167 ]\n"," [ 0.00406671]]\n","Iter:  1494 loss =  0.023407938951965794 learning rate =  0.5 update =  [[-0.00271492]\n"," [-0.00271491]\n"," [ 0.00406404]]\n","Iter:  1495 loss =  0.02339231512386132 learning rate =  0.5 update =  [[-0.00271313]\n"," [-0.00271313]\n"," [ 0.00406137]]\n","Iter:  1496 loss =  0.023376711838379048 learning rate =  0.5 update =  [[-0.00271135]\n"," [-0.00271134]\n"," [ 0.0040587 ]]\n","Iter:  1497 loss =  0.0233611290554053 learning rate =  0.5 update =  [[-0.00270956]\n"," [-0.00270956]\n"," [ 0.00405604]]\n","Iter:  1498 loss =  0.02334556673492999 learning rate =  0.5 update =  [[-0.00270778]\n"," [-0.00270778]\n"," [ 0.00405338]]\n","Iter:  1499 loss =  0.023330024837046458 learning rate =  0.5 update =  [[-0.002706  ]\n"," [-0.002706  ]\n"," [ 0.00405072]]\n","Iter:  1500 loss =  0.023314503321950952 learning rate =  0.5 update =  [[-0.00270423]\n"," [-0.00270422]\n"," [ 0.00404807]]\n","Iter:  1501 loss =  0.02329900214994252 learning rate =  0.5 update =  [[-0.00270245]\n"," [-0.00270245]\n"," [ 0.00404542]]\n","Iter:  1502 loss =  0.023283521281422383 learning rate =  0.5 update =  [[-0.00270068]\n"," [-0.00270068]\n"," [ 0.00404277]]\n","Iter:  1503 loss =  0.02326806067689404 learning rate =  0.5 update =  [[-0.00269891]\n"," [-0.00269891]\n"," [ 0.00404013]]\n","Iter:  1504 loss =  0.023252620296962432 learning rate =  0.5 update =  [[-0.00269715]\n"," [-0.00269714]\n"," [ 0.00403749]]\n","Iter:  1505 loss =  0.02323720010233412 learning rate =  0.5 update =  [[-0.00269538]\n"," [-0.00269537]\n"," [ 0.00403485]]\n","Iter:  1506 loss =  0.023221800053816515 learning rate =  0.5 update =  [[-0.00269362]\n"," [-0.00269361]\n"," [ 0.00403222]]\n","Iter:  1507 loss =  0.02320642011231787 learning rate =  0.5 update =  [[-0.00269186]\n"," [-0.00269185]\n"," [ 0.00402959]]\n","Iter:  1508 loss =  0.023191060238846758 learning rate =  0.5 update =  [[-0.0026901 ]\n"," [-0.00269009]\n"," [ 0.00402697]]\n","Iter:  1509 loss =  0.023175720394511948 learning rate =  0.5 update =  [[-0.00268834]\n"," [-0.00268834]\n"," [ 0.00402434]]\n","Iter:  1510 loss =  0.02316040054052177 learning rate =  0.5 update =  [[-0.00268659]\n"," [-0.00268658]\n"," [ 0.00402172]]\n","Iter:  1511 loss =  0.023145100638184285 learning rate =  0.5 update =  [[-0.00268484]\n"," [-0.00268483]\n"," [ 0.00401911]]\n","Iter:  1512 loss =  0.023129820648906438 learning rate =  0.5 update =  [[-0.00268309]\n"," [-0.00268308]\n"," [ 0.0040165 ]]\n","Iter:  1513 loss =  0.023114560534194167 learning rate =  0.5 update =  [[-0.00268134]\n"," [-0.00268134]\n"," [ 0.00401389]]\n","Iter:  1514 loss =  0.023099320255651756 learning rate =  0.5 update =  [[-0.0026796 ]\n"," [-0.00267959]\n"," [ 0.00401128]]\n","Iter:  1515 loss =  0.023084099774981777 learning rate =  0.5 update =  [[-0.00267786]\n"," [-0.00267785]\n"," [ 0.00400868]]\n","Iter:  1516 loss =  0.023068899053984702 learning rate =  0.5 update =  [[-0.00267612]\n"," [-0.00267611]\n"," [ 0.00400608]]\n","Iter:  1517 loss =  0.02305371805455836 learning rate =  0.5 update =  [[-0.00267438]\n"," [-0.00267437]\n"," [ 0.00400349]]\n","Iter:  1518 loss =  0.023038556738698073 learning rate =  0.5 update =  [[-0.00267264]\n"," [-0.00267264]\n"," [ 0.00400089]]\n","Iter:  1519 loss =  0.023023415068496055 learning rate =  0.5 update =  [[-0.00267091]\n"," [-0.0026709 ]\n"," [ 0.0039983 ]]\n","Iter:  1520 loss =  0.02300829300614113 learning rate =  0.5 update =  [[-0.00266918]\n"," [-0.00266917]\n"," [ 0.00399572]]\n","Iter:  1521 loss =  0.022993190513918297 learning rate =  0.5 update =  [[-0.00266745]\n"," [-0.00266744]\n"," [ 0.00399314]]\n","Iter:  1522 loss =  0.022978107554208912 learning rate =  0.5 update =  [[-0.00266572]\n"," [-0.00266572]\n"," [ 0.00399056]]\n","Iter:  1523 loss =  0.022963044089489744 learning rate =  0.5 update =  [[-0.002664  ]\n"," [-0.00266399]\n"," [ 0.00398798]]\n","Iter:  1524 loss =  0.02294800008233321 learning rate =  0.5 update =  [[-0.00266228]\n"," [-0.00266227]\n"," [ 0.00398541]]\n","Iter:  1525 loss =  0.022932975495406654 learning rate =  0.5 update =  [[-0.00266056]\n"," [-0.00266055]\n"," [ 0.00398284]]\n","Iter:  1526 loss =  0.022917970291472342 learning rate =  0.5 update =  [[-0.00265884]\n"," [-0.00265883]\n"," [ 0.00398027]]\n","Iter:  1527 loss =  0.022902984433387102 learning rate =  0.5 update =  [[-0.00265712]\n"," [-0.00265712]\n"," [ 0.00397771]]\n","Iter:  1528 loss =  0.022888017884101896 learning rate =  0.5 update =  [[-0.00265541]\n"," [-0.0026554 ]\n"," [ 0.00397515]]\n","Iter:  1529 loss =  0.022873070606661577 learning rate =  0.5 update =  [[-0.0026537 ]\n"," [-0.00265369]\n"," [ 0.00397259]]\n","Iter:  1530 loss =  0.02285814256420476 learning rate =  0.5 update =  [[-0.00265199]\n"," [-0.00265198]\n"," [ 0.00397004]]\n","Iter:  1531 loss =  0.022843233719963316 learning rate =  0.5 update =  [[-0.00265028]\n"," [-0.00265028]\n"," [ 0.00396749]]\n","Iter:  1532 loss =  0.02282834403726211 learning rate =  0.5 update =  [[-0.00264858]\n"," [-0.00264857]\n"," [ 0.00396494]]\n","Iter:  1533 loss =  0.02281347347951883 learning rate =  0.5 update =  [[-0.00264687]\n"," [-0.00264687]\n"," [ 0.0039624 ]]\n","Iter:  1534 loss =  0.02279862201024358 learning rate =  0.5 update =  [[-0.00264517]\n"," [-0.00264517]\n"," [ 0.00395986]]\n","Iter:  1535 loss =  0.02278378959303865 learning rate =  0.5 update =  [[-0.00264347]\n"," [-0.00264347]\n"," [ 0.00395732]]\n","Iter:  1536 loss =  0.022768976191598157 learning rate =  0.5 update =  [[-0.00264178]\n"," [-0.00264177]\n"," [ 0.00395479]]\n","Iter:  1537 loss =  0.02275418176970788 learning rate =  0.5 update =  [[-0.00264008]\n"," [-0.00264008]\n"," [ 0.00395226]]\n","Iter:  1538 loss =  0.022739406291244796 learning rate =  0.5 update =  [[-0.00263839]\n"," [-0.00263839]\n"," [ 0.00394973]]\n","Iter:  1539 loss =  0.022724649720177036 learning rate =  0.5 update =  [[-0.0026367 ]\n"," [-0.0026367 ]\n"," [ 0.00394721]]\n","Iter:  1540 loss =  0.022709912020563286 learning rate =  0.5 update =  [[-0.00263501]\n"," [-0.00263501]\n"," [ 0.00394469]]\n","Iter:  1541 loss =  0.022695193156552808 learning rate =  0.5 update =  [[-0.00263333]\n"," [-0.00263332]\n"," [ 0.00394217]]\n","Iter:  1542 loss =  0.022680493092385026 learning rate =  0.5 update =  [[-0.00263165]\n"," [-0.00263164]\n"," [ 0.00393965]]\n","Iter:  1543 loss =  0.0226658117923891 learning rate =  0.5 update =  [[-0.00262996]\n"," [-0.00262996]\n"," [ 0.00393714]]\n","Iter:  1544 loss =  0.022651149220983994 learning rate =  0.5 update =  [[-0.00262829]\n"," [-0.00262828]\n"," [ 0.00393463]]\n","Iter:  1545 loss =  0.02263650534267776 learning rate =  0.5 update =  [[-0.00262661]\n"," [-0.0026266 ]\n"," [ 0.00393213]]\n","Iter:  1546 loss =  0.022621880122067846 learning rate =  0.5 update =  [[-0.00262493]\n"," [-0.00262493]\n"," [ 0.00392963]]\n","Iter:  1547 loss =  0.022607273523840003 learning rate =  0.5 update =  [[-0.00262326]\n"," [-0.00262326]\n"," [ 0.00392713]]\n","Iter:  1548 loss =  0.022592685512768718 learning rate =  0.5 update =  [[-0.00262159]\n"," [-0.00262158]\n"," [ 0.00392463]]\n","Iter:  1549 loss =  0.02257811605371681 learning rate =  0.5 update =  [[-0.00261992]\n"," [-0.00261992]\n"," [ 0.00392214]]\n","Iter:  1550 loss =  0.022563565111634782 learning rate =  0.5 update =  [[-0.00261825]\n"," [-0.00261825]\n"," [ 0.00391965]]\n","Iter:  1551 loss =  0.022549032651560972 learning rate =  0.5 update =  [[-0.00261659]\n"," [-0.00261659]\n"," [ 0.00391716]]\n","Iter:  1552 loss =  0.022534518638620993 learning rate =  0.5 update =  [[-0.00261493]\n"," [-0.00261492]\n"," [ 0.00391468]]\n","Iter:  1553 loss =  0.022520023038027644 learning rate =  0.5 update =  [[-0.00261327]\n"," [-0.00261326]\n"," [ 0.0039122 ]]\n","Iter:  1554 loss =  0.022505545815080515 learning rate =  0.5 update =  [[-0.00261161]\n"," [-0.0026116 ]\n"," [ 0.00390972]]\n","Iter:  1555 loss =  0.022491086935165903 learning rate =  0.5 update =  [[-0.00260995]\n"," [-0.00260995]\n"," [ 0.00390725]]\n","Iter:  1556 loss =  0.02247664636375623 learning rate =  0.5 update =  [[-0.0026083 ]\n"," [-0.00260829]\n"," [ 0.00390478]]\n","Iter:  1557 loss =  0.022462224066410116 learning rate =  0.5 update =  [[-0.00260665]\n"," [-0.00260664]\n"," [ 0.00390231]]\n","Iter:  1558 loss =  0.022447820008771907 learning rate =  0.5 update =  [[-0.002605  ]\n"," [-0.00260499]\n"," [ 0.00389985]]\n","Iter:  1559 loss =  0.022433434156571348 learning rate =  0.5 update =  [[-0.00260335]\n"," [-0.00260334]\n"," [ 0.00389738]]\n","Iter:  1560 loss =  0.022419066475623636 learning rate =  0.5 update =  [[-0.0026017 ]\n"," [-0.0026017 ]\n"," [ 0.00389493]]\n","Iter:  1561 loss =  0.02240471693182878 learning rate =  0.5 update =  [[-0.00260006]\n"," [-0.00260005]\n"," [ 0.00389247]]\n","Iter:  1562 loss =  0.022390385491171633 learning rate =  0.5 update =  [[-0.00259842]\n"," [-0.00259841]\n"," [ 0.00389002]]\n","Iter:  1563 loss =  0.02237607211972134 learning rate =  0.5 update =  [[-0.00259678]\n"," [-0.00259677]\n"," [ 0.00388757]]\n","Iter:  1564 loss =  0.022361776783631413 learning rate =  0.5 update =  [[-0.00259514]\n"," [-0.00259514]\n"," [ 0.00388512]]\n","Iter:  1565 loss =  0.022347499449139274 learning rate =  0.5 update =  [[-0.00259351]\n"," [-0.0025935 ]\n"," [ 0.00388268]]\n","Iter:  1566 loss =  0.022333240082565924 learning rate =  0.5 update =  [[-0.00259187]\n"," [-0.00259187]\n"," [ 0.00388024]]\n","Iter:  1567 loss =  0.022318998650315808 learning rate =  0.5 update =  [[-0.00259024]\n"," [-0.00259024]\n"," [ 0.0038778 ]]\n","Iter:  1568 loss =  0.022304775118876644 learning rate =  0.5 update =  [[-0.00258861]\n"," [-0.00258861]\n"," [ 0.00387537]]\n","Iter:  1569 loss =  0.022290569454818956 learning rate =  0.5 update =  [[-0.00258698]\n"," [-0.00258698]\n"," [ 0.00387294]]\n","Iter:  1570 loss =  0.022276381624795893 learning rate =  0.5 update =  [[-0.00258536]\n"," [-0.00258535]\n"," [ 0.00387051]]\n","Iter:  1571 loss =  0.022262211595543174 learning rate =  0.5 update =  [[-0.00258373]\n"," [-0.00258373]\n"," [ 0.00386808]]\n","Iter:  1572 loss =  0.02224805933387844 learning rate =  0.5 update =  [[-0.00258211]\n"," [-0.00258211]\n"," [ 0.00386566]]\n","Iter:  1573 loss =  0.022233924806701347 learning rate =  0.5 update =  [[-0.00258049]\n"," [-0.00258049]\n"," [ 0.00386324]]\n","Iter:  1574 loss =  0.02221980798099324 learning rate =  0.5 update =  [[-0.00257888]\n"," [-0.00257887]\n"," [ 0.00386082]]\n","Iter:  1575 loss =  0.022205708823816755 learning rate =  0.5 update =  [[-0.00257726]\n"," [-0.00257726]\n"," [ 0.00385841]]\n","Iter:  1576 loss =  0.022191627302315692 learning rate =  0.5 update =  [[-0.00257565]\n"," [-0.00257564]\n"," [ 0.003856  ]]\n","Iter:  1577 loss =  0.022177563383714804 learning rate =  0.5 update =  [[-0.00257404]\n"," [-0.00257403]\n"," [ 0.00385359]]\n","Iter:  1578 loss =  0.022163517035319387 learning rate =  0.5 update =  [[-0.00257243]\n"," [-0.00257242]\n"," [ 0.00385119]]\n","Iter:  1579 loss =  0.022149488224515256 learning rate =  0.5 update =  [[-0.00257082]\n"," [-0.00257081]\n"," [ 0.00384879]]\n","Iter:  1580 loss =  0.0221354769187683 learning rate =  0.5 update =  [[-0.00256921]\n"," [-0.00256921]\n"," [ 0.00384639]]\n","Iter:  1581 loss =  0.02212148308562434 learning rate =  0.5 update =  [[-0.00256761]\n"," [-0.0025676 ]\n"," [ 0.00384399]]\n","Iter:  1582 loss =  0.022107506692708948 learning rate =  0.5 update =  [[-0.00256601]\n"," [-0.002566  ]\n"," [ 0.0038416 ]]\n","Iter:  1583 loss =  0.022093547707726847 learning rate =  0.5 update =  [[-0.00256441]\n"," [-0.0025644 ]\n"," [ 0.00383921]]\n","Iter:  1584 loss =  0.022079606098462268 learning rate =  0.5 update =  [[-0.00256281]\n"," [-0.00256281]\n"," [ 0.00383682]]\n","Iter:  1585 loss =  0.022065681832778152 learning rate =  0.5 update =  [[-0.00256122]\n"," [-0.00256121]\n"," [ 0.00383444]]\n","Iter:  1586 loss =  0.022051774878616336 learning rate =  0.5 update =  [[-0.00255962]\n"," [-0.00255962]\n"," [ 0.00383206]]\n","Iter:  1587 loss =  0.02203788520399684 learning rate =  0.5 update =  [[-0.00255803]\n"," [-0.00255803]\n"," [ 0.00382968]]\n","Iter:  1588 loss =  0.02202401277701821 learning rate =  0.5 update =  [[-0.00255644]\n"," [-0.00255644]\n"," [ 0.00382731]]\n","Iter:  1589 loss =  0.022010157565856777 learning rate =  0.5 update =  [[-0.00255485]\n"," [-0.00255485]\n"," [ 0.00382493]]\n","Iter:  1590 loss =  0.0219963195387667 learning rate =  0.5 update =  [[-0.00255327]\n"," [-0.00255326]\n"," [ 0.00382256]]\n","Iter:  1591 loss =  0.021982498664079485 learning rate =  0.5 update =  [[-0.00255168]\n"," [-0.00255168]\n"," [ 0.0038202 ]]\n","Iter:  1592 loss =  0.021968694910204183 learning rate =  0.5 update =  [[-0.0025501 ]\n"," [-0.0025501 ]\n"," [ 0.00381784]]\n","Iter:  1593 loss =  0.021954908245626695 learning rate =  0.5 update =  [[-0.00254852]\n"," [-0.00254852]\n"," [ 0.00381547]]\n","Iter:  1594 loss =  0.021941138638909806 learning rate =  0.5 update =  [[-0.00254694]\n"," [-0.00254694]\n"," [ 0.00381312]]\n","Iter:  1595 loss =  0.02192738605869282 learning rate =  0.5 update =  [[-0.00254537]\n"," [-0.00254536]\n"," [ 0.00381076]]\n","Iter:  1596 loss =  0.02191365047369138 learning rate =  0.5 update =  [[-0.00254379]\n"," [-0.00254379]\n"," [ 0.00380841]]\n","Iter:  1597 loss =  0.021899931852697264 learning rate =  0.5 update =  [[-0.00254222]\n"," [-0.00254221]\n"," [ 0.00380606]]\n","Iter:  1598 loss =  0.021886230164578177 learning rate =  0.5 update =  [[-0.00254065]\n"," [-0.00254064]\n"," [ 0.00380371]]\n","Iter:  1599 loss =  0.02187254537827737 learning rate =  0.5 update =  [[-0.00253908]\n"," [-0.00253908]\n"," [ 0.00380137]]\n","Iter:  1600 loss =  0.02185887746281366 learning rate =  0.5 update =  [[-0.00253751]\n"," [-0.00253751]\n"," [ 0.00379903]]\n","Iter:  1601 loss =  0.021845226387280908 learning rate =  0.5 update =  [[-0.00253595]\n"," [-0.00253594]\n"," [ 0.00379669]]\n","Iter:  1602 loss =  0.02183159212084794 learning rate =  0.5 update =  [[-0.00253439]\n"," [-0.00253438]\n"," [ 0.00379436]]\n","Iter:  1603 loss =  0.021817974632758495 learning rate =  0.5 update =  [[-0.00253283]\n"," [-0.00253282]\n"," [ 0.00379203]]\n","Iter:  1604 loss =  0.021804373892330732 learning rate =  0.5 update =  [[-0.00253127]\n"," [-0.00253126]\n"," [ 0.0037897 ]]\n","Iter:  1605 loss =  0.02179078986895698 learning rate =  0.5 update =  [[-0.00252971]\n"," [-0.0025297 ]\n"," [ 0.00378737]]\n","Iter:  1606 loss =  0.021777222532103784 learning rate =  0.5 update =  [[-0.00252815]\n"," [-0.00252815]\n"," [ 0.00378505]]\n","Iter:  1607 loss =  0.02176367185131166 learning rate =  0.5 update =  [[-0.0025266 ]\n"," [-0.0025266 ]\n"," [ 0.00378273]]\n","Iter:  1608 loss =  0.021750137796194294 learning rate =  0.5 update =  [[-0.00252505]\n"," [-0.00252504]\n"," [ 0.00378041]]\n","Iter:  1609 loss =  0.021736620336439312 learning rate =  0.5 update =  [[-0.0025235 ]\n"," [-0.0025235 ]\n"," [ 0.00377809]]\n","Iter:  1610 loss =  0.021723119441807128 learning rate =  0.5 update =  [[-0.00252195]\n"," [-0.00252195]\n"," [ 0.00377578]]\n","Iter:  1611 loss =  0.02170963508213137 learning rate =  0.5 update =  [[-0.00252041]\n"," [-0.0025204 ]\n"," [ 0.00377347]]\n","Iter:  1612 loss =  0.021696167227318156 learning rate =  0.5 update =  [[-0.00251886]\n"," [-0.00251886]\n"," [ 0.00377116]]\n","Iter:  1613 loss =  0.02168271584734638 learning rate =  0.5 update =  [[-0.00251732]\n"," [-0.00251732]\n"," [ 0.00376886]]\n","Iter:  1614 loss =  0.021669280912267157 learning rate =  0.5 update =  [[-0.00251578]\n"," [-0.00251578]\n"," [ 0.00376656]]\n","Iter:  1615 loss =  0.0216558623922036 learning rate =  0.5 update =  [[-0.00251424]\n"," [-0.00251424]\n"," [ 0.00376426]]\n","Iter:  1616 loss =  0.02164246025735079 learning rate =  0.5 update =  [[-0.0025127 ]\n"," [-0.0025127 ]\n"," [ 0.00376196]]\n","Iter:  1617 loss =  0.021629074477975498 learning rate =  0.5 update =  [[-0.00251117]\n"," [-0.00251117]\n"," [ 0.00375967]]\n","Iter:  1618 loss =  0.0216157050244158 learning rate =  0.5 update =  [[-0.00250964]\n"," [-0.00250963]\n"," [ 0.00375738]]\n","Iter:  1619 loss =  0.021602351867081187 learning rate =  0.5 update =  [[-0.00250811]\n"," [-0.0025081 ]\n"," [ 0.00375509]]\n","Iter:  1620 loss =  0.021589014976452 learning rate =  0.5 update =  [[-0.00250658]\n"," [-0.00250657]\n"," [ 0.00375281]]\n","Iter:  1621 loss =  0.021575694323079497 learning rate =  0.5 update =  [[-0.00250505]\n"," [-0.00250505]\n"," [ 0.00375053]]\n","Iter:  1622 loss =  0.021562389877585512 learning rate =  0.5 update =  [[-0.00250352]\n"," [-0.00250352]\n"," [ 0.00374825]]\n","Iter:  1623 loss =  0.0215491016106623 learning rate =  0.5 update =  [[-0.002502  ]\n"," [-0.002502  ]\n"," [ 0.00374597]]\n","Iter:  1624 loss =  0.021535829493072173 learning rate =  0.5 update =  [[-0.00250048]\n"," [-0.00250048]\n"," [ 0.0037437 ]]\n","Iter:  1625 loss =  0.021522573495647526 learning rate =  0.5 update =  [[-0.00249896]\n"," [-0.00249896]\n"," [ 0.00374143]]\n","Iter:  1626 loss =  0.021509333589290358 learning rate =  0.5 update =  [[-0.00249744]\n"," [-0.00249744]\n"," [ 0.00373916]]\n","Iter:  1627 loss =  0.021496109744972534 learning rate =  0.5 update =  [[-0.00249592]\n"," [-0.00249592]\n"," [ 0.00373689]]\n","Iter:  1628 loss =  0.021482901933734914 learning rate =  0.5 update =  [[-0.00249441]\n"," [-0.00249441]\n"," [ 0.00373463]]\n","Iter:  1629 loss =  0.02146971012668774 learning rate =  0.5 update =  [[-0.0024929 ]\n"," [-0.00249289]\n"," [ 0.00373237]]\n","Iter:  1630 loss =  0.021456534295010047 learning rate =  0.5 update =  [[-0.00249139]\n"," [-0.00249138]\n"," [ 0.00373011]]\n","Iter:  1631 loss =  0.021443374409949666 learning rate =  0.5 update =  [[-0.00248988]\n"," [-0.00248987]\n"," [ 0.00372786]]\n","Iter:  1632 loss =  0.021430230442822988 learning rate =  0.5 update =  [[-0.00248837]\n"," [-0.00248837]\n"," [ 0.00372561]]\n","Iter:  1633 loss =  0.021417102365014564 learning rate =  0.5 update =  [[-0.00248686]\n"," [-0.00248686]\n"," [ 0.00372336]]\n","Iter:  1634 loss =  0.021403990147977274 learning rate =  0.5 update =  [[-0.00248536]\n"," [-0.00248536]\n"," [ 0.00372111]]\n","Iter:  1635 loss =  0.021390893763231708 learning rate =  0.5 update =  [[-0.00248386]\n"," [-0.00248386]\n"," [ 0.00371887]]\n","Iter:  1636 loss =  0.021377813182366447 learning rate =  0.5 update =  [[-0.00248236]\n"," [-0.00248236]\n"," [ 0.00371662]]\n","Iter:  1637 loss =  0.021364748377037314 learning rate =  0.5 update =  [[-0.00248086]\n"," [-0.00248086]\n"," [ 0.00371439]]\n","Iter:  1638 loss =  0.021351699318967632 learning rate =  0.5 update =  [[-0.00247936]\n"," [-0.00247936]\n"," [ 0.00371215]]\n","Iter:  1639 loss =  0.021338665979947796 learning rate =  0.5 update =  [[-0.00247787]\n"," [-0.00247787]\n"," [ 0.00370992]]\n","Iter:  1640 loss =  0.021325648331834975 learning rate =  0.5 update =  [[-0.00247638]\n"," [-0.00247637]\n"," [ 0.00370769]]\n","Iter:  1641 loss =  0.021312646346553384 learning rate =  0.5 update =  [[-0.00247489]\n"," [-0.00247488]\n"," [ 0.00370546]]\n","Iter:  1642 loss =  0.021299659996093408 learning rate =  0.5 update =  [[-0.0024734 ]\n"," [-0.00247339]\n"," [ 0.00370323]]\n","Iter:  1643 loss =  0.021286689252512005 learning rate =  0.5 update =  [[-0.00247191]\n"," [-0.0024719 ]\n"," [ 0.00370101]]\n","Iter:  1644 loss =  0.021273734087932164 learning rate =  0.5 update =  [[-0.00247042]\n"," [-0.00247042]\n"," [ 0.00369879]]\n","Iter:  1645 loss =  0.021260794474542866 learning rate =  0.5 update =  [[-0.00246894]\n"," [-0.00246893]\n"," [ 0.00369657]]\n","Iter:  1646 loss =  0.02124787038459887 learning rate =  0.5 update =  [[-0.00246746]\n"," [-0.00246745]\n"," [ 0.00369436]]\n","Iter:  1647 loss =  0.021234961790420347 learning rate =  0.5 update =  [[-0.00246598]\n"," [-0.00246597]\n"," [ 0.00369214]]\n","Iter:  1648 loss =  0.021222068664392982 learning rate =  0.5 update =  [[-0.0024645 ]\n"," [-0.00246449]\n"," [ 0.00368993]]\n","Iter:  1649 loss =  0.021209190978967483 learning rate =  0.5 update =  [[-0.00246302]\n"," [-0.00246302]\n"," [ 0.00368773]]\n","Iter:  1650 loss =  0.021196328706659752 learning rate =  0.5 update =  [[-0.00246154]\n"," [-0.00246154]\n"," [ 0.00368552]]\n","Iter:  1651 loss =  0.02118348182005028 learning rate =  0.5 update =  [[-0.00246007]\n"," [-0.00246007]\n"," [ 0.00368332]]\n","Iter:  1652 loss =  0.021170650291784235 learning rate =  0.5 update =  [[-0.0024586 ]\n"," [-0.0024586 ]\n"," [ 0.00368112]]\n","Iter:  1653 loss =  0.021157834094571178 learning rate =  0.5 update =  [[-0.00245713]\n"," [-0.00245713]\n"," [ 0.00367892]]\n","Iter:  1654 loss =  0.021145033201184896 learning rate =  0.5 update =  [[-0.00245566]\n"," [-0.00245566]\n"," [ 0.00367673]]\n","Iter:  1655 loss =  0.021132247584463207 learning rate =  0.5 update =  [[-0.00245419]\n"," [-0.00245419]\n"," [ 0.00367454]]\n","Iter:  1656 loss =  0.0211194772173078 learning rate =  0.5 update =  [[-0.00245273]\n"," [-0.00245273]\n"," [ 0.00367235]]\n","Iter:  1657 loss =  0.02110672207268406 learning rate =  0.5 update =  [[-0.00245127]\n"," [-0.00245126]\n"," [ 0.00367016]]\n","Iter:  1658 loss =  0.021093982123620597 learning rate =  0.5 update =  [[-0.0024498 ]\n"," [-0.0024498 ]\n"," [ 0.00366798]]\n","Iter:  1659 loss =  0.021081257343209668 learning rate =  0.5 update =  [[-0.00244834]\n"," [-0.00244834]\n"," [ 0.0036658 ]]\n","Iter:  1660 loss =  0.02106854770460633 learning rate =  0.5 update =  [[-0.00244689]\n"," [-0.00244688]\n"," [ 0.00366362]]\n","Iter:  1661 loss =  0.021055853181028823 learning rate =  0.5 update =  [[-0.00244543]\n"," [-0.00244543]\n"," [ 0.00366144]]\n","Iter:  1662 loss =  0.02104317374575795 learning rate =  0.5 update =  [[-0.00244398]\n"," [-0.00244397]\n"," [ 0.00365927]]\n","Iter:  1663 loss =  0.02103050937213708 learning rate =  0.5 update =  [[-0.00244252]\n"," [-0.00244252]\n"," [ 0.0036571 ]]\n","Iter:  1664 loss =  0.021017860033571915 learning rate =  0.5 update =  [[-0.00244107]\n"," [-0.00244107]\n"," [ 0.00365493]]\n","Iter:  1665 loss =  0.021005225703530542 learning rate =  0.5 update =  [[-0.00243962]\n"," [-0.00243962]\n"," [ 0.00365276]]\n","Iter:  1666 loss =  0.0209926063555428 learning rate =  0.5 update =  [[-0.00243817]\n"," [-0.00243817]\n"," [ 0.0036506 ]]\n","Iter:  1667 loss =  0.020980001963200576 learning rate =  0.5 update =  [[-0.00243673]\n"," [-0.00243672]\n"," [ 0.00364844]]\n","Iter:  1668 loss =  0.020967412500157358 learning rate =  0.5 update =  [[-0.00243528]\n"," [-0.00243528]\n"," [ 0.00364628]]\n","Iter:  1669 loss =  0.020954837940127875 learning rate =  0.5 update =  [[-0.00243384]\n"," [-0.00243384]\n"," [ 0.00364413]]\n","Iter:  1670 loss =  0.020942278256888443 learning rate =  0.5 update =  [[-0.0024324 ]\n"," [-0.0024324 ]\n"," [ 0.00364197]]\n","Iter:  1671 loss =  0.02092973342427632 learning rate =  0.5 update =  [[-0.00243096]\n"," [-0.00243096]\n"," [ 0.00363982]]\n","Iter:  1672 loss =  0.020917203416189856 learning rate =  0.5 update =  [[-0.00242952]\n"," [-0.00242952]\n"," [ 0.00363767]]\n","Iter:  1673 loss =  0.020904688206587967 learning rate =  0.5 update =  [[-0.00242809]\n"," [-0.00242808]\n"," [ 0.00363553]]\n","Iter:  1674 loss =  0.020892187769490353 learning rate =  0.5 update =  [[-0.00242665]\n"," [-0.00242665]\n"," [ 0.00363338]]\n","Iter:  1675 loss =  0.0208797020789769 learning rate =  0.5 update =  [[-0.00242522]\n"," [-0.00242522]\n"," [ 0.00363124]]\n","Iter:  1676 loss =  0.02086723110918782 learning rate =  0.5 update =  [[-0.00242379]\n"," [-0.00242379]\n"," [ 0.00362911]]\n","Iter:  1677 loss =  0.020854774834323547 learning rate =  0.5 update =  [[-0.00242236]\n"," [-0.00242236]\n"," [ 0.00362697]]\n","Iter:  1678 loss =  0.02084233322864426 learning rate =  0.5 update =  [[-0.00242093]\n"," [-0.00242093]\n"," [ 0.00362484]]\n","Iter:  1679 loss =  0.020829906266469744 learning rate =  0.5 update =  [[-0.00241951]\n"," [-0.0024195 ]\n"," [ 0.0036227 ]]\n","Iter:  1680 loss =  0.020817493922179537 learning rate =  0.5 update =  [[-0.00241808]\n"," [-0.00241808]\n"," [ 0.00362058]]\n","Iter:  1681 loss =  0.020805096170212336 learning rate =  0.5 update =  [[-0.00241666]\n"," [-0.00241666]\n"," [ 0.00361845]]\n","Iter:  1682 loss =  0.020792712985066335 learning rate =  0.5 update =  [[-0.00241524]\n"," [-0.00241523]\n"," [ 0.00361633]]\n","Iter:  1683 loss =  0.020780344341298385 learning rate =  0.5 update =  [[-0.00241382]\n"," [-0.00241382]\n"," [ 0.00361421]]\n","Iter:  1684 loss =  0.020767990213524507 learning rate =  0.5 update =  [[-0.0024124 ]\n"," [-0.0024124 ]\n"," [ 0.00361209]]\n","Iter:  1685 loss =  0.020755650576419267 learning rate =  0.5 update =  [[-0.00241098]\n"," [-0.00241098]\n"," [ 0.00360997]]\n","Iter:  1686 loss =  0.020743325404715685 learning rate =  0.5 update =  [[-0.00240957]\n"," [-0.00240957]\n"," [ 0.00360786]]\n","Iter:  1687 loss =  0.0207310146732053 learning rate =  0.5 update =  [[-0.00240816]\n"," [-0.00240815]\n"," [ 0.00360575]]\n","Iter:  1688 loss =  0.020718718356737734 learning rate =  0.5 update =  [[-0.00240675]\n"," [-0.00240674]\n"," [ 0.00360364]]\n","Iter:  1689 loss =  0.020706436430220698 learning rate =  0.5 update =  [[-0.00240534]\n"," [-0.00240533]\n"," [ 0.00360153]]\n","Iter:  1690 loss =  0.02069416886861964 learning rate =  0.5 update =  [[-0.00240393]\n"," [-0.00240393]\n"," [ 0.00359943]]\n","Iter:  1691 loss =  0.020681915646957868 learning rate =  0.5 update =  [[-0.00240252]\n"," [-0.00240252]\n"," [ 0.00359733]]\n","Iter:  1692 loss =  0.020669676740315925 learning rate =  0.5 update =  [[-0.00240112]\n"," [-0.00240111]\n"," [ 0.00359523]]\n","Iter:  1693 loss =  0.020657452123831972 learning rate =  0.5 update =  [[-0.00239971]\n"," [-0.00239971]\n"," [ 0.00359313]]\n","Iter:  1694 loss =  0.020645241772701314 learning rate =  0.5 update =  [[-0.00239831]\n"," [-0.00239831]\n"," [ 0.00359104]]\n","Iter:  1695 loss =  0.02063304566217622 learning rate =  0.5 update =  [[-0.00239691]\n"," [-0.00239691]\n"," [ 0.00358894]]\n","Iter:  1696 loss =  0.020620863767565834 learning rate =  0.5 update =  [[-0.00239552]\n"," [-0.00239551]\n"," [ 0.00358685]]\n","Iter:  1697 loss =  0.02060869606423605 learning rate =  0.5 update =  [[-0.00239412]\n"," [-0.00239412]\n"," [ 0.00358477]]\n","Iter:  1698 loss =  0.02059654252760918 learning rate =  0.5 update =  [[-0.00239272]\n"," [-0.00239272]\n"," [ 0.00358268]]\n","Iter:  1699 loss =  0.02058440313316417 learning rate =  0.5 update =  [[-0.00239133]\n"," [-0.00239133]\n"," [ 0.0035806 ]]\n","Iter:  1700 loss =  0.020572277856435832 learning rate =  0.5 update =  [[-0.00238994]\n"," [-0.00238994]\n"," [ 0.00357852]]\n","Iter:  1701 loss =  0.020560166673015332 learning rate =  0.5 update =  [[-0.00238855]\n"," [-0.00238855]\n"," [ 0.00357644]]\n","Iter:  1702 loss =  0.020548069558549605 learning rate =  0.5 update =  [[-0.00238716]\n"," [-0.00238716]\n"," [ 0.00357437]]\n","Iter:  1703 loss =  0.02053598648874138 learning rate =  0.5 update =  [[-0.00238577]\n"," [-0.00238577]\n"," [ 0.0035723 ]]\n","Iter:  1704 loss =  0.020523917439348834 learning rate =  0.5 update =  [[-0.00238439]\n"," [-0.00238438]\n"," [ 0.00357022]]\n","Iter:  1705 loss =  0.020511862386185735 learning rate =  0.5 update =  [[-0.002383  ]\n"," [-0.002383  ]\n"," [ 0.00356816]]\n","Iter:  1706 loss =  0.020499821305121093 learning rate =  0.5 update =  [[-0.00238162]\n"," [-0.00238162]\n"," [ 0.00356609]]\n","Iter:  1707 loss =  0.020487794172078893 learning rate =  0.5 update =  [[-0.00238024]\n"," [-0.00238024]\n"," [ 0.00356403]]\n","Iter:  1708 loss =  0.02047578096303812 learning rate =  0.5 update =  [[-0.00237886]\n"," [-0.00237886]\n"," [ 0.00356197]]\n","Iter:  1709 loss =  0.020463781654032686 learning rate =  0.5 update =  [[-0.00237748]\n"," [-0.00237748]\n"," [ 0.00355991]]\n","Iter:  1710 loss =  0.020451796221151033 learning rate =  0.5 update =  [[-0.00237611]\n"," [-0.00237611]\n"," [ 0.00355785]]\n","Iter:  1711 loss =  0.02043982464053604 learning rate =  0.5 update =  [[-0.00237473]\n"," [-0.00237473]\n"," [ 0.0035558 ]]\n","Iter:  1712 loss =  0.020427866888385035 learning rate =  0.5 update =  [[-0.00237336]\n"," [-0.00237336]\n"," [ 0.00355375]]\n","Iter:  1713 loss =  0.020415922940949457 learning rate =  0.5 update =  [[-0.00237199]\n"," [-0.00237199]\n"," [ 0.0035517 ]]\n","Iter:  1714 loss =  0.020403992774534765 learning rate =  0.5 update =  [[-0.00237062]\n"," [-0.00237062]\n"," [ 0.00354965]]\n","Iter:  1715 loss =  0.020392076365500316 learning rate =  0.5 update =  [[-0.00236925]\n"," [-0.00236925]\n"," [ 0.00354761]]\n","Iter:  1716 loss =  0.020380173690259136 learning rate =  0.5 update =  [[-0.00236789]\n"," [-0.00236788]\n"," [ 0.00354556]]\n","Iter:  1717 loss =  0.02036828472527788 learning rate =  0.5 update =  [[-0.00236652]\n"," [-0.00236652]\n"," [ 0.00354352]]\n","Iter:  1718 loss =  0.020356409447076555 learning rate =  0.5 update =  [[-0.00236516]\n"," [-0.00236515]\n"," [ 0.00354149]]\n","Iter:  1719 loss =  0.020344547832228516 learning rate =  0.5 update =  [[-0.0023638 ]\n"," [-0.00236379]\n"," [ 0.00353945]]\n","Iter:  1720 loss =  0.020332699857360123 learning rate =  0.5 update =  [[-0.00236244]\n"," [-0.00236243]\n"," [ 0.00353742]]\n","Iter:  1721 loss =  0.02032086549915086 learning rate =  0.5 update =  [[-0.00236108]\n"," [-0.00236107]\n"," [ 0.00353539]]\n","Iter:  1722 loss =  0.02030904473433282 learning rate =  0.5 update =  [[-0.00235972]\n"," [-0.00235972]\n"," [ 0.00353336]]\n","Iter:  1723 loss =  0.020297237539690913 learning rate =  0.5 update =  [[-0.00235836]\n"," [-0.00235836]\n"," [ 0.00353133]]\n","Iter:  1724 loss =  0.020285443892062514 learning rate =  0.5 update =  [[-0.00235701]\n"," [-0.00235701]\n"," [ 0.00352931]]\n","Iter:  1725 loss =  0.020273663768337377 learning rate =  0.5 update =  [[-0.00235566]\n"," [-0.00235565]\n"," [ 0.00352729]]\n","Iter:  1726 loss =  0.020261897145457543 learning rate =  0.5 update =  [[-0.00235431]\n"," [-0.0023543 ]\n"," [ 0.00352527]]\n","Iter:  1727 loss =  0.020250144000416935 learning rate =  0.5 update =  [[-0.00235296]\n"," [-0.00235295]\n"," [ 0.00352325]]\n","Iter:  1728 loss =  0.0202384043102616 learning rate =  0.5 update =  [[-0.00235161]\n"," [-0.00235161]\n"," [ 0.00352124]]\n","Iter:  1729 loss =  0.020226678052089365 learning rate =  0.5 update =  [[-0.00235026]\n"," [-0.00235026]\n"," [ 0.00351923]]\n","Iter:  1730 loss =  0.020214965203049527 learning rate =  0.5 update =  [[-0.00234892]\n"," [-0.00234891]\n"," [ 0.00351721]]\n","Iter:  1731 loss =  0.02020326574034302 learning rate =  0.5 update =  [[-0.00234757]\n"," [-0.00234757]\n"," [ 0.00351521]]\n","Iter:  1732 loss =  0.0201915796412222 learning rate =  0.5 update =  [[-0.00234623]\n"," [-0.00234623]\n"," [ 0.0035132 ]]\n","Iter:  1733 loss =  0.020179906882990435 learning rate =  0.5 update =  [[-0.00234489]\n"," [-0.00234489]\n"," [ 0.0035112 ]]\n","Iter:  1734 loss =  0.02016824744300225 learning rate =  0.5 update =  [[-0.00234355]\n"," [-0.00234355]\n"," [ 0.0035092 ]]\n","Iter:  1735 loss =  0.02015660129866307 learning rate =  0.5 update =  [[-0.00234222]\n"," [-0.00234221]\n"," [ 0.0035072 ]]\n","Iter:  1736 loss =  0.020144968427429133 learning rate =  0.5 update =  [[-0.00234088]\n"," [-0.00234088]\n"," [ 0.0035052 ]]\n","Iter:  1737 loss =  0.02013334880680724 learning rate =  0.5 update =  [[-0.00233954]\n"," [-0.00233954]\n"," [ 0.00350321]]\n","Iter:  1738 loss =  0.02012174241435484 learning rate =  0.5 update =  [[-0.00233821]\n"," [-0.00233821]\n"," [ 0.00350122]]\n","Iter:  1739 loss =  0.020110149227679574 learning rate =  0.5 update =  [[-0.00233688]\n"," [-0.00233688]\n"," [ 0.00349923]]\n","Iter:  1740 loss =  0.02009856922443936 learning rate =  0.5 update =  [[-0.00233555]\n"," [-0.00233555]\n"," [ 0.00349724]]\n","Iter:  1741 loss =  0.020087002382342147 learning rate =  0.5 update =  [[-0.00233422]\n"," [-0.00233422]\n"," [ 0.00349525]]\n","Iter:  1742 loss =  0.02007544867914593 learning rate =  0.5 update =  [[-0.0023329 ]\n"," [-0.00233289]\n"," [ 0.00349327]]\n","Iter:  1743 loss =  0.0200639080926584 learning rate =  0.5 update =  [[-0.00233157]\n"," [-0.00233157]\n"," [ 0.00349129]]\n","Iter:  1744 loss =  0.02005238060073687 learning rate =  0.5 update =  [[-0.00233025]\n"," [-0.00233024]\n"," [ 0.00348931]]\n","Iter:  1745 loss =  0.020040866181288232 learning rate =  0.5 update =  [[-0.00232892]\n"," [-0.00232892]\n"," [ 0.00348733]]\n","Iter:  1746 loss =  0.02002936481226883 learning rate =  0.5 update =  [[-0.0023276 ]\n"," [-0.0023276 ]\n"," [ 0.00348536]]\n","Iter:  1747 loss =  0.02001787647168416 learning rate =  0.5 update =  [[-0.00232628]\n"," [-0.00232628]\n"," [ 0.00348339]]\n","Iter:  1748 loss =  0.020006401137588718 learning rate =  0.5 update =  [[-0.00232497]\n"," [-0.00232496]\n"," [ 0.00348142]]\n","Iter:  1749 loss =  0.01999493878808626 learning rate =  0.5 update =  [[-0.00232365]\n"," [-0.00232365]\n"," [ 0.00347945]]\n","Iter:  1750 loss =  0.01998348940132902 learning rate =  0.5 update =  [[-0.00232233]\n"," [-0.00232233]\n"," [ 0.00347748]]\n","Iter:  1751 loss =  0.019972052955518203 learning rate =  0.5 update =  [[-0.00232102]\n"," [-0.00232102]\n"," [ 0.00347552]]\n","Iter:  1752 loss =  0.019960629428903552 learning rate =  0.5 update =  [[-0.00231971]\n"," [-0.00231971]\n"," [ 0.00347356]]\n","Iter:  1753 loss =  0.0199492187997831 learning rate =  0.5 update =  [[-0.0023184]\n"," [-0.0023184]\n"," [ 0.0034716]]\n","Iter:  1754 loss =  0.01993782104650319 learning rate =  0.5 update =  [[-0.00231709]\n"," [-0.00231709]\n"," [ 0.00346964]]\n","Iter:  1755 loss =  0.019926436147458423 learning rate =  0.5 update =  [[-0.00231578]\n"," [-0.00231578]\n"," [ 0.00346769]]\n","Iter:  1756 loss =  0.0199150640810914 learning rate =  0.5 update =  [[-0.00231447]\n"," [-0.00231447]\n"," [ 0.00346574]]\n","Iter:  1757 loss =  0.019903704825892623 learning rate =  0.5 update =  [[-0.00231317]\n"," [-0.00231317]\n"," [ 0.00346379]]\n","Iter:  1758 loss =  0.019892358360400333 learning rate =  0.5 update =  [[-0.00231187]\n"," [-0.00231186]\n"," [ 0.00346184]]\n","Iter:  1759 loss =  0.01988102466320036 learning rate =  0.5 update =  [[-0.00231056]\n"," [-0.00231056]\n"," [ 0.00345989]]\n","Iter:  1760 loss =  0.019869703712926068 learning rate =  0.5 update =  [[-0.00230926]\n"," [-0.00230926]\n"," [ 0.00345795]]\n","Iter:  1761 loss =  0.019858395488258268 learning rate =  0.5 update =  [[-0.00230796]\n"," [-0.00230796]\n"," [ 0.00345601]]\n","Iter:  1762 loss =  0.019847099967924848 learning rate =  0.5 update =  [[-0.00230667]\n"," [-0.00230666]\n"," [ 0.00345407]]\n","Iter:  1763 loss =  0.019835817130700928 learning rate =  0.5 update =  [[-0.00230537]\n"," [-0.00230537]\n"," [ 0.00345213]]\n","Iter:  1764 loss =  0.019824546955408626 learning rate =  0.5 update =  [[-0.00230408]\n"," [-0.00230407]\n"," [ 0.0034502 ]]\n","Iter:  1765 loss =  0.01981328942091674 learning rate =  0.5 update =  [[-0.00230278]\n"," [-0.00230278]\n"," [ 0.00344826]]\n","Iter:  1766 loss =  0.019802044506141044 learning rate =  0.5 update =  [[-0.00230149]\n"," [-0.00230149]\n"," [ 0.00344633]]\n","Iter:  1767 loss =  0.019790812190043593 learning rate =  0.5 update =  [[-0.0023002]\n"," [-0.0023002]\n"," [ 0.0034444]]\n","Iter:  1768 loss =  0.019779592451633195 learning rate =  0.5 update =  [[-0.00229891]\n"," [-0.00229891]\n"," [ 0.00344248]]\n","Iter:  1769 loss =  0.019768385269964838 learning rate =  0.5 update =  [[-0.00229762]\n"," [-0.00229762]\n"," [ 0.00344055]]\n","Iter:  1770 loss =  0.01975719062413963 learning rate =  0.5 update =  [[-0.00229634]\n"," [-0.00229634]\n"," [ 0.00343863]]\n","Iter:  1771 loss =  0.01974600849330506 learning rate =  0.5 update =  [[-0.00229505]\n"," [-0.00229505]\n"," [ 0.00343671]]\n","Iter:  1772 loss =  0.01973483885665427 learning rate =  0.5 update =  [[-0.00229377]\n"," [-0.00229377]\n"," [ 0.00343479]]\n","Iter:  1773 loss =  0.01972368169342639 learning rate =  0.5 update =  [[-0.00229249]\n"," [-0.00229249]\n"," [ 0.00343288]]\n","Iter:  1774 loss =  0.01971253698290619 learning rate =  0.5 update =  [[-0.00229121]\n"," [-0.00229121]\n"," [ 0.00343096]]\n","Iter:  1775 loss =  0.01970140470442405 learning rate =  0.5 update =  [[-0.00228993]\n"," [-0.00228993]\n"," [ 0.00342905]]\n","Iter:  1776 loss =  0.01969028483735582 learning rate =  0.5 update =  [[-0.00228865]\n"," [-0.00228865]\n"," [ 0.00342714]]\n","Iter:  1777 loss =  0.01967917736112256 learning rate =  0.5 update =  [[-0.00228738]\n"," [-0.00228737]\n"," [ 0.00342523]]\n","Iter:  1778 loss =  0.019668082255190775 learning rate =  0.5 update =  [[-0.0022861 ]\n"," [-0.0022861 ]\n"," [ 0.00342333]]\n","Iter:  1779 loss =  0.019656999499071768 learning rate =  0.5 update =  [[-0.00228483]\n"," [-0.00228483]\n"," [ 0.00342143]]\n","Iter:  1780 loss =  0.01964592907232203 learning rate =  0.5 update =  [[-0.00228356]\n"," [-0.00228355]\n"," [ 0.00341952]]\n","Iter:  1781 loss =  0.01963487095454269 learning rate =  0.5 update =  [[-0.00228228]\n"," [-0.00228228]\n"," [ 0.00341762]]\n","Iter:  1782 loss =  0.019623825125379753 learning rate =  0.5 update =  [[-0.00228102]\n"," [-0.00228101]\n"," [ 0.00341573]]\n","Iter:  1783 loss =  0.019612791564523814 learning rate =  0.5 update =  [[-0.00227975]\n"," [-0.00227975]\n"," [ 0.00341383]]\n","Iter:  1784 loss =  0.019601770251709772 learning rate =  0.5 update =  [[-0.00227848]\n"," [-0.00227848]\n"," [ 0.00341194]]\n","Iter:  1785 loss =  0.019590761166717076 learning rate =  0.5 update =  [[-0.00227722]\n"," [-0.00227721]\n"," [ 0.00341005]]\n","Iter:  1786 loss =  0.019579764289369216 learning rate =  0.5 update =  [[-0.00227595]\n"," [-0.00227595]\n"," [ 0.00340816]]\n","Iter:  1787 loss =  0.01956877959953401 learning rate =  0.5 update =  [[-0.00227469]\n"," [-0.00227469]\n"," [ 0.00340627]]\n","Iter:  1788 loss =  0.019557807077123034 learning rate =  0.5 update =  [[-0.00227343]\n"," [-0.00227343]\n"," [ 0.00340439]]\n","Iter:  1789 loss =  0.019546846702091798 learning rate =  0.5 update =  [[-0.00227217]\n"," [-0.00227217]\n"," [ 0.00340251]]\n","Iter:  1790 loss =  0.019535898454439773 learning rate =  0.5 update =  [[-0.00227091]\n"," [-0.00227091]\n"," [ 0.00340063]]\n","Iter:  1791 loss =  0.01952496231420977 learning rate =  0.5 update =  [[-0.00226966]\n"," [-0.00226965]\n"," [ 0.00339875]]\n","Iter:  1792 loss =  0.01951403826148821 learning rate =  0.5 update =  [[-0.0022684 ]\n"," [-0.0022684 ]\n"," [ 0.00339687]]\n","Iter:  1793 loss =  0.019503126276404992 learning rate =  0.5 update =  [[-0.00226715]\n"," [-0.00226714]\n"," [ 0.003395  ]]\n","Iter:  1794 loss =  0.019492226339133226 learning rate =  0.5 update =  [[-0.00226589]\n"," [-0.00226589]\n"," [ 0.00339312]]\n","Iter:  1795 loss =  0.019481338429889125 learning rate =  0.5 update =  [[-0.00226464]\n"," [-0.00226464]\n"," [ 0.00339125]]\n","Iter:  1796 loss =  0.019470462528932014 learning rate =  0.5 update =  [[-0.00226339]\n"," [-0.00226339]\n"," [ 0.00338939]]\n","Iter:  1797 loss =  0.01945959861656419 learning rate =  0.5 update =  [[-0.00226214]\n"," [-0.00226214]\n"," [ 0.00338752]]\n","Iter:  1798 loss =  0.019448746673130686 learning rate =  0.5 update =  [[-0.0022609 ]\n"," [-0.00226089]\n"," [ 0.00338566]]\n","Iter:  1799 loss =  0.01943790667901927 learning rate =  0.5 update =  [[-0.00225965]\n"," [-0.00225965]\n"," [ 0.00338379]]\n","Iter:  1800 loss =  0.01942707861466016 learning rate =  0.5 update =  [[-0.00225841]\n"," [-0.0022584 ]\n"," [ 0.00338193]]\n","Iter:  1801 loss =  0.019416262460526225 learning rate =  0.5 update =  [[-0.00225716]\n"," [-0.00225716]\n"," [ 0.00338007]]\n","Iter:  1802 loss =  0.019405458197132533 learning rate =  0.5 update =  [[-0.00225592]\n"," [-0.00225592]\n"," [ 0.00337822]]\n","Iter:  1803 loss =  0.019394665805036546 learning rate =  0.5 update =  [[-0.00225468]\n"," [-0.00225468]\n"," [ 0.00337636]]\n","Iter:  1804 loss =  0.01938388526483777 learning rate =  0.5 update =  [[-0.00225344]\n"," [-0.00225344]\n"," [ 0.00337451]]\n","Iter:  1805 loss =  0.01937311655717753 learning rate =  0.5 update =  [[-0.0022522 ]\n"," [-0.0022522 ]\n"," [ 0.00337266]]\n","Iter:  1806 loss =  0.01936235966273947 learning rate =  0.5 update =  [[-0.00225097]\n"," [-0.00225097]\n"," [ 0.00337081]]\n","Iter:  1807 loss =  0.01935161456224856 learning rate =  0.5 update =  [[-0.00224973]\n"," [-0.00224973]\n"," [ 0.00336897]]\n","Iter:  1808 loss =  0.019340881236471714 learning rate =  0.5 update =  [[-0.0022485 ]\n"," [-0.0022485 ]\n"," [ 0.00336712]]\n","Iter:  1809 loss =  0.0193301596662174 learning rate =  0.5 update =  [[-0.00224727]\n"," [-0.00224726]\n"," [ 0.00336528]]\n","Iter:  1810 loss =  0.019319449832335342 learning rate =  0.5 update =  [[-0.00224604]\n"," [-0.00224603]\n"," [ 0.00336344]]\n","Iter:  1811 loss =  0.019308751715716818 learning rate =  0.5 update =  [[-0.00224481]\n"," [-0.0022448 ]\n"," [ 0.0033616 ]]\n","Iter:  1812 loss =  0.01929806529729424 learning rate =  0.5 update =  [[-0.00224358]\n"," [-0.00224358]\n"," [ 0.00335977]]\n","Iter:  1813 loss =  0.019287390558041066 learning rate =  0.5 update =  [[-0.00224235]\n"," [-0.00224235]\n"," [ 0.00335793]]\n","Iter:  1814 loss =  0.019276727478971908 learning rate =  0.5 update =  [[-0.00224113]\n"," [-0.00224112]\n"," [ 0.0033561 ]]\n","Iter:  1815 loss =  0.01926607604114207 learning rate =  0.5 update =  [[-0.0022399 ]\n"," [-0.0022399 ]\n"," [ 0.00335427]]\n","Iter:  1816 loss =  0.01925543622564775 learning rate =  0.5 update =  [[-0.00223868]\n"," [-0.00223868]\n"," [ 0.00335244]]\n","Iter:  1817 loss =  0.019244808013625908 learning rate =  0.5 update =  [[-0.00223746]\n"," [-0.00223745]\n"," [ 0.00335062]]\n","Iter:  1818 loss =  0.01923419138625395 learning rate =  0.5 update =  [[-0.00223624]\n"," [-0.00223623]\n"," [ 0.00334879]]\n","Iter:  1819 loss =  0.019223586324749777 learning rate =  0.5 update =  [[-0.00223502]\n"," [-0.00223501]\n"," [ 0.00334697]]\n","Iter:  1820 loss =  0.019212992810371594 learning rate =  0.5 update =  [[-0.0022338 ]\n"," [-0.0022338 ]\n"," [ 0.00334515]]\n","Iter:  1821 loss =  0.019202410824417912 learning rate =  0.5 update =  [[-0.00223258]\n"," [-0.00223258]\n"," [ 0.00334333]]\n","Iter:  1822 loss =  0.019191840348227275 learning rate =  0.5 update =  [[-0.00223137]\n"," [-0.00223137]\n"," [ 0.00334151]]\n","Iter:  1823 loss =  0.019181281363178405 learning rate =  0.5 update =  [[-0.00223015]\n"," [-0.00223015]\n"," [ 0.0033397 ]]\n","Iter:  1824 loss =  0.019170733850689854 learning rate =  0.5 update =  [[-0.00222894]\n"," [-0.00222894]\n"," [ 0.00333789]]\n","Iter:  1825 loss =  0.019160197792219953 learning rate =  0.5 update =  [[-0.00222773]\n"," [-0.00222773]\n"," [ 0.00333608]]\n","Iter:  1826 loss =  0.01914967316926687 learning rate =  0.5 update =  [[-0.00222652]\n"," [-0.00222652]\n"," [ 0.00333427]]\n","Iter:  1827 loss =  0.019139159963368173 learning rate =  0.5 update =  [[-0.00222531]\n"," [-0.00222531]\n"," [ 0.00333246]]\n","Iter:  1828 loss =  0.019128658156101087 learning rate =  0.5 update =  [[-0.0022241 ]\n"," [-0.0022241 ]\n"," [ 0.00333066]]\n","Iter:  1829 loss =  0.019118167729082203 learning rate =  0.5 update =  [[-0.0022229 ]\n"," [-0.0022229 ]\n"," [ 0.00332885]]\n","Iter:  1830 loss =  0.019107688663967295 learning rate =  0.5 update =  [[-0.00222169]\n"," [-0.00222169]\n"," [ 0.00332705]]\n","Iter:  1831 loss =  0.01909722094245148 learning rate =  0.5 update =  [[-0.00222049]\n"," [-0.00222049]\n"," [ 0.00332525]]\n","Iter:  1832 loss =  0.019086764546268765 learning rate =  0.5 update =  [[-0.00221929]\n"," [-0.00221929]\n"," [ 0.00332346]]\n","Iter:  1833 loss =  0.01907631945719232 learning rate =  0.5 update =  [[-0.00221809]\n"," [-0.00221808]\n"," [ 0.00332166]]\n","Iter:  1834 loss =  0.019065885657034133 learning rate =  0.5 update =  [[-0.00221689]\n"," [-0.00221688]\n"," [ 0.00331987]]\n","Iter:  1835 loss =  0.01905546312764478 learning rate =  0.5 update =  [[-0.00221569]\n"," [-0.00221569]\n"," [ 0.00331808]]\n","Iter:  1836 loss =  0.01904505185091377 learning rate =  0.5 update =  [[-0.00221449]\n"," [-0.00221449]\n"," [ 0.00331629]]\n","Iter:  1837 loss =  0.019034651808769044 learning rate =  0.5 update =  [[-0.0022133 ]\n"," [-0.00221329]\n"," [ 0.0033145 ]]\n","Iter:  1838 loss =  0.019024262983176887 learning rate =  0.5 update =  [[-0.0022121 ]\n"," [-0.0022121 ]\n"," [ 0.00331271]]\n","Iter:  1839 loss =  0.01901388535614215 learning rate =  0.5 update =  [[-0.00221091]\n"," [-0.00221091]\n"," [ 0.00331093]]\n","Iter:  1840 loss =  0.01900351890970786 learning rate =  0.5 update =  [[-0.00220972]\n"," [-0.00220971]\n"," [ 0.00330915]]\n","Iter:  1841 loss =  0.018993163625955198 learning rate =  0.5 update =  [[-0.00220852]\n"," [-0.00220852]\n"," [ 0.00330737]]\n","Iter:  1842 loss =  0.018982819487003283 learning rate =  0.5 update =  [[-0.00220734]\n"," [-0.00220733]\n"," [ 0.00330559]]\n","Iter:  1843 loss =  0.018972486475009435 learning rate =  0.5 update =  [[-0.00220615]\n"," [-0.00220615]\n"," [ 0.00330381]]\n","Iter:  1844 loss =  0.01896216457216864 learning rate =  0.5 update =  [[-0.00220496]\n"," [-0.00220496]\n"," [ 0.00330204]]\n","Iter:  1845 loss =  0.0189518537607136 learning rate =  0.5 update =  [[-0.00220377]\n"," [-0.00220377]\n"," [ 0.00330027]]\n","Iter:  1846 loss =  0.018941554022914836 learning rate =  0.5 update =  [[-0.00220259]\n"," [-0.00220259]\n"," [ 0.0032985 ]]\n","Iter:  1847 loss =  0.01893126534108034 learning rate =  0.5 update =  [[-0.00220141]\n"," [-0.00220141]\n"," [ 0.00329673]]\n","Iter:  1848 loss =  0.018920987697555562 learning rate =  0.5 update =  [[-0.00220023]\n"," [-0.00220022]\n"," [ 0.00329496]]\n","Iter:  1849 loss =  0.01891072107472322 learning rate =  0.5 update =  [[-0.00219904]\n"," [-0.00219904]\n"," [ 0.0032932 ]]\n","Iter:  1850 loss =  0.018900465455003602 learning rate =  0.5 update =  [[-0.00219787]\n"," [-0.00219786]\n"," [ 0.00329143]]\n","Iter:  1851 loss =  0.018890220820853672 learning rate =  0.5 update =  [[-0.00219669]\n"," [-0.00219669]\n"," [ 0.00328967]]\n","Iter:  1852 loss =  0.018879987154767977 learning rate =  0.5 update =  [[-0.00219551]\n"," [-0.00219551]\n"," [ 0.00328791]]\n","Iter:  1853 loss =  0.018869764439277497 learning rate =  0.5 update =  [[-0.00219434]\n"," [-0.00219433]\n"," [ 0.00328616]]\n","Iter:  1854 loss =  0.01885955265695059 learning rate =  0.5 update =  [[-0.00219316]\n"," [-0.00219316]\n"," [ 0.0032844 ]]\n","Iter:  1855 loss =  0.01884935179039198 learning rate =  0.5 update =  [[-0.00219199]\n"," [-0.00219199]\n"," [ 0.00328265]]\n","Iter:  1856 loss =  0.018839161822243333 learning rate =  0.5 update =  [[-0.00219082]\n"," [-0.00219081]\n"," [ 0.00328089]]\n","Iter:  1857 loss =  0.018828982735182808 learning rate =  0.5 update =  [[-0.00218965]\n"," [-0.00218964]\n"," [ 0.00327914]]\n","Iter:  1858 loss =  0.018818814511924985 learning rate =  0.5 update =  [[-0.00218848]\n"," [-0.00218847]\n"," [ 0.0032774 ]]\n","Iter:  1859 loss =  0.018808657135220928 learning rate =  0.5 update =  [[-0.00218731]\n"," [-0.00218731]\n"," [ 0.00327565]]\n","Iter:  1860 loss =  0.018798510587857916 learning rate =  0.5 update =  [[-0.00218614]\n"," [-0.00218614]\n"," [ 0.00327391]]\n","Iter:  1861 loss =  0.018788374852659502 learning rate =  0.5 update =  [[-0.00218497]\n"," [-0.00218497]\n"," [ 0.00327216]]\n","Iter:  1862 loss =  0.018778249912485202 learning rate =  0.5 update =  [[-0.00218381]\n"," [-0.00218381]\n"," [ 0.00327042]]\n","Iter:  1863 loss =  0.018768135750230765 learning rate =  0.5 update =  [[-0.00218265]\n"," [-0.00218265]\n"," [ 0.00326868]]\n","Iter:  1864 loss =  0.01875803234882767 learning rate =  0.5 update =  [[-0.00218149]\n"," [-0.00218148]\n"," [ 0.00326695]]\n","Iter:  1865 loss =  0.018747939691243196 learning rate =  0.5 update =  [[-0.00218032]\n"," [-0.00218032]\n"," [ 0.00326521]]\n","Iter:  1866 loss =  0.01873785776048056 learning rate =  0.5 update =  [[-0.00217916]\n"," [-0.00217916]\n"," [ 0.00326348]]\n","Iter:  1867 loss =  0.018727786539578378 learning rate =  0.5 update =  [[-0.00217801]\n"," [-0.002178  ]\n"," [ 0.00326175]]\n","Iter:  1868 loss =  0.018717726011611008 learning rate =  0.5 update =  [[-0.00217685]\n"," [-0.00217685]\n"," [ 0.00326002]]\n","Iter:  1869 loss =  0.018707676159688068 learning rate =  0.5 update =  [[-0.00217569]\n"," [-0.00217569]\n"," [ 0.00325829]]\n","Iter:  1870 loss =  0.018697636966954582 learning rate =  0.5 update =  [[-0.00217454]\n"," [-0.00217454]\n"," [ 0.00325656]]\n","Iter:  1871 loss =  0.018687608416590968 learning rate =  0.5 update =  [[-0.00217338]\n"," [-0.00217338]\n"," [ 0.00325484]]\n","Iter:  1872 loss =  0.01867759049181271 learning rate =  0.5 update =  [[-0.00217223]\n"," [-0.00217223]\n"," [ 0.00325311]]\n","Iter:  1873 loss =  0.01866758317587032 learning rate =  0.5 update =  [[-0.00217108]\n"," [-0.00217108]\n"," [ 0.00325139]]\n","Iter:  1874 loss =  0.018657586452049395 learning rate =  0.5 update =  [[-0.00216993]\n"," [-0.00216993]\n"," [ 0.00324967]]\n","Iter:  1875 loss =  0.0186476003036704 learning rate =  0.5 update =  [[-0.00216878]\n"," [-0.00216878]\n"," [ 0.00324796]]\n","Iter:  1876 loss =  0.0186376247140886 learning rate =  0.5 update =  [[-0.00216764]\n"," [-0.00216763]\n"," [ 0.00324624]]\n","Iter:  1877 loss =  0.018627659666694028 learning rate =  0.5 update =  [[-0.00216649]\n"," [-0.00216649]\n"," [ 0.00324453]]\n","Iter:  1878 loss =  0.018617705144911248 learning rate =  0.5 update =  [[-0.00216534]\n"," [-0.00216534]\n"," [ 0.00324282]]\n","Iter:  1879 loss =  0.018607761132199384 learning rate =  0.5 update =  [[-0.0021642 ]\n"," [-0.0021642 ]\n"," [ 0.00324111]]\n","Iter:  1880 loss =  0.018597827612052153 learning rate =  0.5 update =  [[-0.00216306]\n"," [-0.00216306]\n"," [ 0.0032394 ]]\n","Iter:  1881 loss =  0.018587904567997325 learning rate =  0.5 update =  [[-0.00216192]\n"," [-0.00216191]\n"," [ 0.00323769]]\n","Iter:  1882 loss =  0.018577991983597303 learning rate =  0.5 update =  [[-0.00216078]\n"," [-0.00216077]\n"," [ 0.00323599]]\n","Iter:  1883 loss =  0.018568089842448385 learning rate =  0.5 update =  [[-0.00215964]\n"," [-0.00215963]\n"," [ 0.00323428]]\n","Iter:  1884 loss =  0.01855819812818119 learning rate =  0.5 update =  [[-0.0021585 ]\n"," [-0.0021585 ]\n"," [ 0.00323258]]\n","Iter:  1885 loss =  0.018548316824460113 learning rate =  0.5 update =  [[-0.00215736]\n"," [-0.00215736]\n"," [ 0.00323088]]\n","Iter:  1886 loss =  0.018538445914983614 learning rate =  0.5 update =  [[-0.00215623]\n"," [-0.00215622]\n"," [ 0.00322918]]\n","Iter:  1887 loss =  0.018528585383483905 learning rate =  0.5 update =  [[-0.00215509]\n"," [-0.00215509]\n"," [ 0.00322749]]\n","Iter:  1888 loss =  0.018518735213727038 learning rate =  0.5 update =  [[-0.00215396]\n"," [-0.00215396]\n"," [ 0.00322579]]\n","Iter:  1889 loss =  0.01850889538951264 learning rate =  0.5 update =  [[-0.00215283]\n"," [-0.00215283]\n"," [ 0.0032241 ]]\n","Iter:  1890 loss =  0.018499065894673948 learning rate =  0.5 update =  [[-0.0021517 ]\n"," [-0.00215169]\n"," [ 0.00322241]]\n","Iter:  1891 loss =  0.01848924671307761 learning rate =  0.5 update =  [[-0.00215057]\n"," [-0.00215056]\n"," [ 0.00322072]]\n","Iter:  1892 loss =  0.01847943782862372 learning rate =  0.5 update =  [[-0.00214944]\n"," [-0.00214944]\n"," [ 0.00321903]]\n","Iter:  1893 loss =  0.018469639225245717 learning rate =  0.5 update =  [[-0.00214831]\n"," [-0.00214831]\n"," [ 0.00321735]]\n","Iter:  1894 loss =  0.018459850886910142 learning rate =  0.5 update =  [[-0.00214718]\n"," [-0.00214718]\n"," [ 0.00321567]]\n","Iter:  1895 loss =  0.018450072797616887 learning rate =  0.5 update =  [[-0.00214606]\n"," [-0.00214606]\n"," [ 0.00321398]]\n","Iter:  1896 loss =  0.018440304941398693 learning rate =  0.5 update =  [[-0.00214494]\n"," [-0.00214493]\n"," [ 0.0032123 ]]\n","Iter:  1897 loss =  0.018430547302321288 learning rate =  0.5 update =  [[-0.00214381]\n"," [-0.00214381]\n"," [ 0.00321063]]\n","Iter:  1898 loss =  0.018420799864483396 learning rate =  0.5 update =  [[-0.00214269]\n"," [-0.00214269]\n"," [ 0.00320895]]\n","Iter:  1899 loss =  0.018411062612016502 learning rate =  0.5 update =  [[-0.00214157]\n"," [-0.00214157]\n"," [ 0.00320727]]\n","Iter:  1900 loss =  0.018401335529084732 learning rate =  0.5 update =  [[-0.00214045]\n"," [-0.00214045]\n"," [ 0.0032056 ]]\n","Iter:  1901 loss =  0.018391618599884904 learning rate =  0.5 update =  [[-0.00213933]\n"," [-0.00213933]\n"," [ 0.00320393]]\n","Iter:  1902 loss =  0.01838191180864634 learning rate =  0.5 update =  [[-0.00213822]\n"," [-0.00213822]\n"," [ 0.00320226]]\n","Iter:  1903 loss =  0.018372215139630894 learning rate =  0.5 update =  [[-0.0021371 ]\n"," [-0.0021371 ]\n"," [ 0.00320059]]\n","Iter:  1904 loss =  0.01836252857713279 learning rate =  0.5 update =  [[-0.00213599]\n"," [-0.00213598]\n"," [ 0.00319892]]\n","Iter:  1905 loss =  0.018352852105478483 learning rate =  0.5 update =  [[-0.00213487]\n"," [-0.00213487]\n"," [ 0.00319726]]\n","Iter:  1906 loss =  0.01834318570902672 learning rate =  0.5 update =  [[-0.00213376]\n"," [-0.00213376]\n"," [ 0.0031956 ]]\n","Iter:  1907 loss =  0.018333529372168277 learning rate =  0.5 update =  [[-0.00213265]\n"," [-0.00213265]\n"," [ 0.00319394]]\n","Iter:  1908 loss =  0.01832388307932604 learning rate =  0.5 update =  [[-0.00213154]\n"," [-0.00213154]\n"," [ 0.00319228]]\n","Iter:  1909 loss =  0.018314246814954943 learning rate =  0.5 update =  [[-0.00213043]\n"," [-0.00213043]\n"," [ 0.00319062]]\n","Iter:  1910 loss =  0.018304620563541686 learning rate =  0.5 update =  [[-0.00212932]\n"," [-0.00212932]\n"," [ 0.00318896]]\n","Iter:  1911 loss =  0.018295004309604825 learning rate =  0.5 update =  [[-0.00212822]\n"," [-0.00212821]\n"," [ 0.00318731]]\n","Iter:  1912 loss =  0.018285398037694717 learning rate =  0.5 update =  [[-0.00212711]\n"," [-0.00212711]\n"," [ 0.00318565]]\n","Iter:  1913 loss =  0.018275801732393075 learning rate =  0.5 update =  [[-0.00212601]\n"," [-0.00212601]\n"," [ 0.003184  ]]\n","Iter:  1914 loss =  0.018266215378313577 learning rate =  0.5 update =  [[-0.0021249 ]\n"," [-0.0021249 ]\n"," [ 0.00318235]]\n","Iter:  1915 loss =  0.01825663896010111 learning rate =  0.5 update =  [[-0.0021238 ]\n"," [-0.0021238 ]\n"," [ 0.00318071]]\n","Iter:  1916 loss =  0.01824707246243208 learning rate =  0.5 update =  [[-0.0021227 ]\n"," [-0.0021227 ]\n"," [ 0.00317906]]\n","Iter:  1917 loss =  0.01823751587001413 learning rate =  0.5 update =  [[-0.0021216 ]\n"," [-0.0021216 ]\n"," [ 0.00317742]]\n","Iter:  1918 loss =  0.018227969167586223 learning rate =  0.5 update =  [[-0.0021205 ]\n"," [-0.0021205 ]\n"," [ 0.00317577]]\n","Iter:  1919 loss =  0.018218432339918395 learning rate =  0.5 update =  [[-0.0021194 ]\n"," [-0.0021194 ]\n"," [ 0.00317413]]\n","Iter:  1920 loss =  0.018208905371811912 learning rate =  0.5 update =  [[-0.00211831]\n"," [-0.00211831]\n"," [ 0.00317249]]\n","Iter:  1921 loss =  0.01819938824809891 learning rate =  0.5 update =  [[-0.00211721]\n"," [-0.00211721]\n"," [ 0.00317086]]\n","Iter:  1922 loss =  0.018189880953642455 learning rate =  0.5 update =  [[-0.00211612]\n"," [-0.00211612]\n"," [ 0.00316922]]\n","Iter:  1923 loss =  0.018180383473336652 learning rate =  0.5 update =  [[-0.00211503]\n"," [-0.00211502]\n"," [ 0.00316759]]\n","Iter:  1924 loss =  0.018170895792106047 learning rate =  0.5 update =  [[-0.00211393]\n"," [-0.00211393]\n"," [ 0.00316595]]\n","Iter:  1925 loss =  0.01816141789490624 learning rate =  0.5 update =  [[-0.00211284]\n"," [-0.00211284]\n"," [ 0.00316432]]\n","Iter:  1926 loss =  0.01815194976672309 learning rate =  0.5 update =  [[-0.00211175]\n"," [-0.00211175]\n"," [ 0.00316269]]\n","Iter:  1927 loss =  0.018142491392573276 learning rate =  0.5 update =  [[-0.00211066]\n"," [-0.00211066]\n"," [ 0.00316107]]\n","Iter:  1928 loss =  0.018133042757503798 learning rate =  0.5 update =  [[-0.00210958]\n"," [-0.00210958]\n"," [ 0.00315944]]\n","Iter:  1929 loss =  0.01812360384659211 learning rate =  0.5 update =  [[-0.00210849]\n"," [-0.00210849]\n"," [ 0.00315781]]\n","Iter:  1930 loss =  0.018114174644945878 learning rate =  0.5 update =  [[-0.00210741]\n"," [-0.0021074 ]\n"," [ 0.00315619]]\n","Iter:  1931 loss =  0.018104755137703037 learning rate =  0.5 update =  [[-0.00210632]\n"," [-0.00210632]\n"," [ 0.00315457]]\n","Iter:  1932 loss =  0.018095345310031742 learning rate =  0.5 update =  [[-0.00210524]\n"," [-0.00210524]\n"," [ 0.00315295]]\n","Iter:  1933 loss =  0.018085945147130147 learning rate =  0.5 update =  [[-0.00210416]\n"," [-0.00210415]\n"," [ 0.00315133]]\n","Iter:  1934 loss =  0.01807655463422642 learning rate =  0.5 update =  [[-0.00210308]\n"," [-0.00210307]\n"," [ 0.00314972]]\n","Iter:  1935 loss =  0.01806717375657867 learning rate =  0.5 update =  [[-0.002102  ]\n"," [-0.00210199]\n"," [ 0.0031481 ]]\n","Iter:  1936 loss =  0.018057802499474824 learning rate =  0.5 update =  [[-0.00210092]\n"," [-0.00210092]\n"," [ 0.00314649]]\n","Iter:  1937 loss =  0.018048440848232608 learning rate =  0.5 update =  [[-0.00209984]\n"," [-0.00209984]\n"," [ 0.00314488]]\n","Iter:  1938 loss =  0.018039088788199577 learning rate =  0.5 update =  [[-0.00209876]\n"," [-0.00209876]\n"," [ 0.00314327]]\n","Iter:  1939 loss =  0.018029746304752573 learning rate =  0.5 update =  [[-0.00209769]\n"," [-0.00209769]\n"," [ 0.00314166]]\n","Iter:  1940 loss =  0.018020413383298337 learning rate =  0.5 update =  [[-0.00209661]\n"," [-0.00209661]\n"," [ 0.00314006]]\n","Iter:  1941 loss =  0.018011090009272944 learning rate =  0.5 update =  [[-0.00209554]\n"," [-0.00209554]\n"," [ 0.00313845]]\n","Iter:  1942 loss =  0.018001776168141748 learning rate =  0.5 update =  [[-0.00209447]\n"," [-0.00209447]\n"," [ 0.00313685]]\n","Iter:  1943 loss =  0.017992471845399677 learning rate =  0.5 update =  [[-0.0020934 ]\n"," [-0.0020934 ]\n"," [ 0.00313525]]\n","Iter:  1944 loss =  0.017983177026570818 learning rate =  0.5 update =  [[-0.00209233]\n"," [-0.00209233]\n"," [ 0.00313365]]\n","Iter:  1945 loss =  0.017973891697208255 learning rate =  0.5 update =  [[-0.00209126]\n"," [-0.00209126]\n"," [ 0.00313205]]\n","Iter:  1946 loss =  0.0179646158428945 learning rate =  0.5 update =  [[-0.00209019]\n"," [-0.00209019]\n"," [ 0.00313045]]\n","Iter:  1947 loss =  0.017955349449240793 learning rate =  0.5 update =  [[-0.00208912]\n"," [-0.00208912]\n"," [ 0.00312886]]\n","Iter:  1948 loss =  0.017946092501887605 learning rate =  0.5 update =  [[-0.00208806]\n"," [-0.00208806]\n"," [ 0.00312726]]\n","Iter:  1949 loss =  0.017936844986504122 learning rate =  0.5 update =  [[-0.00208699]\n"," [-0.00208699]\n"," [ 0.00312567]]\n","Iter:  1950 loss =  0.01792760688878838 learning rate =  0.5 update =  [[-0.00208593]\n"," [-0.00208593]\n"," [ 0.00312408]]\n","Iter:  1951 loss =  0.017918378194467156 learning rate =  0.5 update =  [[-0.00208487]\n"," [-0.00208487]\n"," [ 0.00312249]]\n","Iter:  1952 loss =  0.01790915888929597 learning rate =  0.5 update =  [[-0.00208381]\n"," [-0.0020838 ]\n"," [ 0.00312091]]\n","Iter:  1953 loss =  0.01789994895905888 learning rate =  0.5 update =  [[-0.00208275]\n"," [-0.00208274]\n"," [ 0.00311932]]\n","Iter:  1954 loss =  0.017890748389568514 learning rate =  0.5 update =  [[-0.00208169]\n"," [-0.00208168]\n"," [ 0.00311774]]\n","Iter:  1955 loss =  0.017881557166665882 learning rate =  0.5 update =  [[-0.00208063]\n"," [-0.00208063]\n"," [ 0.00311616]]\n","Iter:  1956 loss =  0.017872375276220438 learning rate =  0.5 update =  [[-0.00207957]\n"," [-0.00207957]\n"," [ 0.00311457]]\n","Iter:  1957 loss =  0.017863202704129954 learning rate =  0.5 update =  [[-0.00207851]\n"," [-0.00207851]\n"," [ 0.003113  ]]\n","Iter:  1958 loss =  0.017854039436320496 learning rate =  0.5 update =  [[-0.00207746]\n"," [-0.00207746]\n"," [ 0.00311142]]\n","Iter:  1959 loss =  0.017844885458746185 learning rate =  0.5 update =  [[-0.00207641]\n"," [-0.0020764 ]\n"," [ 0.00310984]]\n","Iter:  1960 loss =  0.01783574075738934 learning rate =  0.5 update =  [[-0.00207535]\n"," [-0.00207535]\n"," [ 0.00310827]]\n","Iter:  1961 loss =  0.017826605318260275 learning rate =  0.5 update =  [[-0.0020743]\n"," [-0.0020743]\n"," [ 0.0031067]]\n","Iter:  1962 loss =  0.017817479127397305 learning rate =  0.5 update =  [[-0.00207325]\n"," [-0.00207325]\n"," [ 0.00310512]]\n","Iter:  1963 loss =  0.017808362170866593 learning rate =  0.5 update =  [[-0.0020722 ]\n"," [-0.0020722 ]\n"," [ 0.00310355]]\n","Iter:  1964 loss =  0.017799254434762213 learning rate =  0.5 update =  [[-0.00207115]\n"," [-0.00207115]\n"," [ 0.00310199]]\n","Iter:  1965 loss =  0.01779015590520589 learning rate =  0.5 update =  [[-0.0020701 ]\n"," [-0.0020701 ]\n"," [ 0.00310042]]\n","Iter:  1966 loss =  0.017781066568347055 learning rate =  0.5 update =  [[-0.00206906]\n"," [-0.00206906]\n"," [ 0.00309886]]\n","Iter:  1967 loss =  0.01777198641036297 learning rate =  0.5 update =  [[-0.00206801]\n"," [-0.00206801]\n"," [ 0.00309729]]\n","Iter:  1968 loss =  0.017762915417458038 learning rate =  0.5 update =  [[-0.00206697]\n"," [-0.00206697]\n"," [ 0.00309573]]\n","Iter:  1969 loss =  0.017753853575864575 learning rate =  0.5 update =  [[-0.00206592]\n"," [-0.00206592]\n"," [ 0.00309417]]\n","Iter:  1970 loss =  0.017744800871842062 learning rate =  0.5 update =  [[-0.00206488]\n"," [-0.00206488]\n"," [ 0.00309261]]\n","Iter:  1971 loss =  0.017735757291677205 learning rate =  0.5 update =  [[-0.00206384]\n"," [-0.00206384]\n"," [ 0.00309105]]\n","Iter:  1972 loss =  0.0177267228216845 learning rate =  0.5 update =  [[-0.0020628]\n"," [-0.0020628]\n"," [ 0.0030895]]\n","Iter:  1973 loss =  0.017717697448205054 learning rate =  0.5 update =  [[-0.00206176]\n"," [-0.00206176]\n"," [ 0.00308794]]\n","Iter:  1974 loss =  0.017708681157607493 learning rate =  0.5 update =  [[-0.00206072]\n"," [-0.00206072]\n"," [ 0.00308639]]\n","Iter:  1975 loss =  0.017699673936287415 learning rate =  0.5 update =  [[-0.00205969]\n"," [-0.00205968]\n"," [ 0.00308484]]\n","Iter:  1976 loss =  0.01769067577066743 learning rate =  0.5 update =  [[-0.00205865]\n"," [-0.00205865]\n"," [ 0.00308329]]\n","Iter:  1977 loss =  0.017681686647197076 learning rate =  0.5 update =  [[-0.00205761]\n"," [-0.00205761]\n"," [ 0.00308174]]\n","Iter:  1978 loss =  0.017672706552352823 learning rate =  0.5 update =  [[-0.00205658]\n"," [-0.00205658]\n"," [ 0.0030802 ]]\n","Iter:  1979 loss =  0.01766373547263788 learning rate =  0.5 update =  [[-0.00205555]\n"," [-0.00205555]\n"," [ 0.00307865]]\n","Iter:  1980 loss =  0.017654773394582178 learning rate =  0.5 update =  [[-0.00205452]\n"," [-0.00205451]\n"," [ 0.00307711]]\n","Iter:  1981 loss =  0.01764582030474251 learning rate =  0.5 update =  [[-0.00205348]\n"," [-0.00205348]\n"," [ 0.00307557]]\n","Iter:  1982 loss =  0.017636876189702104 learning rate =  0.5 update =  [[-0.00205245]\n"," [-0.00205245]\n"," [ 0.00307403]]\n","Iter:  1983 loss =  0.017627941036070717 learning rate =  0.5 update =  [[-0.00205143]\n"," [-0.00205142]\n"," [ 0.00307249]]\n","Iter:  1984 loss =  0.017619014830484763 learning rate =  0.5 update =  [[-0.0020504 ]\n"," [-0.0020504 ]\n"," [ 0.00307095]]\n","Iter:  1985 loss =  0.017610097559606915 learning rate =  0.5 update =  [[-0.00204937]\n"," [-0.00204937]\n"," [ 0.00306942]]\n","Iter:  1986 loss =  0.017601189210126312 learning rate =  0.5 update =  [[-0.00204834]\n"," [-0.00204834]\n"," [ 0.00306788]]\n","Iter:  1987 loss =  0.01759228976875827 learning rate =  0.5 update =  [[-0.00204732]\n"," [-0.00204732]\n"," [ 0.00306635]]\n","Iter:  1988 loss =  0.01758339922224441 learning rate =  0.5 update =  [[-0.0020463 ]\n"," [-0.00204629]\n"," [ 0.00306482]]\n","Iter:  1989 loss =  0.017574517557352384 learning rate =  0.5 update =  [[-0.00204527]\n"," [-0.00204527]\n"," [ 0.00306329]]\n","Iter:  1990 loss =  0.017565644760876204 learning rate =  0.5 update =  [[-0.00204425]\n"," [-0.00204425]\n"," [ 0.00306176]]\n","Iter:  1991 loss =  0.017556780819635597 learning rate =  0.5 update =  [[-0.00204323]\n"," [-0.00204323]\n"," [ 0.00306024]]\n","Iter:  1992 loss =  0.017547925720476542 learning rate =  0.5 update =  [[-0.00204221]\n"," [-0.00204221]\n"," [ 0.00305871]]\n","Iter:  1993 loss =  0.01753907945027063 learning rate =  0.5 update =  [[-0.00204119]\n"," [-0.00204119]\n"," [ 0.00305719]]\n","Iter:  1994 loss =  0.017530241995915546 learning rate =  0.5 update =  [[-0.00204017]\n"," [-0.00204017]\n"," [ 0.00305566]]\n","Iter:  1995 loss =  0.017521413344334668 learning rate =  0.5 update =  [[-0.00203916]\n"," [-0.00203916]\n"," [ 0.00305414]]\n","Iter:  1996 loss =  0.017512593482477048 learning rate =  0.5 update =  [[-0.00203814]\n"," [-0.00203814]\n"," [ 0.00305263]]\n","Iter:  1997 loss =  0.01750378239731729 learning rate =  0.5 update =  [[-0.00203713]\n"," [-0.00203713]\n"," [ 0.00305111]]\n","Iter:  1998 loss =  0.017494980075855926 learning rate =  0.5 update =  [[-0.00203611]\n"," [-0.00203611]\n"," [ 0.00304959]]\n","Iter:  1999 loss =  0.017486186505118573 learning rate =  0.5 update =  [[-0.0020351 ]\n"," [-0.0020351 ]\n"," [ 0.00304808]]\n","Iter:  2000 loss =  0.01747740167215671 learning rate =  0.5 update =  [[-0.00203409]\n"," [-0.00203409]\n"," [ 0.00304656]]\n","Iter:  2001 loss =  0.01746862556404688 learning rate =  0.5 update =  [[-0.00203308]\n"," [-0.00203308]\n"," [ 0.00304505]]\n","Iter:  2002 loss =  0.017459858167891213 learning rate =  0.5 update =  [[-0.00203207]\n"," [-0.00203207]\n"," [ 0.00304354]]\n","Iter:  2003 loss =  0.017451099470817043 learning rate =  0.5 update =  [[-0.00203106]\n"," [-0.00203106]\n"," [ 0.00304203]]\n","Iter:  2004 loss =  0.01744234945997684 learning rate =  0.5 update =  [[-0.00203005]\n"," [-0.00203005]\n"," [ 0.00304053]]\n","Iter:  2005 loss =  0.017433608122548246 learning rate =  0.5 update =  [[-0.00202904]\n"," [-0.00202904]\n"," [ 0.00303902]]\n","Iter:  2006 loss =  0.017424875445734143 learning rate =  0.5 update =  [[-0.00202804]\n"," [-0.00202804]\n"," [ 0.00303752]]\n","Iter:  2007 loss =  0.017416151416762272 learning rate =  0.5 update =  [[-0.00202703]\n"," [-0.00202703]\n"," [ 0.00303601]]\n","Iter:  2008 loss =  0.017407436022885435 learning rate =  0.5 update =  [[-0.00202603]\n"," [-0.00202603]\n"," [ 0.00303451]]\n","Iter:  2009 loss =  0.017398729251381275 learning rate =  0.5 update =  [[-0.00202503]\n"," [-0.00202502]\n"," [ 0.00303301]]\n","Iter:  2010 loss =  0.017390031089552394 learning rate =  0.5 update =  [[-0.00202402]\n"," [-0.00202402]\n"," [ 0.00303152]]\n","Iter:  2011 loss =  0.017381341524726025 learning rate =  0.5 update =  [[-0.00202302]\n"," [-0.00202302]\n"," [ 0.00303002]]\n","Iter:  2012 loss =  0.017372660544254234 learning rate =  0.5 update =  [[-0.00202202]\n"," [-0.00202202]\n"," [ 0.00302852]]\n","Iter:  2013 loss =  0.017363988135513833 learning rate =  0.5 update =  [[-0.00202102]\n"," [-0.00202102]\n"," [ 0.00302703]]\n","Iter:  2014 loss =  0.017355324285906085 learning rate =  0.5 update =  [[-0.00202003]\n"," [-0.00202002]\n"," [ 0.00302554]]\n","Iter:  2015 loss =  0.017346668982856833 learning rate =  0.5 update =  [[-0.00201903]\n"," [-0.00201903]\n"," [ 0.00302405]]\n","Iter:  2016 loss =  0.017338022213816445 learning rate =  0.5 update =  [[-0.00201803]\n"," [-0.00201803]\n"," [ 0.00302256]]\n","Iter:  2017 loss =  0.017329383966259767 learning rate =  0.5 update =  [[-0.00201704]\n"," [-0.00201704]\n"," [ 0.00302107]]\n","Iter:  2018 loss =  0.017320754227685954 learning rate =  0.5 update =  [[-0.00201604]\n"," [-0.00201604]\n"," [ 0.00301958]]\n","Iter:  2019 loss =  0.017312132985618404 learning rate =  0.5 update =  [[-0.00201505]\n"," [-0.00201505]\n"," [ 0.0030181 ]]\n","Iter:  2020 loss =  0.017303520227604913 learning rate =  0.5 update =  [[-0.00201406]\n"," [-0.00201406]\n"," [ 0.00301661]]\n","Iter:  2021 loss =  0.01729491594121742 learning rate =  0.5 update =  [[-0.00201307]\n"," [-0.00201307]\n"," [ 0.00301513]]\n","Iter:  2022 loss =  0.01728632011405195 learning rate =  0.5 update =  [[-0.00201208]\n"," [-0.00201208]\n"," [ 0.00301365]]\n","Iter:  2023 loss =  0.01727773273372861 learning rate =  0.5 update =  [[-0.00201109]\n"," [-0.00201109]\n"," [ 0.00301217]]\n","Iter:  2024 loss =  0.017269153787891636 learning rate =  0.5 update =  [[-0.0020101 ]\n"," [-0.0020101 ]\n"," [ 0.00301069]]\n","Iter:  2025 loss =  0.017260583264209057 learning rate =  0.5 update =  [[-0.00200911]\n"," [-0.00200911]\n"," [ 0.00300922]]\n","Iter:  2026 loss =  0.017252021150372922 learning rate =  0.5 update =  [[-0.00200813]\n"," [-0.00200812]\n"," [ 0.00300774]]\n","Iter:  2027 loss =  0.017243467434099147 learning rate =  0.5 update =  [[-0.00200714]\n"," [-0.00200714]\n"," [ 0.00300627]]\n","Iter:  2028 loss =  0.017234922103127333 learning rate =  0.5 update =  [[-0.00200616]\n"," [-0.00200615]\n"," [ 0.00300479]]\n","Iter:  2029 loss =  0.017226385145220784 learning rate =  0.5 update =  [[-0.00200517]\n"," [-0.00200517]\n"," [ 0.00300332]]\n","Iter:  2030 loss =  0.017217856548166783 learning rate =  0.5 update =  [[-0.00200419]\n"," [-0.00200419]\n"," [ 0.00300185]]\n","Iter:  2031 loss =  0.01720933629977575 learning rate =  0.5 update =  [[-0.00200321]\n"," [-0.00200321]\n"," [ 0.00300039]]\n","Iter:  2032 loss =  0.017200824387882063 learning rate =  0.5 update =  [[-0.00200223]\n"," [-0.00200223]\n"," [ 0.00299892]]\n","Iter:  2033 loss =  0.01719232080034343 learning rate =  0.5 update =  [[-0.00200125]\n"," [-0.00200125]\n"," [ 0.00299745]]\n","Iter:  2034 loss =  0.017183825525040997 learning rate =  0.5 update =  [[-0.00200027]\n"," [-0.00200027]\n"," [ 0.00299599]]\n","Iter:  2035 loss =  0.0171753385498795 learning rate =  0.5 update =  [[-0.00199929]\n"," [-0.00199929]\n"," [ 0.00299453]]\n","Iter:  2036 loss =  0.01716685986278658 learning rate =  0.5 update =  [[-0.00199831]\n"," [-0.00199831]\n"," [ 0.00299307]]\n","Iter:  2037 loss =  0.017158389451713607 learning rate =  0.5 update =  [[-0.00199734]\n"," [-0.00199734]\n"," [ 0.00299161]]\n","Iter:  2038 loss =  0.01714992730463492 learning rate =  0.5 update =  [[-0.00199636]\n"," [-0.00199636]\n"," [ 0.00299015]]\n","Iter:  2039 loss =  0.017141473409548115 learning rate =  0.5 update =  [[-0.00199539]\n"," [-0.00199539]\n"," [ 0.00298869]]\n","Iter:  2040 loss =  0.017133027754473888 learning rate =  0.5 update =  [[-0.00199441]\n"," [-0.00199441]\n"," [ 0.00298724]]\n","Iter:  2041 loss =  0.017124590327456016 learning rate =  0.5 update =  [[-0.00199344]\n"," [-0.00199344]\n"," [ 0.00298578]]\n","Iter:  2042 loss =  0.017116161116561137 learning rate =  0.5 update =  [[-0.00199247]\n"," [-0.00199247]\n"," [ 0.00298433]]\n","Iter:  2043 loss =  0.017107740109878974 learning rate =  0.5 update =  [[-0.0019915 ]\n"," [-0.0019915 ]\n"," [ 0.00298288]]\n","Iter:  2044 loss =  0.017099327295522167 learning rate =  0.5 update =  [[-0.00199053]\n"," [-0.00199053]\n"," [ 0.00298143]]\n","Iter:  2045 loss =  0.01709092266162613 learning rate =  0.5 update =  [[-0.00198956]\n"," [-0.00198956]\n"," [ 0.00297998]]\n","Iter:  2046 loss =  0.01708252619634898 learning rate =  0.5 update =  [[-0.0019886 ]\n"," [-0.00198859]\n"," [ 0.00297854]]\n","Iter:  2047 loss =  0.01707413788787166 learning rate =  0.5 update =  [[-0.00198763]\n"," [-0.00198763]\n"," [ 0.00297709]]\n","Iter:  2048 loss =  0.017065757724397775 learning rate =  0.5 update =  [[-0.00198666]\n"," [-0.00198666]\n"," [ 0.00297565]]\n","Iter:  2049 loss =  0.017057385694153587 learning rate =  0.5 update =  [[-0.0019857]\n"," [-0.0019857]\n"," [ 0.0029742]]\n","Iter:  2050 loss =  0.01704902178538777 learning rate =  0.5 update =  [[-0.00198473]\n"," [-0.00198473]\n"," [ 0.00297276]]\n","Iter:  2051 loss =  0.01704066598637176 learning rate =  0.5 update =  [[-0.00198377]\n"," [-0.00198377]\n"," [ 0.00297132]]\n","Iter:  2052 loss =  0.01703231828539913 learning rate =  0.5 update =  [[-0.00198281]\n"," [-0.00198281]\n"," [ 0.00296988]]\n","Iter:  2053 loss =  0.01702397867078612 learning rate =  0.5 update =  [[-0.00198185]\n"," [-0.00198185]\n"," [ 0.00296845]]\n","Iter:  2054 loss =  0.01701564713087118 learning rate =  0.5 update =  [[-0.00198089]\n"," [-0.00198089]\n"," [ 0.00296701]]\n","Iter:  2055 loss =  0.017007323654015284 learning rate =  0.5 update =  [[-0.00197993]\n"," [-0.00197993]\n"," [ 0.00296558]]\n","Iter:  2056 loss =  0.016999008228601193 learning rate =  0.5 update =  [[-0.00197897]\n"," [-0.00197897]\n"," [ 0.00296414]]\n","Iter:  2057 loss =  0.016990700843034286 learning rate =  0.5 update =  [[-0.00197801]\n"," [-0.00197801]\n"," [ 0.00296271]]\n","Iter:  2058 loss =  0.01698240148574199 learning rate =  0.5 update =  [[-0.00197706]\n"," [-0.00197706]\n"," [ 0.00296128]]\n","Iter:  2059 loss =  0.01697411014517363 learning rate =  0.5 update =  [[-0.0019761 ]\n"," [-0.0019761 ]\n"," [ 0.00295985]]\n","Iter:  2060 loss =  0.016965826809800716 learning rate =  0.5 update =  [[-0.00197515]\n"," [-0.00197515]\n"," [ 0.00295842]]\n","Iter:  2061 loss =  0.01695755146811679 learning rate =  0.5 update =  [[-0.00197419]\n"," [-0.00197419]\n"," [ 0.002957  ]]\n","Iter:  2062 loss =  0.0169492841086372 learning rate =  0.5 update =  [[-0.00197324]\n"," [-0.00197324]\n"," [ 0.00295557]]\n","Iter:  2063 loss =  0.01694102471989923 learning rate =  0.5 update =  [[-0.00197229]\n"," [-0.00197229]\n"," [ 0.00295415]]\n","Iter:  2064 loss =  0.01693277329046189 learning rate =  0.5 update =  [[-0.00197134]\n"," [-0.00197134]\n"," [ 0.00295273]]\n","Iter:  2065 loss =  0.016924529808906203 learning rate =  0.5 update =  [[-0.00197039]\n"," [-0.00197039]\n"," [ 0.00295131]]\n","Iter:  2066 loss =  0.016916294263834675 learning rate =  0.5 update =  [[-0.00196944]\n"," [-0.00196944]\n"," [ 0.00294989]]\n","Iter:  2067 loss =  0.016908066643871556 learning rate =  0.5 update =  [[-0.00196849]\n"," [-0.00196849]\n"," [ 0.00294847]]\n","Iter:  2068 loss =  0.016899846937662763 learning rate =  0.5 update =  [[-0.00196754]\n"," [-0.00196754]\n"," [ 0.00294705]]\n","Iter:  2069 loss =  0.01689163513387569 learning rate =  0.5 update =  [[-0.0019666 ]\n"," [-0.0019666 ]\n"," [ 0.00294564]]\n","Iter:  2070 loss =  0.016883431221199403 learning rate =  0.5 update =  [[-0.00196565]\n"," [-0.00196565]\n"," [ 0.00294422]]\n","Iter:  2071 loss =  0.016875235188344183 learning rate =  0.5 update =  [[-0.00196471]\n"," [-0.00196471]\n"," [ 0.00294281]]\n","Iter:  2072 loss =  0.016867047024042056 learning rate =  0.5 update =  [[-0.00196376]\n"," [-0.00196376]\n"," [ 0.0029414 ]]\n","Iter:  2073 loss =  0.0168588667170461 learning rate =  0.5 update =  [[-0.00196282]\n"," [-0.00196282]\n"," [ 0.00293999]]\n","Iter:  2074 loss =  0.01685069425613084 learning rate =  0.5 update =  [[-0.00196188]\n"," [-0.00196188]\n"," [ 0.00293858]]\n","Iter:  2075 loss =  0.016842529630092182 learning rate =  0.5 update =  [[-0.00196094]\n"," [-0.00196094]\n"," [ 0.00293717]]\n","Iter:  2076 loss =  0.016834372827747017 learning rate =  0.5 update =  [[-0.00196   ]\n"," [-0.00196   ]\n"," [ 0.00293576]]\n","Iter:  2077 loss =  0.016826223837933536 learning rate =  0.5 update =  [[-0.00195906]\n"," [-0.00195906]\n"," [ 0.00293436]]\n","Iter:  2078 loss =  0.016818082649511075 learning rate =  0.5 update =  [[-0.00195812]\n"," [-0.00195812]\n"," [ 0.00293296]]\n","Iter:  2079 loss =  0.016809949251359882 learning rate =  0.5 update =  [[-0.00195718]\n"," [-0.00195718]\n"," [ 0.00293155]]\n","Iter:  2080 loss =  0.016801823632381394 learning rate =  0.5 update =  [[-0.00195624]\n"," [-0.00195624]\n"," [ 0.00293015]]\n","Iter:  2081 loss =  0.01679370578149785 learning rate =  0.5 update =  [[-0.00195531]\n"," [-0.00195531]\n"," [ 0.00292875]]\n","Iter:  2082 loss =  0.01678559568765264 learning rate =  0.5 update =  [[-0.00195437]\n"," [-0.00195437]\n"," [ 0.00292736]]\n","Iter:  2083 loss =  0.016777493339809695 learning rate =  0.5 update =  [[-0.00195344]\n"," [-0.00195344]\n"," [ 0.00292596]]\n","Iter:  2084 loss =  0.016769398726954057 learning rate =  0.5 update =  [[-0.00195251]\n"," [-0.00195251]\n"," [ 0.00292456]]\n","Iter:  2085 loss =  0.016761311838091437 learning rate =  0.5 update =  [[-0.00195157]\n"," [-0.00195157]\n"," [ 0.00292317]]\n","Iter:  2086 loss =  0.016753232662248124 learning rate =  0.5 update =  [[-0.00195064]\n"," [-0.00195064]\n"," [ 0.00292178]]\n","Iter:  2087 loss =  0.01674516118847135 learning rate =  0.5 update =  [[-0.00194971]\n"," [-0.00194971]\n"," [ 0.00292039]]\n","Iter:  2088 loss =  0.016737097405828724 learning rate =  0.5 update =  [[-0.00194878]\n"," [-0.00194878]\n"," [ 0.002919  ]]\n","Iter:  2089 loss =  0.016729041303408597 learning rate =  0.5 update =  [[-0.00194785]\n"," [-0.00194785]\n"," [ 0.00291761]]\n","Iter:  2090 loss =  0.016720992870319685 learning rate =  0.5 update =  [[-0.00194693]\n"," [-0.00194693]\n"," [ 0.00291622]]\n","Iter:  2091 loss =  0.016712952095691392 learning rate =  0.5 update =  [[-0.001946  ]\n"," [-0.001946  ]\n"," [ 0.00291483]]\n","Iter:  2092 loss =  0.016704918968673398 learning rate =  0.5 update =  [[-0.00194507]\n"," [-0.00194507]\n"," [ 0.00291345]]\n","Iter:  2093 loss =  0.01669689347843574 learning rate =  0.5 update =  [[-0.00194415]\n"," [-0.00194415]\n"," [ 0.00291206]]\n","Iter:  2094 loss =  0.016688875614168947 learning rate =  0.5 update =  [[-0.00194322]\n"," [-0.00194322]\n"," [ 0.00291068]]\n","Iter:  2095 loss =  0.016680865365083713 learning rate =  0.5 update =  [[-0.0019423]\n"," [-0.0019423]\n"," [ 0.0029093]]\n","Iter:  2096 loss =  0.016672862720411 learning rate =  0.5 update =  [[-0.00194138]\n"," [-0.00194138]\n"," [ 0.00290792]]\n","Iter:  2097 loss =  0.016664867669402013 learning rate =  0.5 update =  [[-0.00194046]\n"," [-0.00194046]\n"," [ 0.00290654]]\n","Iter:  2098 loss =  0.01665688020132794 learning rate =  0.5 update =  [[-0.00193954]\n"," [-0.00193953]\n"," [ 0.00290516]]\n","Iter:  2099 loss =  0.01664890030548032 learning rate =  0.5 update =  [[-0.00193862]\n"," [-0.00193861]\n"," [ 0.00290379]]\n","Iter:  2100 loss =  0.01664092797117052 learning rate =  0.5 update =  [[-0.0019377 ]\n"," [-0.0019377 ]\n"," [ 0.00290241]]\n","Iter:  2101 loss =  0.01663296318773002 learning rate =  0.5 update =  [[-0.00193678]\n"," [-0.00193678]\n"," [ 0.00290104]]\n","Iter:  2102 loss =  0.016625005944510295 learning rate =  0.5 update =  [[-0.00193586]\n"," [-0.00193586]\n"," [ 0.00289967]]\n","Iter:  2103 loss =  0.016617056230882665 learning rate =  0.5 update =  [[-0.00193494]\n"," [-0.00193494]\n"," [ 0.0028983 ]]\n","Iter:  2104 loss =  0.016609114036238295 learning rate =  0.5 update =  [[-0.00193403]\n"," [-0.00193403]\n"," [ 0.00289693]]\n","Iter:  2105 loss =  0.016601179349988178 learning rate =  0.5 update =  [[-0.00193311]\n"," [-0.00193311]\n"," [ 0.00289556]]\n","Iter:  2106 loss =  0.01659325216156316 learning rate =  0.5 update =  [[-0.0019322 ]\n"," [-0.0019322 ]\n"," [ 0.00289419]]\n","Iter:  2107 loss =  0.016585332460413743 learning rate =  0.5 update =  [[-0.00193129]\n"," [-0.00193129]\n"," [ 0.00289283]]\n","Iter:  2108 loss =  0.01657742023601021 learning rate =  0.5 update =  [[-0.00193037]\n"," [-0.00193037]\n"," [ 0.00289146]]\n","Iter:  2109 loss =  0.016569515477842245 learning rate =  0.5 update =  [[-0.00192946]\n"," [-0.00192946]\n"," [ 0.0028901 ]]\n","Iter:  2110 loss =  0.016561618175419426 learning rate =  0.5 update =  [[-0.00192855]\n"," [-0.00192855]\n"," [ 0.00288874]]\n","Iter:  2111 loss =  0.016553728318270584 learning rate =  0.5 update =  [[-0.00192764]\n"," [-0.00192764]\n"," [ 0.00288738]]\n","Iter:  2112 loss =  0.016545845895944267 learning rate =  0.5 update =  [[-0.00192673]\n"," [-0.00192673]\n"," [ 0.00288602]]\n","Iter:  2113 loss =  0.016537970898008472 learning rate =  0.5 update =  [[-0.00192583]\n"," [-0.00192582]\n"," [ 0.00288466]]\n","Iter:  2114 loss =  0.016530103314050477 learning rate =  0.5 update =  [[-0.00192492]\n"," [-0.00192492]\n"," [ 0.0028833 ]]\n","Iter:  2115 loss =  0.01652224313367696 learning rate =  0.5 update =  [[-0.00192401]\n"," [-0.00192401]\n"," [ 0.00288195]]\n","Iter:  2116 loss =  0.016514390346513932 learning rate =  0.5 update =  [[-0.00192311]\n"," [-0.00192311]\n"," [ 0.00288059]]\n","Iter:  2117 loss =  0.01650654494220681 learning rate =  0.5 update =  [[-0.0019222 ]\n"," [-0.0019222 ]\n"," [ 0.00287924]]\n","Iter:  2118 loss =  0.01649870691041999 learning rate =  0.5 update =  [[-0.0019213 ]\n"," [-0.0019213 ]\n"," [ 0.00287789]]\n","Iter:  2119 loss =  0.016490876240837293 learning rate =  0.5 update =  [[-0.0019204 ]\n"," [-0.00192039]\n"," [ 0.00287654]]\n","Iter:  2120 loss =  0.016483052923161542 learning rate =  0.5 update =  [[-0.00191949]\n"," [-0.00191949]\n"," [ 0.00287519]]\n","Iter:  2121 loss =  0.01647523694711461 learning rate =  0.5 update =  [[-0.00191859]\n"," [-0.00191859]\n"," [ 0.00287384]]\n","Iter:  2122 loss =  0.016467428302437623 learning rate =  0.5 update =  [[-0.00191769]\n"," [-0.00191769]\n"," [ 0.00287249]]\n","Iter:  2123 loss =  0.016459626978890533 learning rate =  0.5 update =  [[-0.00191679]\n"," [-0.00191679]\n"," [ 0.00287115]]\n","Iter:  2124 loss =  0.016451832966252338 learning rate =  0.5 update =  [[-0.00191589]\n"," [-0.00191589]\n"," [ 0.0028698 ]]\n","Iter:  2125 loss =  0.016444046254320936 learning rate =  0.5 update =  [[-0.001915  ]\n"," [-0.00191499]\n"," [ 0.00286846]]\n","Iter:  2126 loss =  0.01643626683291319 learning rate =  0.5 update =  [[-0.0019141 ]\n"," [-0.0019141 ]\n"," [ 0.00286712]]\n","Iter:  2127 loss =  0.016428494691864613 learning rate =  0.5 update =  [[-0.0019132 ]\n"," [-0.0019132 ]\n"," [ 0.00286578]]\n","Iter:  2128 loss =  0.016420729821029687 learning rate =  0.5 update =  [[-0.00191231]\n"," [-0.00191231]\n"," [ 0.00286444]]\n","Iter:  2129 loss =  0.016412972210281488 learning rate =  0.5 update =  [[-0.00191141]\n"," [-0.00191141]\n"," [ 0.0028631 ]]\n","Iter:  2130 loss =  0.016405221849511986 learning rate =  0.5 update =  [[-0.00191052]\n"," [-0.00191052]\n"," [ 0.00286177]]\n","Iter:  2131 loss =  0.016397478728631638 learning rate =  0.5 update =  [[-0.00190963]\n"," [-0.00190962]\n"," [ 0.00286043]]\n","Iter:  2132 loss =  0.016389742837569665 learning rate =  0.5 update =  [[-0.00190873]\n"," [-0.00190873]\n"," [ 0.0028591 ]]\n","Iter:  2133 loss =  0.016382014166273762 learning rate =  0.5 update =  [[-0.00190784]\n"," [-0.00190784]\n"," [ 0.00285776]]\n","Iter:  2134 loss =  0.016374292704710094 learning rate =  0.5 update =  [[-0.00190695]\n"," [-0.00190695]\n"," [ 0.00285643]]\n","Iter:  2135 loss =  0.016366578442863516 learning rate =  0.5 update =  [[-0.00190606]\n"," [-0.00190606]\n"," [ 0.0028551 ]]\n","Iter:  2136 loss =  0.01635887137073716 learning rate =  0.5 update =  [[-0.00190517]\n"," [-0.00190517]\n"," [ 0.00285377]]\n","Iter:  2137 loss =  0.016351171478352727 learning rate =  0.5 update =  [[-0.00190428]\n"," [-0.00190428]\n"," [ 0.00285244]]\n","Iter:  2138 loss =  0.016343478755750037 learning rate =  0.5 update =  [[-0.0019034 ]\n"," [-0.0019034 ]\n"," [ 0.00285112]]\n","Iter:  2139 loss =  0.016335793192987582 learning rate =  0.5 update =  [[-0.00190251]\n"," [-0.00190251]\n"," [ 0.00284979]]\n","Iter:  2140 loss =  0.01632811478014177 learning rate =  0.5 update =  [[-0.00190163]\n"," [-0.00190162]\n"," [ 0.00284847]]\n","Iter:  2141 loss =  0.016320443507307422 learning rate =  0.5 update =  [[-0.00190074]\n"," [-0.00190074]\n"," [ 0.00284714]]\n","Iter:  2142 loss =  0.016312779364597612 learning rate =  0.5 update =  [[-0.00189986]\n"," [-0.00189986]\n"," [ 0.00284582]]\n","Iter:  2143 loss =  0.016305122342143534 learning rate =  0.5 update =  [[-0.00189897]\n"," [-0.00189897]\n"," [ 0.0028445 ]]\n","Iter:  2144 loss =  0.0162974724300943 learning rate =  0.5 update =  [[-0.00189809]\n"," [-0.00189809]\n"," [ 0.00284318]]\n","Iter:  2145 loss =  0.016289829618617475 learning rate =  0.5 update =  [[-0.00189721]\n"," [-0.00189721]\n"," [ 0.00284186]]\n","Iter:  2146 loss =  0.016282193897898277 learning rate =  0.5 update =  [[-0.00189633]\n"," [-0.00189633]\n"," [ 0.00284054]]\n","Iter:  2147 loss =  0.016274565258140125 learning rate =  0.5 update =  [[-0.00189545]\n"," [-0.00189545]\n"," [ 0.00283923]]\n","Iter:  2148 loss =  0.0162669436895644 learning rate =  0.5 update =  [[-0.00189457]\n"," [-0.00189457]\n"," [ 0.00283791]]\n","Iter:  2149 loss =  0.016259329182410222 learning rate =  0.5 update =  [[-0.00189369]\n"," [-0.00189369]\n"," [ 0.0028366 ]]\n","Iter:  2150 loss =  0.016251721726934815 learning rate =  0.5 update =  [[-0.00189281]\n"," [-0.00189281]\n"," [ 0.00283529]]\n","Iter:  2151 loss =  0.016244121313413018 learning rate =  0.5 update =  [[-0.00189194]\n"," [-0.00189194]\n"," [ 0.00283398]]\n","Iter:  2152 loss =  0.016236527932137545 learning rate =  0.5 update =  [[-0.00189106]\n"," [-0.00189106]\n"," [ 0.00283267]]\n","Iter:  2153 loss =  0.016228941573418844 learning rate =  0.5 update =  [[-0.00189019]\n"," [-0.00189019]\n"," [ 0.00283136]]\n","Iter:  2154 loss =  0.016221362227585058 learning rate =  0.5 update =  [[-0.00188931]\n"," [-0.00188931]\n"," [ 0.00283005]]\n","Iter:  2155 loss =  0.016213789884982115 learning rate =  0.5 update =  [[-0.00188844]\n"," [-0.00188844]\n"," [ 0.00282874]]\n","Iter:  2156 loss =  0.016206224535973295 learning rate =  0.5 update =  [[-0.00188757]\n"," [-0.00188757]\n"," [ 0.00282744]]\n","Iter:  2157 loss =  0.0161986661709398 learning rate =  0.5 update =  [[-0.0018867 ]\n"," [-0.00188669]\n"," [ 0.00282613]]\n","Iter:  2158 loss =  0.016191114780280128 learning rate =  0.5 update =  [[-0.00188582]\n"," [-0.00188582]\n"," [ 0.00282483]]\n","Iter:  2159 loss =  0.01618357035441038 learning rate =  0.5 update =  [[-0.00188495]\n"," [-0.00188495]\n"," [ 0.00282353]]\n","Iter:  2160 loss =  0.016176032883764003 learning rate =  0.5 update =  [[-0.00188408]\n"," [-0.00188408]\n"," [ 0.00282223]]\n","Iter:  2161 loss =  0.016168502358792112 learning rate =  0.5 update =  [[-0.00188322]\n"," [-0.00188321]\n"," [ 0.00282093]]\n","Iter:  2162 loss =  0.016160978769963065 learning rate =  0.5 update =  [[-0.00188235]\n"," [-0.00188235]\n"," [ 0.00281963]]\n","Iter:  2163 loss =  0.01615346210776241 learning rate =  0.5 update =  [[-0.00188148]\n"," [-0.00188148]\n"," [ 0.00281833]]\n","Iter:  2164 loss =  0.01614595236269336 learning rate =  0.5 update =  [[-0.00188061]\n"," [-0.00188061]\n"," [ 0.00281704]]\n","Iter:  2165 loss =  0.016138449525276027 learning rate =  0.5 update =  [[-0.00187975]\n"," [-0.00187975]\n"," [ 0.00281574]]\n","Iter:  2166 loss =  0.016130953586048127 learning rate =  0.5 update =  [[-0.00187888]\n"," [-0.00187888]\n"," [ 0.00281445]]\n","Iter:  2167 loss =  0.016123464535564107 learning rate =  0.5 update =  [[-0.00187802]\n"," [-0.00187802]\n"," [ 0.00281316]]\n","Iter:  2168 loss =  0.016115982364396042 learning rate =  0.5 update =  [[-0.00187716]\n"," [-0.00187716]\n"," [ 0.00281187]]\n","Iter:  2169 loss =  0.016108507063132733 learning rate =  0.5 update =  [[-0.00187629]\n"," [-0.00187629]\n"," [ 0.00281058]]\n","Iter:  2170 loss =  0.01610103862238026 learning rate =  0.5 update =  [[-0.00187543]\n"," [-0.00187543]\n"," [ 0.00280929]]\n","Iter:  2171 loss =  0.016093577032761798 learning rate =  0.5 update =  [[-0.00187457]\n"," [-0.00187457]\n"," [ 0.002808  ]]\n","Iter:  2172 loss =  0.016086122284917265 learning rate =  0.5 update =  [[-0.00187371]\n"," [-0.00187371]\n"," [ 0.00280672]]\n","Iter:  2173 loss =  0.01607867436950386 learning rate =  0.5 update =  [[-0.00187285]\n"," [-0.00187285]\n"," [ 0.00280543]]\n","Iter:  2174 loss =  0.01607123327719538 learning rate =  0.5 update =  [[-0.00187199]\n"," [-0.00187199]\n"," [ 0.00280415]]\n","Iter:  2175 loss =  0.016063798998682675 learning rate =  0.5 update =  [[-0.00187114]\n"," [-0.00187114]\n"," [ 0.00280286]]\n","Iter:  2176 loss =  0.01605637152467354 learning rate =  0.5 update =  [[-0.00187028]\n"," [-0.00187028]\n"," [ 0.00280158]]\n","Iter:  2177 loss =  0.016048950845892383 learning rate =  0.5 update =  [[-0.00186942]\n"," [-0.00186942]\n"," [ 0.0028003 ]]\n","Iter:  2178 loss =  0.016041536953080377 learning rate =  0.5 update =  [[-0.00186857]\n"," [-0.00186857]\n"," [ 0.00279902]]\n","Iter:  2179 loss =  0.016034129836995654 learning rate =  0.5 update =  [[-0.00186771]\n"," [-0.00186771]\n"," [ 0.00279774]]\n","Iter:  2180 loss =  0.016026729488412775 learning rate =  0.5 update =  [[-0.00186686]\n"," [-0.00186686]\n"," [ 0.00279647]]\n","Iter:  2181 loss =  0.0160193358981231 learning rate =  0.5 update =  [[-0.00186601]\n"," [-0.00186601]\n"," [ 0.00279519]]\n","Iter:  2182 loss =  0.016011949056934555 learning rate =  0.5 update =  [[-0.00186516]\n"," [-0.00186515]\n"," [ 0.00279392]]\n","Iter:  2183 loss =  0.01600456895567177 learning rate =  0.5 update =  [[-0.0018643 ]\n"," [-0.0018643 ]\n"," [ 0.00279264]]\n","Iter:  2184 loss =  0.01599719558517569 learning rate =  0.5 update =  [[-0.00186345]\n"," [-0.00186345]\n"," [ 0.00279137]]\n","Iter:  2185 loss =  0.015989828936303938 learning rate =  0.5 update =  [[-0.0018626]\n"," [-0.0018626]\n"," [ 0.0027901]]\n","Iter:  2186 loss =  0.015982468999930565 learning rate =  0.5 update =  [[-0.00186175]\n"," [-0.00186175]\n"," [ 0.00278883]]\n","Iter:  2187 loss =  0.015975115766946095 learning rate =  0.5 update =  [[-0.00186091]\n"," [-0.0018609 ]\n"," [ 0.00278756]]\n","Iter:  2188 loss =  0.015967769228257305 learning rate =  0.5 update =  [[-0.00186006]\n"," [-0.00186006]\n"," [ 0.00278629]]\n","Iter:  2189 loss =  0.015960429374787474 learning rate =  0.5 update =  [[-0.00185921]\n"," [-0.00185921]\n"," [ 0.00278502]]\n","Iter:  2190 loss =  0.015953096197476188 learning rate =  0.5 update =  [[-0.00185836]\n"," [-0.00185836]\n"," [ 0.00278376]]\n","Iter:  2191 loss =  0.015945769687279274 learning rate =  0.5 update =  [[-0.00185752]\n"," [-0.00185752]\n"," [ 0.00278249]]\n","Iter:  2192 loss =  0.01593844983516881 learning rate =  0.5 update =  [[-0.00185668]\n"," [-0.00185667]\n"," [ 0.00278123]]\n","Iter:  2193 loss =  0.015931136632133067 learning rate =  0.5 update =  [[-0.00185583]\n"," [-0.00185583]\n"," [ 0.00277997]]\n","Iter:  2194 loss =  0.015923830069176616 learning rate =  0.5 update =  [[-0.00185499]\n"," [-0.00185499]\n"," [ 0.00277871]]\n","Iter:  2195 loss =  0.015916530137319924 learning rate =  0.5 update =  [[-0.00185415]\n"," [-0.00185415]\n"," [ 0.00277745]]\n","Iter:  2196 loss =  0.015909236827599944 learning rate =  0.5 update =  [[-0.0018533 ]\n"," [-0.0018533 ]\n"," [ 0.00277619]]\n","Iter:  2197 loss =  0.01590195013106925 learning rate =  0.5 update =  [[-0.00185246]\n"," [-0.00185246]\n"," [ 0.00277493]]\n","Iter:  2198 loss =  0.015894670038796764 learning rate =  0.5 update =  [[-0.00185162]\n"," [-0.00185162]\n"," [ 0.00277367]]\n","Iter:  2199 loss =  0.015887396541867316 learning rate =  0.5 update =  [[-0.00185078]\n"," [-0.00185078]\n"," [ 0.00277242]]\n","Iter:  2200 loss =  0.015880129631381718 learning rate =  0.5 update =  [[-0.00184995]\n"," [-0.00184994]\n"," [ 0.00277117]]\n","Iter:  2201 loss =  0.015872869298456594 learning rate =  0.5 update =  [[-0.00184911]\n"," [-0.00184911]\n"," [ 0.00276991]]\n","Iter:  2202 loss =  0.015865615534224633 learning rate =  0.5 update =  [[-0.00184827]\n"," [-0.00184827]\n"," [ 0.00276866]]\n","Iter:  2203 loss =  0.01585836832983421 learning rate =  0.5 update =  [[-0.00184743]\n"," [-0.00184743]\n"," [ 0.00276741]]\n","Iter:  2204 loss =  0.015851127676449587 learning rate =  0.5 update =  [[-0.0018466 ]\n"," [-0.0018466 ]\n"," [ 0.00276616]]\n","Iter:  2205 loss =  0.015843893565250924 learning rate =  0.5 update =  [[-0.00184576]\n"," [-0.00184576]\n"," [ 0.00276491]]\n","Iter:  2206 loss =  0.015836665987433968 learning rate =  0.5 update =  [[-0.00184493]\n"," [-0.00184493]\n"," [ 0.00276366]]\n","Iter:  2207 loss =  0.01582944493421027 learning rate =  0.5 update =  [[-0.0018441 ]\n"," [-0.0018441 ]\n"," [ 0.00276242]]\n","Iter:  2208 loss =  0.015822230396806938 learning rate =  0.5 update =  [[-0.00184326]\n"," [-0.00184326]\n"," [ 0.00276117]]\n","Iter:  2209 loss =  0.01581502236646691 learning rate =  0.5 update =  [[-0.00184243]\n"," [-0.00184243]\n"," [ 0.00275993]]\n","Iter:  2210 loss =  0.015807820834448642 learning rate =  0.5 update =  [[-0.0018416 ]\n"," [-0.0018416 ]\n"," [ 0.00275868]]\n","Iter:  2211 loss =  0.01580062579202617 learning rate =  0.5 update =  [[-0.00184077]\n"," [-0.00184077]\n"," [ 0.00275744]]\n","Iter:  2212 loss =  0.015793437230488983 learning rate =  0.5 update =  [[-0.00183994]\n"," [-0.00183994]\n"," [ 0.0027562 ]]\n","Iter:  2213 loss =  0.015786255141142208 learning rate =  0.5 update =  [[-0.00183911]\n"," [-0.00183911]\n"," [ 0.00275496]]\n","Iter:  2214 loss =  0.015779079515306462 learning rate =  0.5 update =  [[-0.00183828]\n"," [-0.00183828]\n"," [ 0.00275372]]\n","Iter:  2215 loss =  0.015771910344317623 learning rate =  0.5 update =  [[-0.00183746]\n"," [-0.00183746]\n"," [ 0.00275249]]\n","Iter:  2216 loss =  0.015764747619527078 learning rate =  0.5 update =  [[-0.00183663]\n"," [-0.00183663]\n"," [ 0.00275125]]\n","Iter:  2217 loss =  0.015757591332301743 learning rate =  0.5 update =  [[-0.0018358 ]\n"," [-0.0018358 ]\n"," [ 0.00275001]]\n","Iter:  2218 loss =  0.01575044147402349 learning rate =  0.5 update =  [[-0.00183498]\n"," [-0.00183498]\n"," [ 0.00274878]]\n","Iter:  2219 loss =  0.015743298036089867 learning rate =  0.5 update =  [[-0.00183416]\n"," [-0.00183415]\n"," [ 0.00274755]]\n","Iter:  2220 loss =  0.0157361610099135 learning rate =  0.5 update =  [[-0.00183333]\n"," [-0.00183333]\n"," [ 0.00274631]]\n","Iter:  2221 loss =  0.01572903038692234 learning rate =  0.5 update =  [[-0.00183251]\n"," [-0.00183251]\n"," [ 0.00274508]]\n","Iter:  2222 loss =  0.015721906158559354 learning rate =  0.5 update =  [[-0.00183169]\n"," [-0.00183169]\n"," [ 0.00274385]]\n","Iter:  2223 loss =  0.01571478831628289 learning rate =  0.5 update =  [[-0.00183086]\n"," [-0.00183086]\n"," [ 0.00274262]]\n","Iter:  2224 loss =  0.01570767685156637 learning rate =  0.5 update =  [[-0.00183004]\n"," [-0.00183004]\n"," [ 0.0027414 ]]\n","Iter:  2225 loss =  0.01570057175589816 learning rate =  0.5 update =  [[-0.00182922]\n"," [-0.00182922]\n"," [ 0.00274017]]\n","Iter:  2226 loss =  0.01569347302078202 learning rate =  0.5 update =  [[-0.0018284 ]\n"," [-0.0018284 ]\n"," [ 0.00273894]]\n","Iter:  2227 loss =  0.01568638063773645 learning rate =  0.5 update =  [[-0.00182759]\n"," [-0.00182759]\n"," [ 0.00273772]]\n","Iter:  2228 loss =  0.015679294598294955 learning rate =  0.5 update =  [[-0.00182677]\n"," [-0.00182677]\n"," [ 0.0027365 ]]\n","Iter:  2229 loss =  0.015672214894006244 learning rate =  0.5 update =  [[-0.00182595]\n"," [-0.00182595]\n"," [ 0.00273527]]\n","Iter:  2230 loss =  0.01566514151643374 learning rate =  0.5 update =  [[-0.00182513]\n"," [-0.00182513]\n"," [ 0.00273405]]\n","Iter:  2231 loss =  0.0156580744571558 learning rate =  0.5 update =  [[-0.00182432]\n"," [-0.00182432]\n"," [ 0.00273283]]\n","Iter:  2232 loss =  0.015651013707765817 learning rate =  0.5 update =  [[-0.0018235 ]\n"," [-0.0018235 ]\n"," [ 0.00273161]]\n","Iter:  2233 loss =  0.015643959259871734 learning rate =  0.5 update =  [[-0.00182269]\n"," [-0.00182269]\n"," [ 0.0027304 ]]\n","Iter:  2234 loss =  0.015636911105096622 learning rate =  0.5 update =  [[-0.00182188]\n"," [-0.00182188]\n"," [ 0.00272918]]\n","Iter:  2235 loss =  0.015629869235078114 learning rate =  0.5 update =  [[-0.00182106]\n"," [-0.00182106]\n"," [ 0.00272796]]\n","Iter:  2236 loss =  0.01562283364146856 learning rate =  0.5 update =  [[-0.00182025]\n"," [-0.00182025]\n"," [ 0.00272675]]\n","Iter:  2237 loss =  0.015615804315935115 learning rate =  0.5 update =  [[-0.00181944]\n"," [-0.00181944]\n"," [ 0.00272553]]\n","Iter:  2238 loss =  0.015608781250159617 learning rate =  0.5 update =  [[-0.00181863]\n"," [-0.00181863]\n"," [ 0.00272432]]\n","Iter:  2239 loss =  0.01560176443583845 learning rate =  0.5 update =  [[-0.00181782]\n"," [-0.00181782]\n"," [ 0.00272311]]\n","Iter:  2240 loss =  0.01559475386468264 learning rate =  0.5 update =  [[-0.00181701]\n"," [-0.00181701]\n"," [ 0.0027219 ]]\n","Iter:  2241 loss =  0.015587749528417908 learning rate =  0.5 update =  [[-0.0018162 ]\n"," [-0.0018162 ]\n"," [ 0.00272069]]\n","Iter:  2242 loss =  0.01558075141878433 learning rate =  0.5 update =  [[-0.00181539]\n"," [-0.00181539]\n"," [ 0.00271948]]\n","Iter:  2243 loss =  0.015573759527536642 learning rate =  0.5 update =  [[-0.00181459]\n"," [-0.00181459]\n"," [ 0.00271827]]\n","Iter:  2244 loss =  0.015566773846443966 learning rate =  0.5 update =  [[-0.00181378]\n"," [-0.00181378]\n"," [ 0.00271707]]\n","Iter:  2245 loss =  0.015559794367290044 learning rate =  0.5 update =  [[-0.00181298]\n"," [-0.00181297]\n"," [ 0.00271586]]\n","Iter:  2246 loss =  0.015552821081872698 learning rate =  0.5 update =  [[-0.00181217]\n"," [-0.00181217]\n"," [ 0.00271466]]\n","Iter:  2247 loss =  0.015545853982004507 learning rate =  0.5 update =  [[-0.00181137]\n"," [-0.00181137]\n"," [ 0.00271346]]\n","Iter:  2248 loss =  0.01553889305951224 learning rate =  0.5 update =  [[-0.00181056]\n"," [-0.00181056]\n"," [ 0.00271225]]\n","Iter:  2249 loss =  0.015531938306236937 learning rate =  0.5 update =  [[-0.00180976]\n"," [-0.00180976]\n"," [ 0.00271105]]\n","Iter:  2250 loss =  0.01552498971403405 learning rate =  0.5 update =  [[-0.00180896]\n"," [-0.00180896]\n"," [ 0.00270985]]\n","Iter:  2251 loss =  0.015518047274773218 learning rate =  0.5 update =  [[-0.00180816]\n"," [-0.00180816]\n"," [ 0.00270865]]\n","Iter:  2252 loss =  0.015511110980338406 learning rate =  0.5 update =  [[-0.00180736]\n"," [-0.00180735]\n"," [ 0.00270746]]\n","Iter:  2253 loss =  0.015504180822627563 learning rate =  0.5 update =  [[-0.00180656]\n"," [-0.00180655]\n"," [ 0.00270626]]\n","Iter:  2254 loss =  0.015497256793552985 learning rate =  0.5 update =  [[-0.00180576]\n"," [-0.00180576]\n"," [ 0.00270506]]\n","Iter:  2255 loss =  0.015490338885041123 learning rate =  0.5 update =  [[-0.00180496]\n"," [-0.00180496]\n"," [ 0.00270387]]\n","Iter:  2256 loss =  0.015483427089032498 learning rate =  0.5 update =  [[-0.00180416]\n"," [-0.00180416]\n"," [ 0.00270268]]\n","Iter:  2257 loss =  0.01547652139748162 learning rate =  0.5 update =  [[-0.00180336]\n"," [-0.00180336]\n"," [ 0.00270148]]\n","Iter:  2258 loss =  0.015469621802357173 learning rate =  0.5 update =  [[-0.00180257]\n"," [-0.00180257]\n"," [ 0.00270029]]\n","Iter:  2259 loss =  0.015462728295641819 learning rate =  0.5 update =  [[-0.00180177]\n"," [-0.00180177]\n"," [ 0.0026991 ]]\n","Iter:  2260 loss =  0.0154558408693321 learning rate =  0.5 update =  [[-0.00180097]\n"," [-0.00180097]\n"," [ 0.00269791]]\n","Iter:  2261 loss =  0.015448959515438631 learning rate =  0.5 update =  [[-0.00180018]\n"," [-0.00180018]\n"," [ 0.00269672]]\n","Iter:  2262 loss =  0.01544208422598604 learning rate =  0.5 update =  [[-0.00179939]\n"," [-0.00179939]\n"," [ 0.00269554]]\n","Iter:  2263 loss =  0.01543521499301252 learning rate =  0.5 update =  [[-0.00179859]\n"," [-0.00179859]\n"," [ 0.00269435]]\n","Iter:  2264 loss =  0.015428351808570547 learning rate =  0.5 update =  [[-0.0017978 ]\n"," [-0.0017978 ]\n"," [ 0.00269316]]\n","Iter:  2265 loss =  0.015421494664726058 learning rate =  0.5 update =  [[-0.00179701]\n"," [-0.00179701]\n"," [ 0.00269198]]\n","Iter:  2266 loss =  0.015414643553559064 learning rate =  0.5 update =  [[-0.00179622]\n"," [-0.00179622]\n"," [ 0.0026908 ]]\n","Iter:  2267 loss =  0.01540779846716318 learning rate =  0.5 update =  [[-0.00179543]\n"," [-0.00179543]\n"," [ 0.00268961]]\n","Iter:  2268 loss =  0.015400959397645863 learning rate =  0.5 update =  [[-0.00179464]\n"," [-0.00179464]\n"," [ 0.00268843]]\n","Iter:  2269 loss =  0.015394126337128318 learning rate =  0.5 update =  [[-0.00179385]\n"," [-0.00179385]\n"," [ 0.00268725]]\n","Iter:  2270 loss =  0.015387299277745192 learning rate =  0.5 update =  [[-0.00179306]\n"," [-0.00179306]\n"," [ 0.00268607]]\n","Iter:  2271 loss =  0.015380478211645149 learning rate =  0.5 update =  [[-0.00179227]\n"," [-0.00179227]\n"," [ 0.0026849 ]]\n","Iter:  2272 loss =  0.015373663130990197 learning rate =  0.5 update =  [[-0.00179149]\n"," [-0.00179149]\n"," [ 0.00268372]]\n","Iter:  2273 loss =  0.015366854027956093 learning rate =  0.5 update =  [[-0.0017907 ]\n"," [-0.0017907 ]\n"," [ 0.00268254]]\n","Iter:  2274 loss =  0.015360050894732047 learning rate =  0.5 update =  [[-0.00178992]\n"," [-0.00178991]\n"," [ 0.00268137]]\n","Iter:  2275 loss =  0.015353253723520997 learning rate =  0.5 update =  [[-0.00178913]\n"," [-0.00178913]\n"," [ 0.00268019]]\n","Iter:  2276 loss =  0.015346462506539012 learning rate =  0.5 update =  [[-0.00178835]\n"," [-0.00178835]\n"," [ 0.00267902]]\n","Iter:  2277 loss =  0.015339677236016164 learning rate =  0.5 update =  [[-0.00178756]\n"," [-0.00178756]\n"," [ 0.00267785]]\n","Iter:  2278 loss =  0.015332897904195496 learning rate =  0.5 update =  [[-0.00178678]\n"," [-0.00178678]\n"," [ 0.00267668]]\n","Iter:  2279 loss =  0.015326124503333769 learning rate =  0.5 update =  [[-0.001786  ]\n"," [-0.001786  ]\n"," [ 0.00267551]]\n","Iter:  2280 loss =  0.015319357025701065 learning rate =  0.5 update =  [[-0.00178522]\n"," [-0.00178522]\n"," [ 0.00267434]]\n","Iter:  2281 loss =  0.015312595463580771 learning rate =  0.5 update =  [[-0.00178444]\n"," [-0.00178444]\n"," [ 0.00267317]]\n","Iter:  2282 loss =  0.015305839809269618 learning rate =  0.5 update =  [[-0.00178366]\n"," [-0.00178366]\n"," [ 0.002672  ]]\n","Iter:  2283 loss =  0.015299090055077662 learning rate =  0.5 update =  [[-0.00178288]\n"," [-0.00178288]\n"," [ 0.00267084]]\n","Iter:  2284 loss =  0.015292346193328366 learning rate =  0.5 update =  [[-0.0017821 ]\n"," [-0.0017821 ]\n"," [ 0.00266967]]\n","Iter:  2285 loss =  0.015285608216358337 learning rate =  0.5 update =  [[-0.00178132]\n"," [-0.00178132]\n"," [ 0.00266851]]\n","Iter:  2286 loss =  0.015278876116517195 learning rate =  0.5 update =  [[-0.00178054]\n"," [-0.00178054]\n"," [ 0.00266735]]\n","Iter:  2287 loss =  0.015272149886168039 learning rate =  0.5 update =  [[-0.00177977]\n"," [-0.00177977]\n"," [ 0.00266618]]\n","Iter:  2288 loss =  0.015265429517687062 learning rate =  0.5 update =  [[-0.00177899]\n"," [-0.00177899]\n"," [ 0.00266502]]\n","Iter:  2289 loss =  0.015258715003463607 learning rate =  0.5 update =  [[-0.00177821]\n"," [-0.00177821]\n"," [ 0.00266386]]\n","Iter:  2290 loss =  0.015252006335899949 learning rate =  0.5 update =  [[-0.00177744]\n"," [-0.00177744]\n"," [ 0.0026627 ]]\n","Iter:  2291 loss =  0.015245303507411741 learning rate =  0.5 update =  [[-0.00177667]\n"," [-0.00177667]\n"," [ 0.00266155]]\n","Iter:  2292 loss =  0.015238606510427438 learning rate =  0.5 update =  [[-0.00177589]\n"," [-0.00177589]\n"," [ 0.00266039]]\n","Iter:  2293 loss =  0.015231915337388737 learning rate =  0.5 update =  [[-0.00177512]\n"," [-0.00177512]\n"," [ 0.00265923]]\n","Iter:  2294 loss =  0.015225229980750002 learning rate =  0.5 update =  [[-0.00177435]\n"," [-0.00177435]\n"," [ 0.00265808]]\n","Iter:  2295 loss =  0.015218550432978845 learning rate =  0.5 update =  [[-0.00177358]\n"," [-0.00177358]\n"," [ 0.00265692]]\n","Iter:  2296 loss =  0.015211876686555815 learning rate =  0.5 update =  [[-0.00177281]\n"," [-0.00177281]\n"," [ 0.00265577]]\n","Iter:  2297 loss =  0.015205208733974143 learning rate =  0.5 update =  [[-0.00177204]\n"," [-0.00177204]\n"," [ 0.00265462]]\n","Iter:  2298 loss =  0.01519854656774022 learning rate =  0.5 update =  [[-0.00177127]\n"," [-0.00177127]\n"," [ 0.00265347]]\n","Iter:  2299 loss =  0.015191890180373207 learning rate =  0.5 update =  [[-0.0017705 ]\n"," [-0.0017705 ]\n"," [ 0.00265232]]\n","Iter:  2300 loss =  0.015185239564404932 learning rate =  0.5 update =  [[-0.00176973]\n"," [-0.00176973]\n"," [ 0.00265117]]\n","Iter:  2301 loss =  0.015178594712380226 learning rate =  0.5 update =  [[-0.00176896]\n"," [-0.00176896]\n"," [ 0.00265002]]\n","Iter:  2302 loss =  0.015171955616856612 learning rate =  0.5 update =  [[-0.0017682 ]\n"," [-0.0017682 ]\n"," [ 0.00264887]]\n","Iter:  2303 loss =  0.015165322270404404 learning rate =  0.5 update =  [[-0.00176743]\n"," [-0.00176743]\n"," [ 0.00264773]]\n","Iter:  2304 loss =  0.015158694665606596 learning rate =  0.5 update =  [[-0.00176666]\n"," [-0.00176666]\n"," [ 0.00264658]]\n","Iter:  2305 loss =  0.015152072795058757 learning rate =  0.5 update =  [[-0.0017659 ]\n"," [-0.0017659 ]\n"," [ 0.00264544]]\n","Iter:  2306 loss =  0.015145456651369339 learning rate =  0.5 update =  [[-0.00176514]\n"," [-0.00176513]\n"," [ 0.0026443 ]]\n","Iter:  2307 loss =  0.01513884622715945 learning rate =  0.5 update =  [[-0.00176437]\n"," [-0.00176437]\n"," [ 0.00264315]]\n","Iter:  2308 loss =  0.015132241515062511 learning rate =  0.5 update =  [[-0.00176361]\n"," [-0.00176361]\n"," [ 0.00264201]]\n","Iter:  2309 loss =  0.01512564250772494 learning rate =  0.5 update =  [[-0.00176285]\n"," [-0.00176285]\n"," [ 0.00264087]]\n","Iter:  2310 loss =  0.015119049197805323 learning rate =  0.5 update =  [[-0.00176209]\n"," [-0.00176208]\n"," [ 0.00263973]]\n","Iter:  2311 loss =  0.015112461577975016 learning rate =  0.5 update =  [[-0.00176132]\n"," [-0.00176132]\n"," [ 0.0026386 ]]\n","Iter:  2312 loss =  0.015105879640917724 learning rate =  0.5 update =  [[-0.00176056]\n"," [-0.00176056]\n"," [ 0.00263746]]\n","Iter:  2313 loss =  0.015099303379329857 learning rate =  0.5 update =  [[-0.0017598 ]\n"," [-0.0017598 ]\n"," [ 0.00263632]]\n","Iter:  2314 loss =  0.015092732785920108 learning rate =  0.5 update =  [[-0.00175905]\n"," [-0.00175905]\n"," [ 0.00263519]]\n","Iter:  2315 loss =  0.015086167853409577 learning rate =  0.5 update =  [[-0.00175829]\n"," [-0.00175829]\n"," [ 0.00263405]]\n","Iter:  2316 loss =  0.01507960857453186 learning rate =  0.5 update =  [[-0.00175753]\n"," [-0.00175753]\n"," [ 0.00263292]]\n","Iter:  2317 loss =  0.01507305494203292 learning rate =  0.5 update =  [[-0.00175677]\n"," [-0.00175677]\n"," [ 0.00263179]]\n","Iter:  2318 loss =  0.015066506948670899 learning rate =  0.5 update =  [[-0.00175602]\n"," [-0.00175602]\n"," [ 0.00263066]]\n","Iter:  2319 loss =  0.015059964587216548 learning rate =  0.5 update =  [[-0.00175526]\n"," [-0.00175526]\n"," [ 0.00262952]]\n","Iter:  2320 loss =  0.015053427850452657 learning rate =  0.5 update =  [[-0.00175451]\n"," [-0.00175451]\n"," [ 0.0026284 ]]\n","Iter:  2321 loss =  0.015046896731174469 learning rate =  0.5 update =  [[-0.00175375]\n"," [-0.00175375]\n"," [ 0.00262727]]\n","Iter:  2322 loss =  0.015040371222189292 learning rate =  0.5 update =  [[-0.001753  ]\n"," [-0.001753  ]\n"," [ 0.00262614]]\n","Iter:  2323 loss =  0.015033851316316814 learning rate =  0.5 update =  [[-0.00175225]\n"," [-0.00175224]\n"," [ 0.00262501]]\n","Iter:  2324 loss =  0.015027337006388863 learning rate =  0.5 update =  [[-0.00175149]\n"," [-0.00175149]\n"," [ 0.00262389]]\n","Iter:  2325 loss =  0.015020828285249293 learning rate =  0.5 update =  [[-0.00175074]\n"," [-0.00175074]\n"," [ 0.00262276]]\n","Iter:  2326 loss =  0.015014325145754404 learning rate =  0.5 update =  [[-0.00174999]\n"," [-0.00174999]\n"," [ 0.00262164]]\n","Iter:  2327 loss =  0.015007827580772246 learning rate =  0.5 update =  [[-0.00174924]\n"," [-0.00174924]\n"," [ 0.00262052]]\n","Iter:  2328 loss =  0.015001335583183253 learning rate =  0.5 update =  [[-0.00174849]\n"," [-0.00174849]\n"," [ 0.00261939]]\n","Iter:  2329 loss =  0.014994849145879799 learning rate =  0.5 update =  [[-0.00174774]\n"," [-0.00174774]\n"," [ 0.00261827]]\n","Iter:  2330 loss =  0.014988368261766194 learning rate =  0.5 update =  [[-0.00174699]\n"," [-0.00174699]\n"," [ 0.00261715]]\n","Iter:  2331 loss =  0.014981892923759052 learning rate =  0.5 update =  [[-0.00174624]\n"," [-0.00174624]\n"," [ 0.00261603]]\n","Iter:  2332 loss =  0.01497542312478672 learning rate =  0.5 update =  [[-0.0017455 ]\n"," [-0.0017455 ]\n"," [ 0.00261492]]\n","Iter:  2333 loss =  0.014968958857789514 learning rate =  0.5 update =  [[-0.00174475]\n"," [-0.00174475]\n"," [ 0.0026138 ]]\n","Iter:  2334 loss =  0.014962500115719757 learning rate =  0.5 update =  [[-0.001744  ]\n"," [-0.001744  ]\n"," [ 0.00261268]]\n","Iter:  2335 loss =  0.014956046891541792 learning rate =  0.5 update =  [[-0.00174326]\n"," [-0.00174326]\n"," [ 0.00261157]]\n","Iter:  2336 loss =  0.014949599178231611 learning rate =  0.5 update =  [[-0.00174251]\n"," [-0.00174251]\n"," [ 0.00261045]]\n","Iter:  2337 loss =  0.014943156968777219 learning rate =  0.5 update =  [[-0.00174177]\n"," [-0.00174177]\n"," [ 0.00260934]]\n","Iter:  2338 loss =  0.014936720256178464 learning rate =  0.5 update =  [[-0.00174103]\n"," [-0.00174103]\n"," [ 0.00260823]]\n","Iter:  2339 loss =  0.014930289033447031 learning rate =  0.5 update =  [[-0.00174028]\n"," [-0.00174028]\n"," [ 0.00260712]]\n","Iter:  2340 loss =  0.014923863293606138 learning rate =  0.5 update =  [[-0.00173954]\n"," [-0.00173954]\n"," [ 0.00260601]]\n","Iter:  2341 loss =  0.014917443029691071 learning rate =  0.5 update =  [[-0.0017388]\n"," [-0.0017388]\n"," [ 0.0026049]]\n","Iter:  2342 loss =  0.014911028234748842 learning rate =  0.5 update =  [[-0.00173806]\n"," [-0.00173806]\n"," [ 0.00260379]]\n","Iter:  2343 loss =  0.014904618901837955 learning rate =  0.5 update =  [[-0.00173732]\n"," [-0.00173732]\n"," [ 0.00260268]]\n","Iter:  2344 loss =  0.014898215024028667 learning rate =  0.5 update =  [[-0.00173658]\n"," [-0.00173658]\n"," [ 0.00260157]]\n","Iter:  2345 loss =  0.014891816594403046 learning rate =  0.5 update =  [[-0.00173584]\n"," [-0.00173584]\n"," [ 0.00260047]]\n","Iter:  2346 loss =  0.014885423606054715 learning rate =  0.5 update =  [[-0.0017351 ]\n"," [-0.0017351 ]\n"," [ 0.00259936]]\n","Iter:  2347 loss =  0.014879036052088861 learning rate =  0.5 update =  [[-0.00173436]\n"," [-0.00173436]\n"," [ 0.00259826]]\n","Iter:  2348 loss =  0.014872653925622377 learning rate =  0.5 update =  [[-0.00173362]\n"," [-0.00173362]\n"," [ 0.00259715]]\n","Iter:  2349 loss =  0.014866277219783494 learning rate =  0.5 update =  [[-0.00173289]\n"," [-0.00173289]\n"," [ 0.00259605]]\n","Iter:  2350 loss =  0.01485990592771226 learning rate =  0.5 update =  [[-0.00173215]\n"," [-0.00173215]\n"," [ 0.00259495]]\n","Iter:  2351 loss =  0.014853540042560117 learning rate =  0.5 update =  [[-0.00173142]\n"," [-0.00173142]\n"," [ 0.00259385]]\n","Iter:  2352 loss =  0.014847179557489877 learning rate =  0.5 update =  [[-0.00173068]\n"," [-0.00173068]\n"," [ 0.00259275]]\n","Iter:  2353 loss =  0.014840824465676139 learning rate =  0.5 update =  [[-0.00172995]\n"," [-0.00172995]\n"," [ 0.00259165]]\n","Iter:  2354 loss =  0.014834474760304691 learning rate =  0.5 update =  [[-0.00172921]\n"," [-0.00172921]\n"," [ 0.00259056]]\n","Iter:  2355 loss =  0.0148281304345727 learning rate =  0.5 update =  [[-0.00172848]\n"," [-0.00172848]\n"," [ 0.00258946]]\n","Iter:  2356 loss =  0.014821791481688933 learning rate =  0.5 update =  [[-0.00172775]\n"," [-0.00172775]\n"," [ 0.00258836]]\n","Iter:  2357 loss =  0.014815457894873464 learning rate =  0.5 update =  [[-0.00172702]\n"," [-0.00172702]\n"," [ 0.00258727]]\n","Iter:  2358 loss =  0.014809129667357571 learning rate =  0.5 update =  [[-0.00172629]\n"," [-0.00172629]\n"," [ 0.00258617]]\n","Iter:  2359 loss =  0.014802806792384155 learning rate =  0.5 update =  [[-0.00172556]\n"," [-0.00172555]\n"," [ 0.00258508]]\n","Iter:  2360 loss =  0.014796489263207115 learning rate =  0.5 update =  [[-0.00172483]\n"," [-0.00172482]\n"," [ 0.00258399]]\n","Iter:  2361 loss =  0.014790177073091777 learning rate =  0.5 update =  [[-0.0017241]\n"," [-0.0017241]\n"," [ 0.0025829]]\n","Iter:  2362 loss =  0.014783870215314767 learning rate =  0.5 update =  [[-0.00172337]\n"," [-0.00172337]\n"," [ 0.00258181]]\n","Iter:  2363 loss =  0.014777568683163746 learning rate =  0.5 update =  [[-0.00172264]\n"," [-0.00172264]\n"," [ 0.00258072]]\n","Iter:  2364 loss =  0.014771272469937959 learning rate =  0.5 update =  [[-0.00172191]\n"," [-0.00172191]\n"," [ 0.00257963]]\n","Iter:  2365 loss =  0.014764981568947365 learning rate =  0.5 update =  [[-0.00172119]\n"," [-0.00172118]\n"," [ 0.00257854]]\n","Iter:  2366 loss =  0.014758695973513394 learning rate =  0.5 update =  [[-0.00172046]\n"," [-0.00172046]\n"," [ 0.00257746]]\n","Iter:  2367 loss =  0.014752415676968587 learning rate =  0.5 update =  [[-0.00171973]\n"," [-0.00171973]\n"," [ 0.00257637]]\n","Iter:  2368 loss =  0.014746140672656553 learning rate =  0.5 update =  [[-0.00171901]\n"," [-0.00171901]\n"," [ 0.00257529]]\n","Iter:  2369 loss =  0.014739870953931906 learning rate =  0.5 update =  [[-0.00171828]\n"," [-0.00171828]\n"," [ 0.0025742 ]]\n","Iter:  2370 loss =  0.014733606514160547 learning rate =  0.5 update =  [[-0.00171756]\n"," [-0.00171756]\n"," [ 0.00257312]]\n","Iter:  2371 loss =  0.014727347346719178 learning rate =  0.5 update =  [[-0.00171684]\n"," [-0.00171684]\n"," [ 0.00257204]]\n","Iter:  2372 loss =  0.01472109344499582 learning rate =  0.5 update =  [[-0.00171611]\n"," [-0.00171611]\n"," [ 0.00257096]]\n","Iter:  2373 loss =  0.01471484480238906 learning rate =  0.5 update =  [[-0.00171539]\n"," [-0.00171539]\n"," [ 0.00256988]]\n","Iter:  2374 loss =  0.014708601412308979 learning rate =  0.5 update =  [[-0.00171467]\n"," [-0.00171467]\n"," [ 0.0025688 ]]\n","Iter:  2375 loss =  0.01470236326817628 learning rate =  0.5 update =  [[-0.00171395]\n"," [-0.00171395]\n"," [ 0.00256772]]\n","Iter:  2376 loss =  0.014696130363422589 learning rate =  0.5 update =  [[-0.00171323]\n"," [-0.00171323]\n"," [ 0.00256664]]\n","Iter:  2377 loss =  0.014689902691490654 learning rate =  0.5 update =  [[-0.00171251]\n"," [-0.00171251]\n"," [ 0.00256556]]\n","Iter:  2378 loss =  0.014683680245833957 learning rate =  0.5 update =  [[-0.00171179]\n"," [-0.00171179]\n"," [ 0.00256449]]\n","Iter:  2379 loss =  0.014677463019916844 learning rate =  0.5 update =  [[-0.00171107]\n"," [-0.00171107]\n"," [ 0.00256341]]\n","Iter:  2380 loss =  0.014671251007214642 learning rate =  0.5 update =  [[-0.00171035]\n"," [-0.00171035]\n"," [ 0.00256234]]\n","Iter:  2381 loss =  0.014665044201213286 learning rate =  0.5 update =  [[-0.00170964]\n"," [-0.00170964]\n"," [ 0.00256127]]\n","Iter:  2382 loss =  0.014658842595409727 learning rate =  0.5 update =  [[-0.00170892]\n"," [-0.00170892]\n"," [ 0.00256019]]\n","Iter:  2383 loss =  0.014652646183311458 learning rate =  0.5 update =  [[-0.00170821]\n"," [-0.0017082 ]\n"," [ 0.00255912]]\n","Iter:  2384 loss =  0.014646454958437012 learning rate =  0.5 update =  [[-0.00170749]\n"," [-0.00170749]\n"," [ 0.00255805]]\n","Iter:  2385 loss =  0.014640268914315354 learning rate =  0.5 update =  [[-0.00170677]\n"," [-0.00170677]\n"," [ 0.00255698]]\n","Iter:  2386 loss =  0.014634088044486408 learning rate =  0.5 update =  [[-0.00170606]\n"," [-0.00170606]\n"," [ 0.00255592]]\n","Iter:  2387 loss =  0.014627912342500576 learning rate =  0.5 update =  [[-0.00170535]\n"," [-0.00170535]\n"," [ 0.00255485]]\n","Iter:  2388 loss =  0.014621741801919177 learning rate =  0.5 update =  [[-0.00170463]\n"," [-0.00170463]\n"," [ 0.00255378]]\n","Iter:  2389 loss =  0.01461557641631398 learning rate =  0.5 update =  [[-0.00170392]\n"," [-0.00170392]\n"," [ 0.00255271]]\n","Iter:  2390 loss =  0.014609416179267285 learning rate =  0.5 update =  [[-0.00170321]\n"," [-0.00170321]\n"," [ 0.00255165]]\n","Iter:  2391 loss =  0.01460326108437231 learning rate =  0.5 update =  [[-0.0017025 ]\n"," [-0.0017025 ]\n"," [ 0.00255059]]\n","Iter:  2392 loss =  0.014597111125232622 learning rate =  0.5 update =  [[-0.00170179]\n"," [-0.00170179]\n"," [ 0.00254952]]\n","Iter:  2393 loss =  0.014590966295462356 learning rate =  0.5 update =  [[-0.00170108]\n"," [-0.00170108]\n"," [ 0.00254846]]\n","Iter:  2394 loss =  0.014584826588686251 learning rate =  0.5 update =  [[-0.00170037]\n"," [-0.00170037]\n"," [ 0.0025474 ]]\n","Iter:  2395 loss =  0.014578691998539528 learning rate =  0.5 update =  [[-0.00169966]\n"," [-0.00169966]\n"," [ 0.00254634]]\n","Iter:  2396 loss =  0.014572562518667839 learning rate =  0.5 update =  [[-0.00169895]\n"," [-0.00169895]\n"," [ 0.00254528]]\n","Iter:  2397 loss =  0.014566438142727456 learning rate =  0.5 update =  [[-0.00169824]\n"," [-0.00169824]\n"," [ 0.00254422]]\n","Iter:  2398 loss =  0.014560318864385 learning rate =  0.5 update =  [[-0.00169754]\n"," [-0.00169754]\n"," [ 0.00254316]]\n","Iter:  2399 loss =  0.014554204677317481 learning rate =  0.5 update =  [[-0.00169683]\n"," [-0.00169683]\n"," [ 0.0025421 ]]\n","Iter:  2400 loss =  0.014548095575212422 learning rate =  0.5 update =  [[-0.00169612]\n"," [-0.00169612]\n"," [ 0.00254105]]\n","Iter:  2401 loss =  0.014541991551767577 learning rate =  0.5 update =  [[-0.00169542]\n"," [-0.00169542]\n"," [ 0.00253999]]\n","Iter:  2402 loss =  0.01453589260069128 learning rate =  0.5 update =  [[-0.00169471]\n"," [-0.00169471]\n"," [ 0.00253894]]\n","Iter:  2403 loss =  0.014529798715701986 learning rate =  0.5 update =  [[-0.00169401]\n"," [-0.00169401]\n"," [ 0.00253788]]\n","Iter:  2404 loss =  0.014523709890528624 learning rate =  0.5 update =  [[-0.0016933 ]\n"," [-0.0016933 ]\n"," [ 0.00253683]]\n","Iter:  2405 loss =  0.014517626118910401 learning rate =  0.5 update =  [[-0.0016926 ]\n"," [-0.0016926 ]\n"," [ 0.00253578]]\n","Iter:  2406 loss =  0.014511547394596744 learning rate =  0.5 update =  [[-0.0016919 ]\n"," [-0.0016919 ]\n"," [ 0.00253473]]\n","Iter:  2407 loss =  0.014505473711347333 learning rate =  0.5 update =  [[-0.0016912 ]\n"," [-0.0016912 ]\n"," [ 0.00253368]]\n","Iter:  2408 loss =  0.014499405062932121 learning rate =  0.5 update =  [[-0.0016905 ]\n"," [-0.0016905 ]\n"," [ 0.00253263]]\n","Iter:  2409 loss =  0.01449334144313125 learning rate =  0.5 update =  [[-0.00168979]\n"," [-0.00168979]\n"," [ 0.00253158]]\n","Iter:  2410 loss =  0.014487282845735115 learning rate =  0.5 update =  [[-0.00168909]\n"," [-0.00168909]\n"," [ 0.00253053]]\n","Iter:  2411 loss =  0.01448122926454434 learning rate =  0.5 update =  [[-0.0016884 ]\n"," [-0.00168839]\n"," [ 0.00252948]]\n","Iter:  2412 loss =  0.014475180693369442 learning rate =  0.5 update =  [[-0.0016877 ]\n"," [-0.0016877 ]\n"," [ 0.00252844]]\n","Iter:  2413 loss =  0.014469137126031345 learning rate =  0.5 update =  [[-0.001687  ]\n"," [-0.001687  ]\n"," [ 0.00252739]]\n","Iter:  2414 loss =  0.014463098556360961 learning rate =  0.5 update =  [[-0.0016863 ]\n"," [-0.0016863 ]\n"," [ 0.00252635]]\n","Iter:  2415 loss =  0.014457064978199246 learning rate =  0.5 update =  [[-0.0016856 ]\n"," [-0.0016856 ]\n"," [ 0.00252531]]\n","Iter:  2416 loss =  0.014451036385397397 learning rate =  0.5 update =  [[-0.00168491]\n"," [-0.00168491]\n"," [ 0.00252426]]\n","Iter:  2417 loss =  0.014445012771816538 learning rate =  0.5 update =  [[-0.00168421]\n"," [-0.00168421]\n"," [ 0.00252322]]\n","Iter:  2418 loss =  0.014438994131327723 learning rate =  0.5 update =  [[-0.00168351]\n"," [-0.00168351]\n"," [ 0.00252218]]\n","Iter:  2419 loss =  0.0144329804578123 learning rate =  0.5 update =  [[-0.00168282]\n"," [-0.00168282]\n"," [ 0.00252114]]\n","Iter:  2420 loss =  0.01442697174516135 learning rate =  0.5 update =  [[-0.00168212]\n"," [-0.00168212]\n"," [ 0.0025201 ]]\n","Iter:  2421 loss =  0.014420967987276065 learning rate =  0.5 update =  [[-0.00168143]\n"," [-0.00168143]\n"," [ 0.00251906]]\n","Iter:  2422 loss =  0.01441496917806749 learning rate =  0.5 update =  [[-0.00168074]\n"," [-0.00168074]\n"," [ 0.00251803]]\n","Iter:  2423 loss =  0.014408975311456646 learning rate =  0.5 update =  [[-0.00168004]\n"," [-0.00168004]\n"," [ 0.00251699]]\n","Iter:  2424 loss =  0.014402986381374416 learning rate =  0.5 update =  [[-0.00167935]\n"," [-0.00167935]\n"," [ 0.00251595]]\n","Iter:  2425 loss =  0.014397002381761784 learning rate =  0.5 update =  [[-0.00167866]\n"," [-0.00167866]\n"," [ 0.00251492]]\n","Iter:  2426 loss =  0.01439102330656922 learning rate =  0.5 update =  [[-0.00167797]\n"," [-0.00167797]\n"," [ 0.00251388]]\n","Iter:  2427 loss =  0.014385049149757355 learning rate =  0.5 update =  [[-0.00167728]\n"," [-0.00167728]\n"," [ 0.00251285]]\n","Iter:  2428 loss =  0.014379079905296525 learning rate =  0.5 update =  [[-0.00167659]\n"," [-0.00167659]\n"," [ 0.00251182]]\n","Iter:  2429 loss =  0.014373115567166968 learning rate =  0.5 update =  [[-0.0016759 ]\n"," [-0.0016759 ]\n"," [ 0.00251079]]\n","Iter:  2430 loss =  0.014367156129358554 learning rate =  0.5 update =  [[-0.00167521]\n"," [-0.00167521]\n"," [ 0.00250976]]\n","Iter:  2431 loss =  0.014361201585871015 learning rate =  0.5 update =  [[-0.00167452]\n"," [-0.00167452]\n"," [ 0.00250873]]\n","Iter:  2432 loss =  0.014355251930713856 learning rate =  0.5 update =  [[-0.00167383]\n"," [-0.00167383]\n"," [ 0.0025077 ]]\n","Iter:  2433 loss =  0.01434930715790627 learning rate =  0.5 update =  [[-0.00167315]\n"," [-0.00167315]\n"," [ 0.00250667]]\n","Iter:  2434 loss =  0.014343367261477145 learning rate =  0.5 update =  [[-0.00167246]\n"," [-0.00167246]\n"," [ 0.00250564]]\n","Iter:  2435 loss =  0.014337432235465082 learning rate =  0.5 update =  [[-0.00167177]\n"," [-0.00167177]\n"," [ 0.00250462]]\n","Iter:  2436 loss =  0.01433150207391836 learning rate =  0.5 update =  [[-0.00167109]\n"," [-0.00167109]\n"," [ 0.00250359]]\n","Iter:  2437 loss =  0.014325576770894962 learning rate =  0.5 update =  [[-0.0016704 ]\n"," [-0.0016704 ]\n"," [ 0.00250256]]\n","Iter:  2438 loss =  0.014319656320462344 learning rate =  0.5 update =  [[-0.00166972]\n"," [-0.00166972]\n"," [ 0.00250154]]\n","Iter:  2439 loss =  0.014313740716697679 learning rate =  0.5 update =  [[-0.00166904]\n"," [-0.00166903]\n"," [ 0.00250052]]\n","Iter:  2440 loss =  0.014307829953687715 learning rate =  0.5 update =  [[-0.00166835]\n"," [-0.00166835]\n"," [ 0.0024995 ]]\n","Iter:  2441 loss =  0.014301924025528847 learning rate =  0.5 update =  [[-0.00166767]\n"," [-0.00166767]\n"," [ 0.00249847]]\n","Iter:  2442 loss =  0.0142960229263269 learning rate =  0.5 update =  [[-0.00166699]\n"," [-0.00166699]\n"," [ 0.00249745]]\n","Iter:  2443 loss =  0.014290126650197358 learning rate =  0.5 update =  [[-0.00166631]\n"," [-0.00166631]\n"," [ 0.00249643]]\n","Iter:  2444 loss =  0.014284235191265073 learning rate =  0.5 update =  [[-0.00166562]\n"," [-0.00166562]\n"," [ 0.00249541]]\n","Iter:  2445 loss =  0.014278348543664481 learning rate =  0.5 update =  [[-0.00166494]\n"," [-0.00166494]\n"," [ 0.0024944 ]]\n","Iter:  2446 loss =  0.014272466701539515 learning rate =  0.5 update =  [[-0.00166426]\n"," [-0.00166426]\n"," [ 0.00249338]]\n","Iter:  2447 loss =  0.014266589659043493 learning rate =  0.5 update =  [[-0.00166358]\n"," [-0.00166358]\n"," [ 0.00249236]]\n","Iter:  2448 loss =  0.014260717410339272 learning rate =  0.5 update =  [[-0.00166291]\n"," [-0.00166291]\n"," [ 0.00249135]]\n","Iter:  2449 loss =  0.014254849949599135 learning rate =  0.5 update =  [[-0.00166223]\n"," [-0.00166223]\n"," [ 0.00249033]]\n","Iter:  2450 loss =  0.01424898727100455 learning rate =  0.5 update =  [[-0.00166155]\n"," [-0.00166155]\n"," [ 0.00248932]]\n","Iter:  2451 loss =  0.014243129368746535 learning rate =  0.5 update =  [[-0.00166087]\n"," [-0.00166087]\n"," [ 0.0024883 ]]\n","Iter:  2452 loss =  0.01423727623702554 learning rate =  0.5 update =  [[-0.0016602 ]\n"," [-0.0016602 ]\n"," [ 0.00248729]]\n","Iter:  2453 loss =  0.014231427870051274 learning rate =  0.5 update =  [[-0.00165952]\n"," [-0.00165952]\n"," [ 0.00248628]]\n","Iter:  2454 loss =  0.01422558426204275 learning rate =  0.5 update =  [[-0.00165884]\n"," [-0.00165884]\n"," [ 0.00248527]]\n","Iter:  2455 loss =  0.014219745407228254 learning rate =  0.5 update =  [[-0.00165817]\n"," [-0.00165817]\n"," [ 0.00248426]]\n","Iter:  2456 loss =  0.014213911299845549 learning rate =  0.5 update =  [[-0.00165749]\n"," [-0.00165749]\n"," [ 0.00248325]]\n","Iter:  2457 loss =  0.014208081934141387 learning rate =  0.5 update =  [[-0.00165682]\n"," [-0.00165682]\n"," [ 0.00248224]]\n","Iter:  2458 loss =  0.01420225730437197 learning rate =  0.5 update =  [[-0.00165615]\n"," [-0.00165615]\n"," [ 0.00248123]]\n","Iter:  2459 loss =  0.014196437404802763 learning rate =  0.5 update =  [[-0.00165547]\n"," [-0.00165547]\n"," [ 0.00248023]]\n","Iter:  2460 loss =  0.01419062222970826 learning rate =  0.5 update =  [[-0.0016548 ]\n"," [-0.0016548 ]\n"," [ 0.00247922]]\n","Iter:  2461 loss =  0.014184811773372275 learning rate =  0.5 update =  [[-0.00165413]\n"," [-0.00165413]\n"," [ 0.00247822]]\n","Iter:  2462 loss =  0.014179006030087808 learning rate =  0.5 update =  [[-0.00165346]\n"," [-0.00165346]\n"," [ 0.00247721]]\n","Iter:  2463 loss =  0.014173204994157026 learning rate =  0.5 update =  [[-0.00165279]\n"," [-0.00165279]\n"," [ 0.00247621]]\n","Iter:  2464 loss =  0.014167408659891158 learning rate =  0.5 update =  [[-0.00165212]\n"," [-0.00165212]\n"," [ 0.0024752 ]]\n","Iter:  2465 loss =  0.014161617021610538 learning rate =  0.5 update =  [[-0.00165145]\n"," [-0.00165145]\n"," [ 0.0024742 ]]\n","Iter:  2466 loss =  0.014155830073644706 learning rate =  0.5 update =  [[-0.00165078]\n"," [-0.00165078]\n"," [ 0.0024732 ]]\n","Iter:  2467 loss =  0.014150047810332276 learning rate =  0.5 update =  [[-0.00165011]\n"," [-0.00165011]\n"," [ 0.0024722 ]]\n","Iter:  2468 loss =  0.014144270226020958 learning rate =  0.5 update =  [[-0.00164944]\n"," [-0.00164944]\n"," [ 0.0024712 ]]\n","Iter:  2469 loss =  0.014138497315067299 learning rate =  0.5 update =  [[-0.00164878]\n"," [-0.00164877]\n"," [ 0.0024702 ]]\n","Iter:  2470 loss =  0.014132729071837177 learning rate =  0.5 update =  [[-0.00164811]\n"," [-0.00164811]\n"," [ 0.0024692 ]]\n","Iter:  2471 loss =  0.014126965490705218 learning rate =  0.5 update =  [[-0.00164744]\n"," [-0.00164744]\n"," [ 0.00246821]]\n","Iter:  2472 loss =  0.014121206566055255 learning rate =  0.5 update =  [[-0.00164678]\n"," [-0.00164678]\n"," [ 0.00246721]]\n","Iter:  2473 loss =  0.01411545229227994 learning rate =  0.5 update =  [[-0.00164611]\n"," [-0.00164611]\n"," [ 0.00246622]]\n","Iter:  2474 loss =  0.014109702663780982 learning rate =  0.5 update =  [[-0.00164545]\n"," [-0.00164545]\n"," [ 0.00246522]]\n","Iter:  2475 loss =  0.014103957674969041 learning rate =  0.5 update =  [[-0.00164478]\n"," [-0.00164478]\n"," [ 0.00246423]]\n","Iter:  2476 loss =  0.014098217320263697 learning rate =  0.5 update =  [[-0.00164412]\n"," [-0.00164412]\n"," [ 0.00246323]]\n","Iter:  2477 loss =  0.014092481594093232 learning rate =  0.5 update =  [[-0.00164345]\n"," [-0.00164345]\n"," [ 0.00246224]]\n","Iter:  2478 loss =  0.014086750490895094 learning rate =  0.5 update =  [[-0.00164279]\n"," [-0.00164279]\n"," [ 0.00246125]]\n","Iter:  2479 loss =  0.014081024005115546 learning rate =  0.5 update =  [[-0.00164213]\n"," [-0.00164213]\n"," [ 0.00246026]]\n","Iter:  2480 loss =  0.014075302131209534 learning rate =  0.5 update =  [[-0.00164147]\n"," [-0.00164147]\n"," [ 0.00245927]]\n","Iter:  2481 loss =  0.014069584863641042 learning rate =  0.5 update =  [[-0.00164081]\n"," [-0.00164081]\n"," [ 0.00245828]]\n","Iter:  2482 loss =  0.014063872196882802 learning rate =  0.5 update =  [[-0.00164015]\n"," [-0.00164015]\n"," [ 0.00245729]]\n","Iter:  2483 loss =  0.014058164125416314 learning rate =  0.5 update =  [[-0.00163949]\n"," [-0.00163949]\n"," [ 0.0024563 ]]\n","Iter:  2484 loss =  0.014052460643731864 learning rate =  0.5 update =  [[-0.00163883]\n"," [-0.00163883]\n"," [ 0.00245532]]\n","Iter:  2485 loss =  0.014046761746328585 learning rate =  0.5 update =  [[-0.00163817]\n"," [-0.00163817]\n"," [ 0.00245433]]\n","Iter:  2486 loss =  0.01404106742771422 learning rate =  0.5 update =  [[-0.00163751]\n"," [-0.00163751]\n"," [ 0.00245335]]\n","Iter:  2487 loss =  0.01403537768240534 learning rate =  0.5 update =  [[-0.00163685]\n"," [-0.00163685]\n"," [ 0.00245236]]\n","Iter:  2488 loss =  0.014029692504927287 learning rate =  0.5 update =  [[-0.00163619]\n"," [-0.00163619]\n"," [ 0.00245138]]\n","Iter:  2489 loss =  0.014024011889813979 learning rate =  0.5 update =  [[-0.00163554]\n"," [-0.00163554]\n"," [ 0.00245039]]\n","Iter:  2490 loss =  0.014018335831608013 learning rate =  0.5 update =  [[-0.00163488]\n"," [-0.00163488]\n"," [ 0.00244941]]\n","Iter:  2491 loss =  0.014012664324860709 learning rate =  0.5 update =  [[-0.00163422]\n"," [-0.00163422]\n"," [ 0.00244843]]\n","Iter:  2492 loss =  0.014006997364132035 learning rate =  0.5 update =  [[-0.00163357]\n"," [-0.00163357]\n"," [ 0.00244745]]\n","Iter:  2493 loss =  0.014001334943990616 learning rate =  0.5 update =  [[-0.00163291]\n"," [-0.00163291]\n"," [ 0.00244647]]\n","Iter:  2494 loss =  0.01399567705901362 learning rate =  0.5 update =  [[-0.00163226]\n"," [-0.00163226]\n"," [ 0.00244549]]\n","Iter:  2495 loss =  0.013990023703786783 learning rate =  0.5 update =  [[-0.00163161]\n"," [-0.00163161]\n"," [ 0.00244451]]\n","Iter:  2496 loss =  0.01398437487290444 learning rate =  0.5 update =  [[-0.00163095]\n"," [-0.00163095]\n"," [ 0.00244354]]\n","Iter:  2497 loss =  0.013978730560969686 learning rate =  0.5 update =  [[-0.0016303 ]\n"," [-0.0016303 ]\n"," [ 0.00244256]]\n","Iter:  2498 loss =  0.013973090762593794 learning rate =  0.5 update =  [[-0.00162965]\n"," [-0.00162965]\n"," [ 0.00244158]]\n","Iter:  2499 loss =  0.013967455472396854 learning rate =  0.5 update =  [[-0.001629  ]\n"," [-0.001629  ]\n"," [ 0.00244061]]\n","Iter:  2500 loss =  0.01396182468500741 learning rate =  0.5 update =  [[-0.00162835]\n"," [-0.00162835]\n"," [ 0.00243963]]\n","Iter:  2501 loss =  0.013956198395062345 learning rate =  0.5 update =  [[-0.00162769]\n"," [-0.00162769]\n"," [ 0.00243866]]\n","Iter:  2502 loss =  0.013950576597207246 learning rate =  0.5 update =  [[-0.00162704]\n"," [-0.00162704]\n"," [ 0.00243769]]\n","Iter:  2503 loss =  0.013944959286095987 learning rate =  0.5 update =  [[-0.0016264 ]\n"," [-0.00162639]\n"," [ 0.00243671]]\n","Iter:  2504 loss =  0.013939346456390934 learning rate =  0.5 update =  [[-0.00162575]\n"," [-0.00162575]\n"," [ 0.00243574]]\n","Iter:  2505 loss =  0.01393373810276292 learning rate =  0.5 update =  [[-0.0016251 ]\n"," [-0.0016251 ]\n"," [ 0.00243477]]\n","Iter:  2506 loss =  0.013928134219891107 learning rate =  0.5 update =  [[-0.00162445]\n"," [-0.00162445]\n"," [ 0.0024338 ]]\n","Iter:  2507 loss =  0.0139225348024631 learning rate =  0.5 update =  [[-0.0016238 ]\n"," [-0.0016238 ]\n"," [ 0.00243283]]\n","Iter:  2508 loss =  0.013916939845174957 learning rate =  0.5 update =  [[-0.00162315]\n"," [-0.00162315]\n"," [ 0.00243187]]\n","Iter:  2509 loss =  0.013911349342731012 learning rate =  0.5 update =  [[-0.00162251]\n"," [-0.00162251]\n"," [ 0.0024309 ]]\n","Iter:  2510 loss =  0.013905763289843873 learning rate =  0.5 update =  [[-0.00162186]\n"," [-0.00162186]\n"," [ 0.00242993]]\n","Iter:  2511 loss =  0.013900181681234598 learning rate =  0.5 update =  [[-0.00162122]\n"," [-0.00162122]\n"," [ 0.00242897]]\n","Iter:  2512 loss =  0.013894604511632602 learning rate =  0.5 update =  [[-0.00162057]\n"," [-0.00162057]\n"," [ 0.002428  ]]\n","Iter:  2513 loss =  0.013889031775775405 learning rate =  0.5 update =  [[-0.00161993]\n"," [-0.00161993]\n"," [ 0.00242704]]\n","Iter:  2514 loss =  0.013883463468408885 learning rate =  0.5 update =  [[-0.00161928]\n"," [-0.00161928]\n"," [ 0.00242607]]\n","Iter:  2515 loss =  0.013877899584287253 learning rate =  0.5 update =  [[-0.00161864]\n"," [-0.00161864]\n"," [ 0.00242511]]\n","Iter:  2516 loss =  0.013872340118172934 learning rate =  0.5 update =  [[-0.001618  ]\n"," [-0.001618  ]\n"," [ 0.00242415]]\n","Iter:  2517 loss =  0.013866785064836566 learning rate =  0.5 update =  [[-0.00161735]\n"," [-0.00161735]\n"," [ 0.00242319]]\n","Iter:  2518 loss =  0.013861234419056925 learning rate =  0.5 update =  [[-0.00161671]\n"," [-0.00161671]\n"," [ 0.00242222]]\n","Iter:  2519 loss =  0.013855688175621187 learning rate =  0.5 update =  [[-0.00161607]\n"," [-0.00161607]\n"," [ 0.00242126]]\n","Iter:  2520 loss =  0.013850146329324471 learning rate =  0.5 update =  [[-0.00161543]\n"," [-0.00161543]\n"," [ 0.00242031]]\n","Iter:  2521 loss =  0.013844608874970242 learning rate =  0.5 update =  [[-0.00161479]\n"," [-0.00161479]\n"," [ 0.00241935]]\n","Iter:  2522 loss =  0.013839075807370038 learning rate =  0.5 update =  [[-0.00161415]\n"," [-0.00161415]\n"," [ 0.00241839]]\n","Iter:  2523 loss =  0.013833547121343558 learning rate =  0.5 update =  [[-0.00161351]\n"," [-0.00161351]\n"," [ 0.00241743]]\n","Iter:  2524 loss =  0.013828022811718504 learning rate =  0.5 update =  [[-0.00161287]\n"," [-0.00161287]\n"," [ 0.00241648]]\n","Iter:  2525 loss =  0.013822502873330995 learning rate =  0.5 update =  [[-0.00161223]\n"," [-0.00161223]\n"," [ 0.00241552]]\n","Iter:  2526 loss =  0.013816987301024757 learning rate =  0.5 update =  [[-0.00161159]\n"," [-0.00161159]\n"," [ 0.00241457]]\n","Iter:  2527 loss =  0.013811476089652077 learning rate =  0.5 update =  [[-0.00161096]\n"," [-0.00161096]\n"," [ 0.00241361]]\n","Iter:  2528 loss =  0.01380596923407296 learning rate =  0.5 update =  [[-0.00161032]\n"," [-0.00161032]\n"," [ 0.00241266]]\n","Iter:  2529 loss =  0.013800466729155594 learning rate =  0.5 update =  [[-0.00160968]\n"," [-0.00160968]\n"," [ 0.00241171]]\n","Iter:  2530 loss =  0.013794968569776148 learning rate =  0.5 update =  [[-0.00160905]\n"," [-0.00160905]\n"," [ 0.00241076]]\n","Iter:  2531 loss =  0.013789474750818802 learning rate =  0.5 update =  [[-0.00160841]\n"," [-0.00160841]\n"," [ 0.0024098 ]]\n","Iter:  2532 loss =  0.013783985267175766 learning rate =  0.5 update =  [[-0.00160778]\n"," [-0.00160778]\n"," [ 0.00240885]]\n","Iter:  2533 loss =  0.013778500113747209 learning rate =  0.5 update =  [[-0.00160714]\n"," [-0.00160714]\n"," [ 0.00240791]]\n","Iter:  2534 loss =  0.013773019285441248 learning rate =  0.5 update =  [[-0.00160651]\n"," [-0.00160651]\n"," [ 0.00240696]]\n","Iter:  2535 loss =  0.013767542777173864 learning rate =  0.5 update =  [[-0.00160587]\n"," [-0.00160587]\n"," [ 0.00240601]]\n","Iter:  2536 loss =  0.0137620705838691 learning rate =  0.5 update =  [[-0.00160524]\n"," [-0.00160524]\n"," [ 0.00240506]]\n","Iter:  2537 loss =  0.013756602700458944 learning rate =  0.5 update =  [[-0.00160461]\n"," [-0.00160461]\n"," [ 0.00240411]]\n","Iter:  2538 loss =  0.013751139121883148 learning rate =  0.5 update =  [[-0.00160398]\n"," [-0.00160398]\n"," [ 0.00240317]]\n","Iter:  2539 loss =  0.013745679843089394 learning rate =  0.5 update =  [[-0.00160335]\n"," [-0.00160335]\n"," [ 0.00240222]]\n","Iter:  2540 loss =  0.013740224859033295 learning rate =  0.5 update =  [[-0.00160271]\n"," [-0.00160271]\n"," [ 0.00240128]]\n","Iter:  2541 loss =  0.013734774164678266 learning rate =  0.5 update =  [[-0.00160208]\n"," [-0.00160208]\n"," [ 0.00240034]]\n","Iter:  2542 loss =  0.013729327754995545 learning rate =  0.5 update =  [[-0.00160145]\n"," [-0.00160145]\n"," [ 0.00239939]]\n","Iter:  2543 loss =  0.013723885624964167 learning rate =  0.5 update =  [[-0.00160082]\n"," [-0.00160082]\n"," [ 0.00239845]]\n","Iter:  2544 loss =  0.013718447769571034 learning rate =  0.5 update =  [[-0.0016002 ]\n"," [-0.0016002 ]\n"," [ 0.00239751]]\n","Iter:  2545 loss =  0.01371301418381097 learning rate =  0.5 update =  [[-0.00159957]\n"," [-0.00159957]\n"," [ 0.00239657]]\n","Iter:  2546 loss =  0.013707584862686297 learning rate =  0.5 update =  [[-0.00159894]\n"," [-0.00159894]\n"," [ 0.00239563]]\n","Iter:  2547 loss =  0.013702159801207219 learning rate =  0.5 update =  [[-0.00159831]\n"," [-0.00159831]\n"," [ 0.00239469]]\n","Iter:  2548 loss =  0.01369673899439182 learning rate =  0.5 update =  [[-0.00159768]\n"," [-0.00159768]\n"," [ 0.00239375]]\n","Iter:  2549 loss =  0.013691322437265736 learning rate =  0.5 update =  [[-0.00159706]\n"," [-0.00159706]\n"," [ 0.00239281]]\n","Iter:  2550 loss =  0.013685910124862434 learning rate =  0.5 update =  [[-0.00159643]\n"," [-0.00159643]\n"," [ 0.00239188]]\n","Iter:  2551 loss =  0.013680502052222973 learning rate =  0.5 update =  [[-0.00159581]\n"," [-0.00159581]\n"," [ 0.00239094]]\n","Iter:  2552 loss =  0.013675098214396247 learning rate =  0.5 update =  [[-0.00159518]\n"," [-0.00159518]\n"," [ 0.00239001]]\n","Iter:  2553 loss =  0.013669698606438664 learning rate =  0.5 update =  [[-0.00159456]\n"," [-0.00159456]\n"," [ 0.00238907]]\n","Iter:  2554 loss =  0.01366430322341448 learning rate =  0.5 update =  [[-0.00159393]\n"," [-0.00159393]\n"," [ 0.00238814]]\n","Iter:  2555 loss =  0.013658912060395413 learning rate =  0.5 update =  [[-0.00159331]\n"," [-0.00159331]\n"," [ 0.0023872 ]]\n","Iter:  2556 loss =  0.013653525112460867 learning rate =  0.5 update =  [[-0.00159269]\n"," [-0.00159268]\n"," [ 0.00238627]]\n","Iter:  2557 loss =  0.013648142374697896 learning rate =  0.5 update =  [[-0.00159206]\n"," [-0.00159206]\n"," [ 0.00238534]]\n","Iter:  2558 loss =  0.013642763842201214 learning rate =  0.5 update =  [[-0.00159144]\n"," [-0.00159144]\n"," [ 0.00238441]]\n","Iter:  2559 loss =  0.013637389510072949 learning rate =  0.5 update =  [[-0.00159082]\n"," [-0.00159082]\n"," [ 0.00238348]]\n","Iter:  2560 loss =  0.013632019373422923 learning rate =  0.5 update =  [[-0.0015902 ]\n"," [-0.0015902 ]\n"," [ 0.00238255]]\n","Iter:  2561 loss =  0.013626653427368466 learning rate =  0.5 update =  [[-0.00158958]\n"," [-0.00158958]\n"," [ 0.00238162]]\n","Iter:  2562 loss =  0.0136212916670345 learning rate =  0.5 update =  [[-0.00158896]\n"," [-0.00158896]\n"," [ 0.00238069]]\n","Iter:  2563 loss =  0.013615934087553402 learning rate =  0.5 update =  [[-0.00158834]\n"," [-0.00158834]\n"," [ 0.00237976]]\n","Iter:  2564 loss =  0.01361058068406511 learning rate =  0.5 update =  [[-0.00158772]\n"," [-0.00158772]\n"," [ 0.00237884]]\n","Iter:  2565 loss =  0.01360523145171711 learning rate =  0.5 update =  [[-0.0015871 ]\n"," [-0.0015871 ]\n"," [ 0.00237791]]\n","Iter:  2566 loss =  0.013599886385664187 learning rate =  0.5 update =  [[-0.00158648]\n"," [-0.00158648]\n"," [ 0.00237698]]\n","Iter:  2567 loss =  0.013594545481068837 learning rate =  0.5 update =  [[-0.00158586]\n"," [-0.00158586]\n"," [ 0.00237606]]\n","Iter:  2568 loss =  0.013589208733100922 learning rate =  0.5 update =  [[-0.00158524]\n"," [-0.00158524]\n"," [ 0.00237514]]\n","Iter:  2569 loss =  0.013583876136937548 learning rate =  0.5 update =  [[-0.00158463]\n"," [-0.00158463]\n"," [ 0.00237421]]\n","Iter:  2570 loss =  0.013578547687763539 learning rate =  0.5 update =  [[-0.00158401]\n"," [-0.00158401]\n"," [ 0.00237329]]\n","Iter:  2571 loss =  0.013573223380771068 learning rate =  0.5 update =  [[-0.00158339]\n"," [-0.00158339]\n"," [ 0.00237237]]\n","Iter:  2572 loss =  0.013567903211159426 learning rate =  0.5 update =  [[-0.00158278]\n"," [-0.00158278]\n"," [ 0.00237145]]\n","Iter:  2573 loss =  0.013562587174135776 learning rate =  0.5 update =  [[-0.00158216]\n"," [-0.00158216]\n"," [ 0.00237053]]\n","Iter:  2574 loss =  0.01355727526491416 learning rate =  0.5 update =  [[-0.00158155]\n"," [-0.00158155]\n"," [ 0.00236961]]\n","Iter:  2575 loss =  0.013551967478716405 learning rate =  0.5 update =  [[-0.00158094]\n"," [-0.00158093]\n"," [ 0.00236869]]\n","Iter:  2576 loss =  0.01354666381077125 learning rate =  0.5 update =  [[-0.00158032]\n"," [-0.00158032]\n"," [ 0.00236777]]\n","Iter:  2577 loss =  0.013541364256315101 learning rate =  0.5 update =  [[-0.00157971]\n"," [-0.00157971]\n"," [ 0.00236685]]\n","Iter:  2578 loss =  0.013536068810591554 learning rate =  0.5 update =  [[-0.0015791 ]\n"," [-0.0015791 ]\n"," [ 0.00236594]]\n","Iter:  2579 loss =  0.013530777468851552 learning rate =  0.5 update =  [[-0.00157848]\n"," [-0.00157848]\n"," [ 0.00236502]]\n","Iter:  2580 loss =  0.013525490226353205 learning rate =  0.5 update =  [[-0.00157787]\n"," [-0.00157787]\n"," [ 0.0023641 ]]\n","Iter:  2581 loss =  0.013520207078361995 learning rate =  0.5 update =  [[-0.00157726]\n"," [-0.00157726]\n"," [ 0.00236319]]\n","Iter:  2582 loss =  0.013514928020150675 learning rate =  0.5 update =  [[-0.00157665]\n"," [-0.00157665]\n"," [ 0.00236227]]\n","Iter:  2583 loss =  0.013509653046999158 learning rate =  0.5 update =  [[-0.00157604]\n"," [-0.00157604]\n"," [ 0.00236136]]\n","Iter:  2584 loss =  0.013504382154194652 learning rate =  0.5 update =  [[-0.00157543]\n"," [-0.00157543]\n"," [ 0.00236045]]\n","Iter:  2585 loss =  0.013499115337031534 learning rate =  0.5 update =  [[-0.00157482]\n"," [-0.00157482]\n"," [ 0.00235954]]\n","Iter:  2586 loss =  0.013493852590811473 learning rate =  0.5 update =  [[-0.00157421]\n"," [-0.00157421]\n"," [ 0.00235863]]\n","Iter:  2587 loss =  0.01348859391084318 learning rate =  0.5 update =  [[-0.0015736 ]\n"," [-0.0015736 ]\n"," [ 0.00235771]]\n","Iter:  2588 loss =  0.013483339292442727 learning rate =  0.5 update =  [[-0.00157299]\n"," [-0.00157299]\n"," [ 0.0023568 ]]\n","Iter:  2589 loss =  0.013478088730933204 learning rate =  0.5 update =  [[-0.00157239]\n"," [-0.00157239]\n"," [ 0.0023559 ]]\n","Iter:  2590 loss =  0.013472842221644906 learning rate =  0.5 update =  [[-0.00157178]\n"," [-0.00157178]\n"," [ 0.00235499]]\n","Iter:  2591 loss =  0.013467599759915114 learning rate =  0.5 update =  [[-0.00157117]\n"," [-0.00157117]\n"," [ 0.00235408]]\n","Iter:  2592 loss =  0.013462361341088552 learning rate =  0.5 update =  [[-0.00157057]\n"," [-0.00157057]\n"," [ 0.00235317]]\n","Iter:  2593 loss =  0.01345712696051673 learning rate =  0.5 update =  [[-0.00156996]\n"," [-0.00156996]\n"," [ 0.00235227]]\n","Iter:  2594 loss =  0.013451896613558476 learning rate =  0.5 update =  [[-0.00156936]\n"," [-0.00156936]\n"," [ 0.00235136]]\n","Iter:  2595 loss =  0.013446670295579555 learning rate =  0.5 update =  [[-0.00156875]\n"," [-0.00156875]\n"," [ 0.00235045]]\n","Iter:  2596 loss =  0.01344144800195285 learning rate =  0.5 update =  [[-0.00156815]\n"," [-0.00156815]\n"," [ 0.00234955]]\n","Iter:  2597 loss =  0.01343622972805828 learning rate =  0.5 update =  [[-0.00156754]\n"," [-0.00156754]\n"," [ 0.00234865]]\n","Iter:  2598 loss =  0.013431015469282836 learning rate =  0.5 update =  [[-0.00156694]\n"," [-0.00156694]\n"," [ 0.00234774]]\n","Iter:  2599 loss =  0.013425805221020583 learning rate =  0.5 update =  [[-0.00156634]\n"," [-0.00156634]\n"," [ 0.00234684]]\n","Iter:  2600 loss =  0.013420598978672415 learning rate =  0.5 update =  [[-0.00156573]\n"," [-0.00156573]\n"," [ 0.00234594]]\n","Iter:  2601 loss =  0.01341539673764646 learning rate =  0.5 update =  [[-0.00156513]\n"," [-0.00156513]\n"," [ 0.00234504]]\n","Iter:  2602 loss =  0.013410198493357613 learning rate =  0.5 update =  [[-0.00156453]\n"," [-0.00156453]\n"," [ 0.00234414]]\n","Iter:  2603 loss =  0.013405004241227876 learning rate =  0.5 update =  [[-0.00156393]\n"," [-0.00156393]\n"," [ 0.00234324]]\n","Iter:  2604 loss =  0.013399813976686242 learning rate =  0.5 update =  [[-0.00156333]\n"," [-0.00156333]\n"," [ 0.00234234]]\n","Iter:  2605 loss =  0.013394627695168557 learning rate =  0.5 update =  [[-0.00156273]\n"," [-0.00156273]\n"," [ 0.00234144]]\n","Iter:  2606 loss =  0.01338944539211755 learning rate =  0.5 update =  [[-0.00156213]\n"," [-0.00156213]\n"," [ 0.00234054]]\n","Iter:  2607 loss =  0.013384267062983107 learning rate =  0.5 update =  [[-0.00156153]\n"," [-0.00156153]\n"," [ 0.00233965]]\n","Iter:  2608 loss =  0.01337909270322168 learning rate =  0.5 update =  [[-0.00156093]\n"," [-0.00156093]\n"," [ 0.00233875]]\n","Iter:  2609 loss =  0.013373922308297002 learning rate =  0.5 update =  [[-0.00156033]\n"," [-0.00156033]\n"," [ 0.00233786]]\n","Iter:  2610 loss =  0.013368755873679392 learning rate =  0.5 update =  [[-0.00155973]\n"," [-0.00155973]\n"," [ 0.00233696]]\n","Iter:  2611 loss =  0.013363593394846118 learning rate =  0.5 update =  [[-0.00155914]\n"," [-0.00155914]\n"," [ 0.00233607]]\n","Iter:  2612 loss =  0.013358434867281327 learning rate =  0.5 update =  [[-0.00155854]\n"," [-0.00155854]\n"," [ 0.00233517]]\n","Iter:  2613 loss =  0.013353280286476098 learning rate =  0.5 update =  [[-0.00155794]\n"," [-0.00155794]\n"," [ 0.00233428]]\n","Iter:  2614 loss =  0.013348129647928083 learning rate =  0.5 update =  [[-0.00155735]\n"," [-0.00155735]\n"," [ 0.00233339]]\n","Iter:  2615 loss =  0.013342982947142019 learning rate =  0.5 update =  [[-0.00155675]\n"," [-0.00155675]\n"," [ 0.0023325 ]]\n","Iter:  2616 loss =  0.013337840179629253 learning rate =  0.5 update =  [[-0.00155616]\n"," [-0.00155616]\n"," [ 0.00233161]]\n","Iter:  2617 loss =  0.013332701340908073 learning rate =  0.5 update =  [[-0.00155556]\n"," [-0.00155556]\n"," [ 0.00233072]]\n","Iter:  2618 loss =  0.013327566426503547 learning rate =  0.5 update =  [[-0.00155497]\n"," [-0.00155497]\n"," [ 0.00232983]]\n","Iter:  2619 loss =  0.01332243543194734 learning rate =  0.5 update =  [[-0.00155437]\n"," [-0.00155437]\n"," [ 0.00232894]]\n","Iter:  2620 loss =  0.013317308352777932 learning rate =  0.5 update =  [[-0.00155378]\n"," [-0.00155378]\n"," [ 0.00232805]]\n","Iter:  2621 loss =  0.013312185184540672 learning rate =  0.5 update =  [[-0.00155319]\n"," [-0.00155319]\n"," [ 0.00232716]]\n","Iter:  2622 loss =  0.013307065922787555 learning rate =  0.5 update =  [[-0.0015526 ]\n"," [-0.0015526 ]\n"," [ 0.00232628]]\n","Iter:  2623 loss =  0.013301950563077283 learning rate =  0.5 update =  [[-0.001552  ]\n"," [-0.001552  ]\n"," [ 0.00232539]]\n","Iter:  2624 loss =  0.013296839100975108 learning rate =  0.5 update =  [[-0.00155141]\n"," [-0.00155141]\n"," [ 0.00232451]]\n","Iter:  2625 loss =  0.013291731532053338 learning rate =  0.5 update =  [[-0.00155082]\n"," [-0.00155082]\n"," [ 0.00232362]]\n","Iter:  2626 loss =  0.013286627851890567 learning rate =  0.5 update =  [[-0.00155023]\n"," [-0.00155023]\n"," [ 0.00232274]]\n","Iter:  2627 loss =  0.0132815280560723 learning rate =  0.5 update =  [[-0.00154964]\n"," [-0.00154964]\n"," [ 0.00232185]]\n","Iter:  2628 loss =  0.013276432140190678 learning rate =  0.5 update =  [[-0.00154905]\n"," [-0.00154905]\n"," [ 0.00232097]]\n","Iter:  2629 loss =  0.013271340099844348 learning rate =  0.5 update =  [[-0.00154846]\n"," [-0.00154846]\n"," [ 0.00232009]]\n","Iter:  2630 loss =  0.013266251930638703 learning rate =  0.5 update =  [[-0.00154787]\n"," [-0.00154787]\n"," [ 0.00231921]]\n","Iter:  2631 loss =  0.01326116762818565 learning rate =  0.5 update =  [[-0.00154728]\n"," [-0.00154728]\n"," [ 0.00231833]]\n","Iter:  2632 loss =  0.013256087188103838 learning rate =  0.5 update =  [[-0.0015467 ]\n"," [-0.00154669]\n"," [ 0.00231745]]\n","Iter:  2633 loss =  0.013251010606018368 learning rate =  0.5 update =  [[-0.00154611]\n"," [-0.00154611]\n"," [ 0.00231657]]\n","Iter:  2634 loss =  0.01324593787756094 learning rate =  0.5 update =  [[-0.00154552]\n"," [-0.00154552]\n"," [ 0.00231569]]\n","Iter:  2635 loss =  0.013240868998369936 learning rate =  0.5 update =  [[-0.00154493]\n"," [-0.00154493]\n"," [ 0.00231481]]\n","Iter:  2636 loss =  0.013235803964090197 learning rate =  0.5 update =  [[-0.00154435]\n"," [-0.00154435]\n"," [ 0.00231393]]\n","Iter:  2637 loss =  0.013230742770373075 learning rate =  0.5 update =  [[-0.00154376]\n"," [-0.00154376]\n"," [ 0.00231306]]\n","Iter:  2638 loss =  0.013225685412876484 learning rate =  0.5 update =  [[-0.00154318]\n"," [-0.00154318]\n"," [ 0.00231218]]\n","Iter:  2639 loss =  0.013220631887264907 learning rate =  0.5 update =  [[-0.00154259]\n"," [-0.00154259]\n"," [ 0.00231131]]\n","Iter:  2640 loss =  0.013215582189209287 learning rate =  0.5 update =  [[-0.00154201]\n"," [-0.00154201]\n"," [ 0.00231043]]\n","Iter:  2641 loss =  0.013210536314387043 learning rate =  0.5 update =  [[-0.00154142]\n"," [-0.00154142]\n"," [ 0.00230956]]\n","Iter:  2642 loss =  0.013205494258481987 learning rate =  0.5 update =  [[-0.00154084]\n"," [-0.00154084]\n"," [ 0.00230868]]\n","Iter:  2643 loss =  0.013200456017184618 learning rate =  0.5 update =  [[-0.00154026]\n"," [-0.00154026]\n"," [ 0.00230781]]\n","Iter:  2644 loss =  0.01319542158619177 learning rate =  0.5 update =  [[-0.00153967]\n"," [-0.00153967]\n"," [ 0.00230694]]\n","Iter:  2645 loss =  0.013190390961206638 learning rate =  0.5 update =  [[-0.00153909]\n"," [-0.00153909]\n"," [ 0.00230607]]\n","Iter:  2646 loss =  0.013185364137939075 learning rate =  0.5 update =  [[-0.00153851]\n"," [-0.00153851]\n"," [ 0.0023052 ]]\n","Iter:  2647 loss =  0.013180341112105042 learning rate =  0.5 update =  [[-0.00153793]\n"," [-0.00153793]\n"," [ 0.00230433]]\n","Iter:  2648 loss =  0.013175321879427142 learning rate =  0.5 update =  [[-0.00153735]\n"," [-0.00153735]\n"," [ 0.00230346]]\n","Iter:  2649 loss =  0.013170306435634296 learning rate =  0.5 update =  [[-0.00153677]\n"," [-0.00153677]\n"," [ 0.00230259]]\n","Iter:  2650 loss =  0.013165294776461896 learning rate =  0.5 update =  [[-0.00153619]\n"," [-0.00153619]\n"," [ 0.00230172]]\n","Iter:  2651 loss =  0.013160286897651533 learning rate =  0.5 update =  [[-0.00153561]\n"," [-0.00153561]\n"," [ 0.00230085]]\n","Iter:  2652 loss =  0.013155282794951298 learning rate =  0.5 update =  [[-0.00153503]\n"," [-0.00153503]\n"," [ 0.00229999]]\n","Iter:  2653 loss =  0.013150282464115565 learning rate =  0.5 update =  [[-0.00153445]\n"," [-0.00153445]\n"," [ 0.00229912]]\n","Iter:  2654 loss =  0.013145285900905065 learning rate =  0.5 update =  [[-0.00153387]\n"," [-0.00153387]\n"," [ 0.00229825]]\n","Iter:  2655 loss =  0.013140293101086841 learning rate =  0.5 update =  [[-0.00153329]\n"," [-0.00153329]\n"," [ 0.00229739]]\n","Iter:  2656 loss =  0.013135304060434293 learning rate =  0.5 update =  [[-0.00153271]\n"," [-0.00153271]\n"," [ 0.00229652]]\n","Iter:  2657 loss =  0.013130318774727074 learning rate =  0.5 update =  [[-0.00153214]\n"," [-0.00153214]\n"," [ 0.00229566]]\n","Iter:  2658 loss =  0.013125337239751195 learning rate =  0.5 update =  [[-0.00153156]\n"," [-0.00153156]\n"," [ 0.0022948 ]]\n","Iter:  2659 loss =  0.013120359451298745 learning rate =  0.5 update =  [[-0.00153098]\n"," [-0.00153098]\n"," [ 0.00229393]]\n","Iter:  2660 loss =  0.013115385405168286 learning rate =  0.5 update =  [[-0.00153041]\n"," [-0.00153041]\n"," [ 0.00229307]]\n","Iter:  2661 loss =  0.013110415097164685 learning rate =  0.5 update =  [[-0.00152983]\n"," [-0.00152983]\n"," [ 0.00229221]]\n","Iter:  2662 loss =  0.013105448523098799 learning rate =  0.5 update =  [[-0.00152926]\n"," [-0.00152926]\n"," [ 0.00229135]]\n","Iter:  2663 loss =  0.013100485678787984 learning rate =  0.5 update =  [[-0.00152868]\n"," [-0.00152868]\n"," [ 0.00229049]]\n","Iter:  2664 loss =  0.013095526560055536 learning rate =  0.5 update =  [[-0.00152811]\n"," [-0.00152811]\n"," [ 0.00228963]]\n","Iter:  2665 loss =  0.013090571162731188 learning rate =  0.5 update =  [[-0.00152754]\n"," [-0.00152754]\n"," [ 0.00228877]]\n","Iter:  2666 loss =  0.013085619482650836 learning rate =  0.5 update =  [[-0.00152696]\n"," [-0.00152696]\n"," [ 0.00228792]]\n","Iter:  2667 loss =  0.013080671515656506 learning rate =  0.5 update =  [[-0.00152639]\n"," [-0.00152639]\n"," [ 0.00228706]]\n","Iter:  2668 loss =  0.013075727257596297 learning rate =  0.5 update =  [[-0.00152582]\n"," [-0.00152582]\n"," [ 0.0022862 ]]\n","Iter:  2669 loss =  0.013070786704324728 learning rate =  0.5 update =  [[-0.00152525]\n"," [-0.00152525]\n"," [ 0.00228535]]\n","Iter:  2670 loss =  0.013065849851702206 learning rate =  0.5 update =  [[-0.00152467]\n"," [-0.00152467]\n"," [ 0.00228449]]\n","Iter:  2671 loss =  0.013060916695595457 learning rate =  0.5 update =  [[-0.0015241 ]\n"," [-0.0015241 ]\n"," [ 0.00228364]]\n","Iter:  2672 loss =  0.013055987231877236 learning rate =  0.5 update =  [[-0.00152353]\n"," [-0.00152353]\n"," [ 0.00228278]]\n","Iter:  2673 loss =  0.013051061456426516 learning rate =  0.5 update =  [[-0.00152296]\n"," [-0.00152296]\n"," [ 0.00228193]]\n","Iter:  2674 loss =  0.013046139365128233 learning rate =  0.5 update =  [[-0.00152239]\n"," [-0.00152239]\n"," [ 0.00228108]]\n","Iter:  2675 loss =  0.013041220953873528 learning rate =  0.5 update =  [[-0.00152182]\n"," [-0.00152182]\n"," [ 0.00228022]]\n","Iter:  2676 loss =  0.013036306218559649 learning rate =  0.5 update =  [[-0.00152125]\n"," [-0.00152125]\n"," [ 0.00227937]]\n","Iter:  2677 loss =  0.013031395155089711 learning rate =  0.5 update =  [[-0.00152069]\n"," [-0.00152068]\n"," [ 0.00227852]]\n","Iter:  2678 loss =  0.013026487759373212 learning rate =  0.5 update =  [[-0.00152012]\n"," [-0.00152012]\n"," [ 0.00227767]]\n","Iter:  2679 loss =  0.01302158402732535 learning rate =  0.5 update =  [[-0.00151955]\n"," [-0.00151955]\n"," [ 0.00227682]]\n","Iter:  2680 loss =  0.01301668395486771 learning rate =  0.5 update =  [[-0.00151898]\n"," [-0.00151898]\n"," [ 0.00227597]]\n","Iter:  2681 loss =  0.013011787537927599 learning rate =  0.5 update =  [[-0.00151842]\n"," [-0.00151841]\n"," [ 0.00227512]]\n","Iter:  2682 loss =  0.01300689477243859 learning rate =  0.5 update =  [[-0.00151785]\n"," [-0.00151785]\n"," [ 0.00227428]]\n","Iter:  2683 loss =  0.013002005654340022 learning rate =  0.5 update =  [[-0.00151728]\n"," [-0.00151728]\n"," [ 0.00227343]]\n","Iter:  2684 loss =  0.01299712017957735 learning rate =  0.5 update =  [[-0.00151672]\n"," [-0.00151672]\n"," [ 0.00227258]]\n","Iter:  2685 loss =  0.012992238344102097 learning rate =  0.5 update =  [[-0.00151615]\n"," [-0.00151615]\n"," [ 0.00227174]]\n","Iter:  2686 loss =  0.012987360143871663 learning rate =  0.5 update =  [[-0.00151559]\n"," [-0.00151559]\n"," [ 0.00227089]]\n","Iter:  2687 loss =  0.012982485574849276 learning rate =  0.5 update =  [[-0.00151502]\n"," [-0.00151502]\n"," [ 0.00227005]]\n","Iter:  2688 loss =  0.012977614633004408 learning rate =  0.5 update =  [[-0.00151446]\n"," [-0.00151446]\n"," [ 0.0022692 ]]\n","Iter:  2689 loss =  0.012972747314312286 learning rate =  0.5 update =  [[-0.0015139 ]\n"," [-0.00151389]\n"," [ 0.00226836]]\n","Iter:  2690 loss =  0.012967883614754056 learning rate =  0.5 update =  [[-0.00151333]\n"," [-0.00151333]\n"," [ 0.00226752]]\n","Iter:  2691 loss =  0.012963023530316822 learning rate =  0.5 update =  [[-0.00151277]\n"," [-0.00151277]\n"," [ 0.00226667]]\n","Iter:  2692 loss =  0.012958167056993632 learning rate =  0.5 update =  [[-0.00151221]\n"," [-0.00151221]\n"," [ 0.00226583]]\n","Iter:  2693 loss =  0.01295331419078335 learning rate =  0.5 update =  [[-0.00151165]\n"," [-0.00151164]\n"," [ 0.00226499]]\n","Iter:  2694 loss =  0.012948464927690818 learning rate =  0.5 update =  [[-0.00151108]\n"," [-0.00151108]\n"," [ 0.00226415]]\n","Iter:  2695 loss =  0.012943619263726659 learning rate =  0.5 update =  [[-0.00151052]\n"," [-0.00151052]\n"," [ 0.00226331]]\n","Iter:  2696 loss =  0.012938777194907353 learning rate =  0.5 update =  [[-0.00150996]\n"," [-0.00150996]\n"," [ 0.00226247]]\n","Iter:  2697 loss =  0.012933938717255402 learning rate =  0.5 update =  [[-0.0015094 ]\n"," [-0.0015094 ]\n"," [ 0.00226163]]\n","Iter:  2698 loss =  0.012929103826798901 learning rate =  0.5 update =  [[-0.00150884]\n"," [-0.00150884]\n"," [ 0.0022608 ]]\n","Iter:  2699 loss =  0.012924272519572021 learning rate =  0.5 update =  [[-0.00150828]\n"," [-0.00150828]\n"," [ 0.00225996]]\n","Iter:  2700 loss =  0.012919444791614592 learning rate =  0.5 update =  [[-0.00150772]\n"," [-0.00150772]\n"," [ 0.00225912]]\n","Iter:  2701 loss =  0.012914620638972219 learning rate =  0.5 update =  [[-0.00150716]\n"," [-0.00150716]\n"," [ 0.00225829]]\n","Iter:  2702 loss =  0.012909800057696461 learning rate =  0.5 update =  [[-0.00150661]\n"," [-0.00150661]\n"," [ 0.00225745]]\n","Iter:  2703 loss =  0.012904983043844569 learning rate =  0.5 update =  [[-0.00150605]\n"," [-0.00150605]\n"," [ 0.00225662]]\n","Iter:  2704 loss =  0.012900169593479541 learning rate =  0.5 update =  [[-0.00150549]\n"," [-0.00150549]\n"," [ 0.00225578]]\n","Iter:  2705 loss =  0.012895359702670377 learning rate =  0.5 update =  [[-0.00150493]\n"," [-0.00150493]\n"," [ 0.00225495]]\n","Iter:  2706 loss =  0.012890553367491377 learning rate =  0.5 update =  [[-0.00150438]\n"," [-0.00150438]\n"," [ 0.00225412]]\n","Iter:  2707 loss =  0.012885750584023067 learning rate =  0.5 update =  [[-0.00150382]\n"," [-0.00150382]\n"," [ 0.00225328]]\n","Iter:  2708 loss =  0.012880951348351467 learning rate =  0.5 update =  [[-0.00150327]\n"," [-0.00150327]\n"," [ 0.00225245]]\n","Iter:  2709 loss =  0.012876155656568286 learning rate =  0.5 update =  [[-0.00150271]\n"," [-0.00150271]\n"," [ 0.00225162]]\n","Iter:  2710 loss =  0.01287136350477101 learning rate =  0.5 update =  [[-0.00150216]\n"," [-0.00150216]\n"," [ 0.00225079]]\n","Iter:  2711 loss =  0.012866574889062982 learning rate =  0.5 update =  [[-0.0015016 ]\n"," [-0.0015016 ]\n"," [ 0.00224996]]\n","Iter:  2712 loss =  0.01286178980555296 learning rate =  0.5 update =  [[-0.00150105]\n"," [-0.00150105]\n"," [ 0.00224913]]\n","Iter:  2713 loss =  0.012857008250355604 learning rate =  0.5 update =  [[-0.00150049]\n"," [-0.00150049]\n"," [ 0.0022483 ]]\n","Iter:  2714 loss =  0.012852230219591105 learning rate =  0.5 update =  [[-0.00149994]\n"," [-0.00149994]\n"," [ 0.00224747]]\n","Iter:  2715 loss =  0.01284745570938539 learning rate =  0.5 update =  [[-0.00149939]\n"," [-0.00149939]\n"," [ 0.00224665]]\n","Iter:  2716 loss =  0.012842684715870054 learning rate =  0.5 update =  [[-0.00149884]\n"," [-0.00149883]\n"," [ 0.00224582]]\n","Iter:  2717 loss =  0.012837917235182319 learning rate =  0.5 update =  [[-0.00149828]\n"," [-0.00149828]\n"," [ 0.00224499]]\n","Iter:  2718 loss =  0.01283315326346501 learning rate =  0.5 update =  [[-0.00149773]\n"," [-0.00149773]\n"," [ 0.00224417]]\n","Iter:  2719 loss =  0.012828392796866608 learning rate =  0.5 update =  [[-0.00149718]\n"," [-0.00149718]\n"," [ 0.00224334]]\n","Iter:  2720 loss =  0.012823635831541174 learning rate =  0.5 update =  [[-0.00149663]\n"," [-0.00149663]\n"," [ 0.00224252]]\n","Iter:  2721 loss =  0.01281888236364831 learning rate =  0.5 update =  [[-0.00149608]\n"," [-0.00149608]\n"," [ 0.00224169]]\n","Iter:  2722 loss =  0.012814132389353444 learning rate =  0.5 update =  [[-0.00149553]\n"," [-0.00149553]\n"," [ 0.00224087]]\n","Iter:  2723 loss =  0.012809385904827383 learning rate =  0.5 update =  [[-0.00149498]\n"," [-0.00149498]\n"," [ 0.00224005]]\n","Iter:  2724 loss =  0.012804642906246357 learning rate =  0.5 update =  [[-0.00149443]\n"," [-0.00149443]\n"," [ 0.00223923]]\n","Iter:  2725 loss =  0.01279990338979266 learning rate =  0.5 update =  [[-0.00149388]\n"," [-0.00149388]\n"," [ 0.0022384 ]]\n","Iter:  2726 loss =  0.012795167351653668 learning rate =  0.5 update =  [[-0.00149333]\n"," [-0.00149333]\n"," [ 0.00223758]]\n","Iter:  2727 loss =  0.01279043478802247 learning rate =  0.5 update =  [[-0.00149278]\n"," [-0.00149278]\n"," [ 0.00223676]]\n","Iter:  2728 loss =  0.012785705695097675 learning rate =  0.5 update =  [[-0.00149224]\n"," [-0.00149224]\n"," [ 0.00223594]]\n","Iter:  2729 loss =  0.012780980069083427 learning rate =  0.5 update =  [[-0.00149169]\n"," [-0.00149169]\n"," [ 0.00223512]]\n","Iter:  2730 loss =  0.012776257906189305 learning rate =  0.5 update =  [[-0.00149114]\n"," [-0.00149114]\n"," [ 0.00223431]]\n","Iter:  2731 loss =  0.012771539202630565 learning rate =  0.5 update =  [[-0.0014906 ]\n"," [-0.0014906 ]\n"," [ 0.00223349]]\n","Iter:  2732 loss =  0.012766823954627814 learning rate =  0.5 update =  [[-0.00149005]\n"," [-0.00149005]\n"," [ 0.00223267]]\n","Iter:  2733 loss =  0.012762112158407106 learning rate =  0.5 update =  [[-0.0014895 ]\n"," [-0.0014895 ]\n"," [ 0.00223185]]\n","Iter:  2734 loss =  0.01275740381020004 learning rate =  0.5 update =  [[-0.00148896]\n"," [-0.00148896]\n"," [ 0.00223104]]\n","Iter:  2735 loss =  0.012752698906243757 learning rate =  0.5 update =  [[-0.00148841]\n"," [-0.00148841]\n"," [ 0.00223022]]\n","Iter:  2736 loss =  0.012747997442780651 learning rate =  0.5 update =  [[-0.00148787]\n"," [-0.00148787]\n"," [ 0.00222941]]\n","Iter:  2737 loss =  0.012743299416058715 learning rate =  0.5 update =  [[-0.00148733]\n"," [-0.00148733]\n"," [ 0.00222859]]\n","Iter:  2738 loss =  0.012738604822331252 learning rate =  0.5 update =  [[-0.00148678]\n"," [-0.00148678]\n"," [ 0.00222778]]\n","Iter:  2739 loss =  0.012733913657857135 learning rate =  0.5 update =  [[-0.00148624]\n"," [-0.00148624]\n"," [ 0.00222697]]\n","Iter:  2740 loss =  0.012729225918900605 learning rate =  0.5 update =  [[-0.0014857 ]\n"," [-0.0014857 ]\n"," [ 0.00222615]]\n","Iter:  2741 loss =  0.012724541601731153 learning rate =  0.5 update =  [[-0.00148515]\n"," [-0.00148515]\n"," [ 0.00222534]]\n","Iter:  2742 loss =  0.012719860702623934 learning rate =  0.5 update =  [[-0.00148461]\n"," [-0.00148461]\n"," [ 0.00222453]]\n","Iter:  2743 loss =  0.012715183217859175 learning rate =  0.5 update =  [[-0.00148407]\n"," [-0.00148407]\n"," [ 0.00222372]]\n","Iter:  2744 loss =  0.012710509143722754 learning rate =  0.5 update =  [[-0.00148353]\n"," [-0.00148353]\n"," [ 0.00222291]]\n","Iter:  2745 loss =  0.012705838476505759 learning rate =  0.5 update =  [[-0.00148299]\n"," [-0.00148299]\n"," [ 0.0022221 ]]\n","Iter:  2746 loss =  0.012701171212504685 learning rate =  0.5 update =  [[-0.00148245]\n"," [-0.00148245]\n"," [ 0.00222129]]\n","Iter:  2747 loss =  0.012696507348021292 learning rate =  0.5 update =  [[-0.00148191]\n"," [-0.00148191]\n"," [ 0.00222048]]\n","Iter:  2748 loss =  0.01269184687936284 learning rate =  0.5 update =  [[-0.00148137]\n"," [-0.00148137]\n"," [ 0.00221967]]\n","Iter:  2749 loss =  0.012687189802841734 learning rate =  0.5 update =  [[-0.00148083]\n"," [-0.00148083]\n"," [ 0.00221887]]\n","Iter:  2750 loss =  0.012682536114775857 learning rate =  0.5 update =  [[-0.00148029]\n"," [-0.00148029]\n"," [ 0.00221806]]\n","Iter:  2751 loss =  0.012677885811488172 learning rate =  0.5 update =  [[-0.00147975]\n"," [-0.00147975]\n"," [ 0.00221725]]\n","Iter:  2752 loss =  0.012673238889307232 learning rate =  0.5 update =  [[-0.00147921]\n"," [-0.00147921]\n"," [ 0.00221645]]\n","Iter:  2753 loss =  0.012668595344566675 learning rate =  0.5 update =  [[-0.00147867]\n"," [-0.00147867]\n"," [ 0.00221564]]\n","Iter:  2754 loss =  0.012663955173605444 learning rate =  0.5 update =  [[-0.00147814]\n"," [-0.00147814]\n"," [ 0.00221484]]\n","Iter:  2755 loss =  0.012659318372767736 learning rate =  0.5 update =  [[-0.0014776 ]\n"," [-0.0014776 ]\n"," [ 0.00221404]]\n","Iter:  2756 loss =  0.012654684938403167 learning rate =  0.5 update =  [[-0.00147706]\n"," [-0.00147706]\n"," [ 0.00221323]]\n","Iter:  2757 loss =  0.012650054866866399 learning rate =  0.5 update =  [[-0.00147653]\n"," [-0.00147653]\n"," [ 0.00221243]]\n","Iter:  2758 loss =  0.012645428154517552 learning rate =  0.5 update =  [[-0.00147599]\n"," [-0.00147599]\n"," [ 0.00221163]]\n","Iter:  2759 loss =  0.012640804797721564 learning rate =  0.5 update =  [[-0.00147545]\n"," [-0.00147545]\n"," [ 0.00221083]]\n","Iter:  2760 loss =  0.012636184792849132 learning rate =  0.5 update =  [[-0.00147492]\n"," [-0.00147492]\n"," [ 0.00221003]]\n","Iter:  2761 loss =  0.01263156813627578 learning rate =  0.5 update =  [[-0.00147438]\n"," [-0.00147438]\n"," [ 0.00220922]]\n","Iter:  2762 loss =  0.012626954824382423 learning rate =  0.5 update =  [[-0.00147385]\n"," [-0.00147385]\n"," [ 0.00220843]]\n","Iter:  2763 loss =  0.012622344853555019 learning rate =  0.5 update =  [[-0.00147332]\n"," [-0.00147332]\n"," [ 0.00220763]]\n","Iter:  2764 loss =  0.012617738220184894 learning rate =  0.5 update =  [[-0.00147278]\n"," [-0.00147278]\n"," [ 0.00220683]]\n","Iter:  2765 loss =  0.012613134920668397 learning rate =  0.5 update =  [[-0.00147225]\n"," [-0.00147225]\n"," [ 0.00220603]]\n","Iter:  2766 loss =  0.01260853495140709 learning rate =  0.5 update =  [[-0.00147172]\n"," [-0.00147172]\n"," [ 0.00220523]]\n","Iter:  2767 loss =  0.012603938308807752 learning rate =  0.5 update =  [[-0.00147118]\n"," [-0.00147118]\n"," [ 0.00220443]]\n","Iter:  2768 loss =  0.01259934498928222 learning rate =  0.5 update =  [[-0.00147065]\n"," [-0.00147065]\n"," [ 0.00220364]]\n","Iter:  2769 loss =  0.012594754989247484 learning rate =  0.5 update =  [[-0.00147012]\n"," [-0.00147012]\n"," [ 0.00220284]]\n","Iter:  2770 loss =  0.012590168305125759 learning rate =  0.5 update =  [[-0.00146959]\n"," [-0.00146959]\n"," [ 0.00220205]]\n","Iter:  2771 loss =  0.012585584933344239 learning rate =  0.5 update =  [[-0.00146906]\n"," [-0.00146906]\n"," [ 0.00220125]]\n","Iter:  2772 loss =  0.012581004870335349 learning rate =  0.5 update =  [[-0.00146853]\n"," [-0.00146853]\n"," [ 0.00220046]]\n","Iter:  2773 loss =  0.012576428112536522 learning rate =  0.5 update =  [[-0.001468  ]\n"," [-0.001468  ]\n"," [ 0.00219967]]\n","Iter:  2774 loss =  0.01257185465639022 learning rate =  0.5 update =  [[-0.00146747]\n"," [-0.00146747]\n"," [ 0.00219887]]\n","Iter:  2775 loss =  0.012567284498344313 learning rate =  0.5 update =  [[-0.00146694]\n"," [-0.00146694]\n"," [ 0.00219808]]\n","Iter:  2776 loss =  0.01256271763485132 learning rate =  0.5 update =  [[-0.00146641]\n"," [-0.00146641]\n"," [ 0.00219729]]\n","Iter:  2777 loss =  0.012558154062369117 learning rate =  0.5 update =  [[-0.00146588]\n"," [-0.00146588]\n"," [ 0.0021965 ]]\n","Iter:  2778 loss =  0.012553593777360629 learning rate =  0.5 update =  [[-0.00146535]\n"," [-0.00146535]\n"," [ 0.00219571]]\n","Iter:  2779 loss =  0.012549036776293598 learning rate =  0.5 update =  [[-0.00146482]\n"," [-0.00146482]\n"," [ 0.00219492]]\n","Iter:  2780 loss =  0.012544483055641042 learning rate =  0.5 update =  [[-0.0014643 ]\n"," [-0.0014643 ]\n"," [ 0.00219413]]\n","Iter:  2781 loss =  0.012539932611880873 learning rate =  0.5 update =  [[-0.00146377]\n"," [-0.00146377]\n"," [ 0.00219334]]\n","Iter:  2782 loss =  0.012535385441495996 learning rate =  0.5 update =  [[-0.00146324]\n"," [-0.00146324]\n"," [ 0.00219255]]\n","Iter:  2783 loss =  0.012530841540974587 learning rate =  0.5 update =  [[-0.00146272]\n"," [-0.00146272]\n"," [ 0.00219176]]\n","Iter:  2784 loss =  0.012526300906809527 learning rate =  0.5 update =  [[-0.00146219]\n"," [-0.00146219]\n"," [ 0.00219097]]\n","Iter:  2785 loss =  0.012521763535498824 learning rate =  0.5 update =  [[-0.00146167]\n"," [-0.00146167]\n"," [ 0.00219019]]\n","Iter:  2786 loss =  0.012517229423545384 learning rate =  0.5 update =  [[-0.00146114]\n"," [-0.00146114]\n"," [ 0.0021894 ]]\n","Iter:  2787 loss =  0.01251269856745725 learning rate =  0.5 update =  [[-0.00146062]\n"," [-0.00146062]\n"," [ 0.00218862]]\n","Iter:  2788 loss =  0.0125081709637473 learning rate =  0.5 update =  [[-0.00146009]\n"," [-0.00146009]\n"," [ 0.00218783]]\n","Iter:  2789 loss =  0.012503646608933348 learning rate =  0.5 update =  [[-0.00145957]\n"," [-0.00145957]\n"," [ 0.00218705]]\n","Iter:  2790 loss =  0.01249912549953833 learning rate =  0.5 update =  [[-0.00145904]\n"," [-0.00145904]\n"," [ 0.00218626]]\n","Iter:  2791 loss =  0.01249460763208992 learning rate =  0.5 update =  [[-0.00145852]\n"," [-0.00145852]\n"," [ 0.00218548]]\n","Iter:  2792 loss =  0.012490093003120793 learning rate =  0.5 update =  [[-0.001458 ]\n"," [-0.001458 ]\n"," [ 0.0021847]]\n","Iter:  2793 loss =  0.012485581609168591 learning rate =  0.5 update =  [[-0.00145747]\n"," [-0.00145747]\n"," [ 0.00218391]]\n","Iter:  2794 loss =  0.012481073446775993 learning rate =  0.5 update =  [[-0.00145695]\n"," [-0.00145695]\n"," [ 0.00218313]]\n","Iter:  2795 loss =  0.012476568512490214 learning rate =  0.5 update =  [[-0.00145643]\n"," [-0.00145643]\n"," [ 0.00218235]]\n","Iter:  2796 loss =  0.012472066802863665 learning rate =  0.5 update =  [[-0.00145591]\n"," [-0.00145591]\n"," [ 0.00218157]]\n","Iter:  2797 loss =  0.012467568314453592 learning rate =  0.5 update =  [[-0.00145539]\n"," [-0.00145539]\n"," [ 0.00218079]]\n","Iter:  2798 loss =  0.012463073043822062 learning rate =  0.5 update =  [[-0.00145487]\n"," [-0.00145487]\n"," [ 0.00218001]]\n","Iter:  2799 loss =  0.012458580987536142 learning rate =  0.5 update =  [[-0.00145435]\n"," [-0.00145435]\n"," [ 0.00217923]]\n","Iter:  2800 loss =  0.012454092142167583 learning rate =  0.5 update =  [[-0.00145383]\n"," [-0.00145383]\n"," [ 0.00217845]]\n","Iter:  2801 loss =  0.012449606504293106 learning rate =  0.5 update =  [[-0.00145331]\n"," [-0.00145331]\n"," [ 0.00217768]]\n","Iter:  2802 loss =  0.012445124070494204 learning rate =  0.5 update =  [[-0.00145279]\n"," [-0.00145279]\n"," [ 0.0021769 ]]\n","Iter:  2803 loss =  0.012440644837357299 learning rate =  0.5 update =  [[-0.00145227]\n"," [-0.00145227]\n"," [ 0.00217612]]\n","Iter:  2804 loss =  0.012436168801473598 learning rate =  0.5 update =  [[-0.00145175]\n"," [-0.00145175]\n"," [ 0.00217535]]\n","Iter:  2805 loss =  0.012431695959439147 learning rate =  0.5 update =  [[-0.00145123]\n"," [-0.00145123]\n"," [ 0.00217457]]\n","Iter:  2806 loss =  0.012427226307854606 learning rate =  0.5 update =  [[-0.00145071]\n"," [-0.00145071]\n"," [ 0.00217379]]\n","Iter:  2807 loss =  0.012422759843325866 learning rate =  0.5 update =  [[-0.0014502 ]\n"," [-0.0014502 ]\n"," [ 0.00217302]]\n","Iter:  2808 loss =  0.012418296562463293 learning rate =  0.5 update =  [[-0.00144968]\n"," [-0.00144968]\n"," [ 0.00217225]]\n","Iter:  2809 loss =  0.01241383646188195 learning rate =  0.5 update =  [[-0.00144916]\n"," [-0.00144916]\n"," [ 0.00217147]]\n","Iter:  2810 loss =  0.012409379538201969 learning rate =  0.5 update =  [[-0.00144865]\n"," [-0.00144865]\n"," [ 0.0021707 ]]\n","Iter:  2811 loss =  0.012404925788048053 learning rate =  0.5 update =  [[-0.00144813]\n"," [-0.00144813]\n"," [ 0.00216993]]\n","Iter:  2812 loss =  0.012400475208049811 learning rate =  0.5 update =  [[-0.00144761]\n"," [-0.00144761]\n"," [ 0.00216916]]\n","Iter:  2813 loss =  0.012396027794841405 learning rate =  0.5 update =  [[-0.0014471 ]\n"," [-0.0014471 ]\n"," [ 0.00216838]]\n","Iter:  2814 loss =  0.012391583545061969 learning rate =  0.5 update =  [[-0.00144658]\n"," [-0.00144658]\n"," [ 0.00216761]]\n","Iter:  2815 loss =  0.01238714245535517 learning rate =  0.5 update =  [[-0.00144607]\n"," [-0.00144607]\n"," [ 0.00216684]]\n","Iter:  2816 loss =  0.012382704522369566 learning rate =  0.5 update =  [[-0.00144556]\n"," [-0.00144556]\n"," [ 0.00216607]]\n","Iter:  2817 loss =  0.012378269742758356 learning rate =  0.5 update =  [[-0.00144504]\n"," [-0.00144504]\n"," [ 0.0021653 ]]\n","Iter:  2818 loss =  0.012373838113179426 learning rate =  0.5 update =  [[-0.00144453]\n"," [-0.00144453]\n"," [ 0.00216454]]\n","Iter:  2819 loss =  0.012369409630295366 learning rate =  0.5 update =  [[-0.00144401]\n"," [-0.00144401]\n"," [ 0.00216377]]\n","Iter:  2820 loss =  0.012364984290773595 learning rate =  0.5 update =  [[-0.0014435]\n"," [-0.0014435]\n"," [ 0.002163 ]]\n","Iter:  2821 loss =  0.012360562091285946 learning rate =  0.5 update =  [[-0.00144299]\n"," [-0.00144299]\n"," [ 0.00216223]]\n","Iter:  2822 loss =  0.012356143028509257 learning rate =  0.5 update =  [[-0.00144248]\n"," [-0.00144248]\n"," [ 0.00216147]]\n","Iter:  2823 loss =  0.012351727099124861 learning rate =  0.5 update =  [[-0.00144197]\n"," [-0.00144197]\n"," [ 0.0021607 ]]\n","Iter:  2824 loss =  0.012347314299818613 learning rate =  0.5 update =  [[-0.00144145]\n"," [-0.00144145]\n"," [ 0.00215994]]\n","Iter:  2825 loss =  0.012342904627281319 learning rate =  0.5 update =  [[-0.00144094]\n"," [-0.00144094]\n"," [ 0.00215917]]\n","Iter:  2826 loss =  0.012338498078208276 learning rate =  0.5 update =  [[-0.00144043]\n"," [-0.00144043]\n"," [ 0.00215841]]\n","Iter:  2827 loss =  0.012334094649299446 learning rate =  0.5 update =  [[-0.00143992]\n"," [-0.00143992]\n"," [ 0.00215764]]\n","Iter:  2828 loss =  0.012329694337259372 learning rate =  0.5 update =  [[-0.00143941]\n"," [-0.00143941]\n"," [ 0.00215688]]\n","Iter:  2829 loss =  0.012325297138797174 learning rate =  0.5 update =  [[-0.0014389 ]\n"," [-0.0014389 ]\n"," [ 0.00215612]]\n","Iter:  2830 loss =  0.012320903050626806 learning rate =  0.5 update =  [[-0.00143839]\n"," [-0.00143839]\n"," [ 0.00215536]]\n","Iter:  2831 loss =  0.012316512069466544 learning rate =  0.5 update =  [[-0.00143789]\n"," [-0.00143789]\n"," [ 0.00215459]]\n","Iter:  2832 loss =  0.012312124192039496 learning rate =  0.5 update =  [[-0.00143738]\n"," [-0.00143738]\n"," [ 0.00215383]]\n","Iter:  2833 loss =  0.012307739415073238 learning rate =  0.5 update =  [[-0.00143687]\n"," [-0.00143687]\n"," [ 0.00215307]]\n","Iter:  2834 loss =  0.012303357735299923 learning rate =  0.5 update =  [[-0.00143636]\n"," [-0.00143636]\n"," [ 0.00215231]]\n","Iter:  2835 loss =  0.012298979149456334 learning rate =  0.5 update =  [[-0.00143585]\n"," [-0.00143585]\n"," [ 0.00215155]]\n","Iter:  2836 loss =  0.012294603654283833 learning rate =  0.5 update =  [[-0.00143535]\n"," [-0.00143535]\n"," [ 0.00215079]]\n","Iter:  2837 loss =  0.012290231246528165 learning rate =  0.5 update =  [[-0.00143484]\n"," [-0.00143484]\n"," [ 0.00215004]]\n","Iter:  2838 loss =  0.012285861922939893 learning rate =  0.5 update =  [[-0.00143433]\n"," [-0.00143433]\n"," [ 0.00214928]]\n","Iter:  2839 loss =  0.01228149568027389 learning rate =  0.5 update =  [[-0.00143383]\n"," [-0.00143383]\n"," [ 0.00214852]]\n","Iter:  2840 loss =  0.012277132515289679 learning rate =  0.5 update =  [[-0.00143332]\n"," [-0.00143332]\n"," [ 0.00214776]]\n","Iter:  2841 loss =  0.012272772424751333 learning rate =  0.5 update =  [[-0.00143282]\n"," [-0.00143282]\n"," [ 0.00214701]]\n","Iter:  2842 loss =  0.012268415405427386 learning rate =  0.5 update =  [[-0.00143231]\n"," [-0.00143231]\n"," [ 0.00214625]]\n","Iter:  2843 loss =  0.01226406145409081 learning rate =  0.5 update =  [[-0.00143181]\n"," [-0.00143181]\n"," [ 0.0021455 ]]\n","Iter:  2844 loss =  0.012259710567519233 learning rate =  0.5 update =  [[-0.0014313 ]\n"," [-0.0014313 ]\n"," [ 0.00214474]]\n","Iter:  2845 loss =  0.012255362742494717 learning rate =  0.5 update =  [[-0.0014308 ]\n"," [-0.0014308 ]\n"," [ 0.00214399]]\n","Iter:  2846 loss =  0.012251017975803701 learning rate =  0.5 update =  [[-0.0014303 ]\n"," [-0.0014303 ]\n"," [ 0.00214323]]\n","Iter:  2847 loss =  0.012246676264237307 learning rate =  0.5 update =  [[-0.00142979]\n"," [-0.00142979]\n"," [ 0.00214248]]\n","Iter:  2848 loss =  0.012242337604590928 learning rate =  0.5 update =  [[-0.00142929]\n"," [-0.00142929]\n"," [ 0.00214173]]\n","Iter:  2849 loss =  0.01223800199366458 learning rate =  0.5 update =  [[-0.00142879]\n"," [-0.00142879]\n"," [ 0.00214098]]\n","Iter:  2850 loss =  0.012233669428262605 learning rate =  0.5 update =  [[-0.00142829]\n"," [-0.00142829]\n"," [ 0.00214023]]\n","Iter:  2851 loss =  0.012229339905193877 learning rate =  0.5 update =  [[-0.00142778]\n"," [-0.00142778]\n"," [ 0.00213947]]\n","Iter:  2852 loss =  0.012225013421271688 learning rate =  0.5 update =  [[-0.00142728]\n"," [-0.00142728]\n"," [ 0.00213872]]\n","Iter:  2853 loss =  0.012220689973313752 learning rate =  0.5 update =  [[-0.00142678]\n"," [-0.00142678]\n"," [ 0.00213797]]\n","Iter:  2854 loss =  0.012216369558142162 learning rate =  0.5 update =  [[-0.00142628]\n"," [-0.00142628]\n"," [ 0.00213722]]\n","Iter:  2855 loss =  0.01221205217258358 learning rate =  0.5 update =  [[-0.00142578]\n"," [-0.00142578]\n"," [ 0.00213648]]\n","Iter:  2856 loss =  0.01220773781346893 learning rate =  0.5 update =  [[-0.00142528]\n"," [-0.00142528]\n"," [ 0.00213573]]\n","Iter:  2857 loss =  0.01220342647763354 learning rate =  0.5 update =  [[-0.00142478]\n"," [-0.00142478]\n"," [ 0.00213498]]\n","Iter:  2858 loss =  0.012199118161917245 learning rate =  0.5 update =  [[-0.00142428]\n"," [-0.00142428]\n"," [ 0.00213423]]\n","Iter:  2859 loss =  0.012194812863164125 learning rate =  0.5 update =  [[-0.00142378]\n"," [-0.00142378]\n"," [ 0.00213349]]\n","Iter:  2860 loss =  0.012190510578222763 learning rate =  0.5 update =  [[-0.00142328]\n"," [-0.00142328]\n"," [ 0.00213274]]\n","Iter:  2861 loss =  0.012186211303946062 learning rate =  0.5 update =  [[-0.00142279]\n"," [-0.00142279]\n"," [ 0.00213199]]\n","Iter:  2862 loss =  0.012181915037191269 learning rate =  0.5 update =  [[-0.00142229]\n"," [-0.00142229]\n"," [ 0.00213125]]\n","Iter:  2863 loss =  0.012177621774819975 learning rate =  0.5 update =  [[-0.00142179]\n"," [-0.00142179]\n"," [ 0.0021305 ]]\n","Iter:  2864 loss =  0.012173331513698173 learning rate =  0.5 update =  [[-0.00142129]\n"," [-0.00142129]\n"," [ 0.00212976]]\n","Iter:  2865 loss =  0.012169044250696114 learning rate =  0.5 update =  [[-0.0014208 ]\n"," [-0.0014208 ]\n"," [ 0.00212901]]\n","Iter:  2866 loss =  0.0121647599826885 learning rate =  0.5 update =  [[-0.0014203 ]\n"," [-0.0014203 ]\n"," [ 0.00212827]]\n","Iter:  2867 loss =  0.012160478706554387 learning rate =  0.5 update =  [[-0.0014198 ]\n"," [-0.0014198 ]\n"," [ 0.00212753]]\n","Iter:  2868 loss =  0.012156200419176906 learning rate =  0.5 update =  [[-0.00141931]\n"," [-0.00141931]\n"," [ 0.00212679]]\n","Iter:  2869 loss =  0.012151925117443653 learning rate =  0.5 update =  [[-0.00141881]\n"," [-0.00141881]\n"," [ 0.00212605]]\n","Iter:  2870 loss =  0.012147652798246648 learning rate =  0.5 update =  [[-0.00141832]\n"," [-0.00141832]\n"," [ 0.0021253 ]]\n","Iter:  2871 loss =  0.012143383458482 learning rate =  0.5 update =  [[-0.00141782]\n"," [-0.00141782]\n"," [ 0.00212456]]\n","Iter:  2872 loss =  0.012139117095050243 learning rate =  0.5 update =  [[-0.00141733]\n"," [-0.00141733]\n"," [ 0.00212382]]\n","Iter:  2873 loss =  0.012134853704856084 learning rate =  0.5 update =  [[-0.00141683]\n"," [-0.00141683]\n"," [ 0.00212308]]\n","Iter:  2874 loss =  0.012130593284808679 learning rate =  0.5 update =  [[-0.00141634]\n"," [-0.00141634]\n"," [ 0.00212234]]\n","Iter:  2875 loss =  0.012126335831821225 learning rate =  0.5 update =  [[-0.00141585]\n"," [-0.00141585]\n"," [ 0.00212161]]\n","Iter:  2876 loss =  0.01212208134281125 learning rate =  0.5 update =  [[-0.00141535]\n"," [-0.00141535]\n"," [ 0.00212087]]\n","Iter:  2877 loss =  0.012117829814700713 learning rate =  0.5 update =  [[-0.00141486]\n"," [-0.00141486]\n"," [ 0.00212013]]\n","Iter:  2878 loss =  0.012113581244415573 learning rate =  0.5 update =  [[-0.00141437]\n"," [-0.00141437]\n"," [ 0.00211939]]\n","Iter:  2879 loss =  0.01210933562888621 learning rate =  0.5 update =  [[-0.00141388]\n"," [-0.00141388]\n"," [ 0.00211866]]\n","Iter:  2880 loss =  0.012105092965047092 learning rate =  0.5 update =  [[-0.00141339]\n"," [-0.00141339]\n"," [ 0.00211792]]\n","Iter:  2881 loss =  0.012100853249836944 learning rate =  0.5 update =  [[-0.00141289]\n"," [-0.00141289]\n"," [ 0.00211719]]\n","Iter:  2882 loss =  0.012096616480198805 learning rate =  0.5 update =  [[-0.0014124 ]\n"," [-0.0014124 ]\n"," [ 0.00211645]]\n","Iter:  2883 loss =  0.01209238265307979 learning rate =  0.5 update =  [[-0.00141191]\n"," [-0.00141191]\n"," [ 0.00211572]]\n","Iter:  2884 loss =  0.012088151765431302 learning rate =  0.5 update =  [[-0.00141142]\n"," [-0.00141142]\n"," [ 0.00211498]]\n","Iter:  2885 loss =  0.012083923814208995 learning rate =  0.5 update =  [[-0.00141093]\n"," [-0.00141093]\n"," [ 0.00211425]]\n","Iter:  2886 loss =  0.012079698796372496 learning rate =  0.5 update =  [[-0.00141044]\n"," [-0.00141044]\n"," [ 0.00211352]]\n","Iter:  2887 loss =  0.012075476708885858 learning rate =  0.5 update =  [[-0.00140995]\n"," [-0.00140995]\n"," [ 0.00211278]]\n","Iter:  2888 loss =  0.012071257548717126 learning rate =  0.5 update =  [[-0.00140946]\n"," [-0.00140946]\n"," [ 0.00211205]]\n","Iter:  2889 loss =  0.012067041312838547 learning rate =  0.5 update =  [[-0.00140898]\n"," [-0.00140897]\n"," [ 0.00211132]]\n","Iter:  2890 loss =  0.012062827998226531 learning rate =  0.5 update =  [[-0.00140849]\n"," [-0.00140849]\n"," [ 0.00211059]]\n","Iter:  2891 loss =  0.012058617601861863 learning rate =  0.5 update =  [[-0.001408  ]\n"," [-0.001408  ]\n"," [ 0.00210986]]\n","Iter:  2892 loss =  0.012054410120729117 learning rate =  0.5 update =  [[-0.00140751]\n"," [-0.00140751]\n"," [ 0.00210913]]\n","Iter:  2893 loss =  0.012050205551817069 learning rate =  0.5 update =  [[-0.00140702]\n"," [-0.00140702]\n"," [ 0.0021084 ]]\n","Iter:  2894 loss =  0.012046003892118885 learning rate =  0.5 update =  [[-0.00140654]\n"," [-0.00140654]\n"," [ 0.00210767]]\n","Iter:  2895 loss =  0.012041805138631703 learning rate =  0.5 update =  [[-0.00140605]\n"," [-0.00140605]\n"," [ 0.00210694]]\n","Iter:  2896 loss =  0.012037609288356545 learning rate =  0.5 update =  [[-0.00140556]\n"," [-0.00140556]\n"," [ 0.00210621]]\n","Iter:  2897 loss =  0.012033416338298988 learning rate =  0.5 update =  [[-0.00140508]\n"," [-0.00140508]\n"," [ 0.00210549]]\n","Iter:  2898 loss =  0.012029226285468278 learning rate =  0.5 update =  [[-0.00140459]\n"," [-0.00140459]\n"," [ 0.00210476]]\n","Iter:  2899 loss =  0.012025039126878103 learning rate =  0.5 update =  [[-0.00140411]\n"," [-0.00140411]\n"," [ 0.00210403]]\n","Iter:  2900 loss =  0.01202085485954606 learning rate =  0.5 update =  [[-0.00140362]\n"," [-0.00140362]\n"," [ 0.00210331]]\n","Iter:  2901 loss =  0.012016673480493774 learning rate =  0.5 update =  [[-0.00140314]\n"," [-0.00140314]\n"," [ 0.00210258]]\n","Iter:  2902 loss =  0.012012494986747148 learning rate =  0.5 update =  [[-0.00140265]\n"," [-0.00140265]\n"," [ 0.00210186]]\n","Iter:  2903 loss =  0.01200831937533591 learning rate =  0.5 update =  [[-0.00140217]\n"," [-0.00140217]\n"," [ 0.00210113]]\n","Iter:  2904 loss =  0.012004146643294068 learning rate =  0.5 update =  [[-0.00140168]\n"," [-0.00140168]\n"," [ 0.00210041]]\n","Iter:  2905 loss =  0.011999976787659478 learning rate =  0.5 update =  [[-0.0014012 ]\n"," [-0.0014012 ]\n"," [ 0.00209968]]\n","Iter:  2906 loss =  0.011995809805474253 learning rate =  0.5 update =  [[-0.00140072]\n"," [-0.00140072]\n"," [ 0.00209896]]\n","Iter:  2907 loss =  0.011991645693784355 learning rate =  0.5 update =  [[-0.00140024]\n"," [-0.00140024]\n"," [ 0.00209824]]\n","Iter:  2908 loss =  0.01198748444963991 learning rate =  0.5 update =  [[-0.00139975]\n"," [-0.00139975]\n"," [ 0.00209752]]\n","Iter:  2909 loss =  0.011983326070095009 learning rate =  0.5 update =  [[-0.00139927]\n"," [-0.00139927]\n"," [ 0.00209679]]\n","Iter:  2910 loss =  0.011979170552207796 learning rate =  0.5 update =  [[-0.00139879]\n"," [-0.00139879]\n"," [ 0.00209607]]\n","Iter:  2911 loss =  0.011975017893040407 learning rate =  0.5 update =  [[-0.00139831]\n"," [-0.00139831]\n"," [ 0.00209535]]\n","Iter:  2912 loss =  0.011970868089658887 learning rate =  0.5 update =  [[-0.00139783]\n"," [-0.00139783]\n"," [ 0.00209463]]\n","Iter:  2913 loss =  0.011966721139133501 learning rate =  0.5 update =  [[-0.00139735]\n"," [-0.00139735]\n"," [ 0.00209391]]\n","Iter:  2914 loss =  0.01196257703853831 learning rate =  0.5 update =  [[-0.00139687]\n"," [-0.00139687]\n"," [ 0.00209319]]\n","Iter:  2915 loss =  0.011958435784951395 learning rate =  0.5 update =  [[-0.00139639]\n"," [-0.00139639]\n"," [ 0.00209247]]\n","Iter:  2916 loss =  0.011954297375454911 learning rate =  0.5 update =  [[-0.00139591]\n"," [-0.00139591]\n"," [ 0.00209176]]\n","Iter:  2917 loss =  0.0119501618071349 learning rate =  0.5 update =  [[-0.00139543]\n"," [-0.00139543]\n"," [ 0.00209104]]\n","Iter:  2918 loss =  0.011946029077081418 learning rate =  0.5 update =  [[-0.00139495]\n"," [-0.00139495]\n"," [ 0.00209032]]\n","Iter:  2919 loss =  0.011941899182388346 learning rate =  0.5 update =  [[-0.00139447]\n"," [-0.00139447]\n"," [ 0.00208961]]\n","Iter:  2920 loss =  0.011937772120153727 learning rate =  0.5 update =  [[-0.00139399]\n"," [-0.00139399]\n"," [ 0.00208889]]\n","Iter:  2921 loss =  0.011933647887479368 learning rate =  0.5 update =  [[-0.00139351]\n"," [-0.00139351]\n"," [ 0.00208817]]\n","Iter:  2922 loss =  0.011929526481471126 learning rate =  0.5 update =  [[-0.00139304]\n"," [-0.00139303]\n"," [ 0.00208746]]\n","Iter:  2923 loss =  0.011925407899238788 learning rate =  0.5 update =  [[-0.00139256]\n"," [-0.00139256]\n"," [ 0.00208674]]\n","Iter:  2924 loss =  0.011921292137895884 learning rate =  0.5 update =  [[-0.00139208]\n"," [-0.00139208]\n"," [ 0.00208603]]\n","Iter:  2925 loss =  0.011917179194560213 learning rate =  0.5 update =  [[-0.0013916 ]\n"," [-0.0013916 ]\n"," [ 0.00208532]]\n","Iter:  2926 loss =  0.011913069066353057 learning rate =  0.5 update =  [[-0.00139113]\n"," [-0.00139113]\n"," [ 0.0020846 ]]\n","Iter:  2927 loss =  0.01190896175039997 learning rate =  0.5 update =  [[-0.00139065]\n"," [-0.00139065]\n"," [ 0.00208389]]\n","Iter:  2928 loss =  0.011904857243830162 learning rate =  0.5 update =  [[-0.00139018]\n"," [-0.00139018]\n"," [ 0.00208318]]\n","Iter:  2929 loss =  0.01190075554377688 learning rate =  0.5 update =  [[-0.0013897 ]\n"," [-0.0013897 ]\n"," [ 0.00208247]]\n","Iter:  2930 loss =  0.011896656647377199 learning rate =  0.5 update =  [[-0.00138922]\n"," [-0.00138922]\n"," [ 0.00208175]]\n","Iter:  2931 loss =  0.011892560551772069 learning rate =  0.5 update =  [[-0.00138875]\n"," [-0.00138875]\n"," [ 0.00208104]]\n","Iter:  2932 loss =  0.011888467254106312 learning rate =  0.5 update =  [[-0.00138828]\n"," [-0.00138828]\n"," [ 0.00208033]]\n","Iter:  2933 loss =  0.011884376751528578 learning rate =  0.5 update =  [[-0.0013878 ]\n"," [-0.0013878 ]\n"," [ 0.00207962]]\n","Iter:  2934 loss =  0.011880289041191438 learning rate =  0.5 update =  [[-0.00138733]\n"," [-0.00138733]\n"," [ 0.00207891]]\n","Iter:  2935 loss =  0.01187620412025138 learning rate =  0.5 update =  [[-0.00138685]\n"," [-0.00138685]\n"," [ 0.00207821]]\n","Iter:  2936 loss =  0.011872121985868612 learning rate =  0.5 update =  [[-0.00138638]\n"," [-0.00138638]\n"," [ 0.0020775 ]]\n","Iter:  2937 loss =  0.011868042635207143 learning rate =  0.5 update =  [[-0.00138591]\n"," [-0.00138591]\n"," [ 0.00207679]]\n","Iter:  2938 loss =  0.01186396606543494 learning rate =  0.5 update =  [[-0.00138543]\n"," [-0.00138543]\n"," [ 0.00207608]]\n","Iter:  2939 loss =  0.011859892273723756 learning rate =  0.5 update =  [[-0.00138496]\n"," [-0.00138496]\n"," [ 0.00207537]]\n","Iter:  2940 loss =  0.011855821257249167 learning rate =  0.5 update =  [[-0.00138449]\n"," [-0.00138449]\n"," [ 0.00207467]]\n","Iter:  2941 loss =  0.011851753013190506 learning rate =  0.5 update =  [[-0.00138402]\n"," [-0.00138402]\n"," [ 0.00207396]]\n","Iter:  2942 loss =  0.01184768753873101 learning rate =  0.5 update =  [[-0.00138355]\n"," [-0.00138355]\n"," [ 0.00207326]]\n","Iter:  2943 loss =  0.011843624831057623 learning rate =  0.5 update =  [[-0.00138308]\n"," [-0.00138308]\n"," [ 0.00207255]]\n","Iter:  2944 loss =  0.011839564887361188 learning rate =  0.5 update =  [[-0.00138261]\n"," [-0.00138261]\n"," [ 0.00207185]]\n","Iter:  2945 loss =  0.011835507704836195 learning rate =  0.5 update =  [[-0.00138214]\n"," [-0.00138214]\n"," [ 0.00207114]]\n","Iter:  2946 loss =  0.011831453280680955 learning rate =  0.5 update =  [[-0.00138167]\n"," [-0.00138167]\n"," [ 0.00207044]]\n","Iter:  2947 loss =  0.011827401612097818 learning rate =  0.5 update =  [[-0.0013812 ]\n"," [-0.0013812 ]\n"," [ 0.00206974]]\n","Iter:  2948 loss =  0.011823352696292387 learning rate =  0.5 update =  [[-0.00138073]\n"," [-0.00138073]\n"," [ 0.00206903]]\n","Iter:  2949 loss =  0.011819306530474506 learning rate =  0.5 update =  [[-0.00138026]\n"," [-0.00138026]\n"," [ 0.00206833]]\n","Iter:  2950 loss =  0.011815263111857464 learning rate =  0.5 update =  [[-0.00137979]\n"," [-0.00137979]\n"," [ 0.00206763]]\n","Iter:  2951 loss =  0.011811222437658512 learning rate =  0.5 update =  [[-0.00137932]\n"," [-0.00137932]\n"," [ 0.00206693]]\n","Iter:  2952 loss =  0.011807184505098486 learning rate =  0.5 update =  [[-0.00137885]\n"," [-0.00137885]\n"," [ 0.00206623]]\n","Iter:  2953 loss =  0.011803149311402127 learning rate =  0.5 update =  [[-0.00137838]\n"," [-0.00137838]\n"," [ 0.00206553]]\n","Iter:  2954 loss =  0.011799116853797677 learning rate =  0.5 update =  [[-0.00137792]\n"," [-0.00137792]\n"," [ 0.00206483]]\n","Iter:  2955 loss =  0.011795087129517335 learning rate =  0.5 update =  [[-0.00137745]\n"," [-0.00137745]\n"," [ 0.00206413]]\n","Iter:  2956 loss =  0.011791060135796807 learning rate =  0.5 update =  [[-0.00137698]\n"," [-0.00137698]\n"," [ 0.00206343]]\n","Iter:  2957 loss =  0.011787035869875753 learning rate =  0.5 update =  [[-0.00137652]\n"," [-0.00137652]\n"," [ 0.00206273]]\n","Iter:  2958 loss =  0.011783014328997275 learning rate =  0.5 update =  [[-0.00137605]\n"," [-0.00137605]\n"," [ 0.00206203]]\n","Iter:  2959 loss =  0.01177899551040841 learning rate =  0.5 update =  [[-0.00137558]\n"," [-0.00137558]\n"," [ 0.00206133]]\n","Iter:  2960 loss =  0.011774979411359729 learning rate =  0.5 update =  [[-0.00137512]\n"," [-0.00137512]\n"," [ 0.00206064]]\n","Iter:  2961 loss =  0.011770966029105517 learning rate =  0.5 update =  [[-0.00137465]\n"," [-0.00137465]\n"," [ 0.00205994]]\n","Iter:  2962 loss =  0.01176695536090381 learning rate =  0.5 update =  [[-0.00137419]\n"," [-0.00137419]\n"," [ 0.00205924]]\n","Iter:  2963 loss =  0.011762947404016212 learning rate =  0.5 update =  [[-0.00137372]\n"," [-0.00137372]\n"," [ 0.00205855]]\n","Iter:  2964 loss =  0.011758942155708205 learning rate =  0.5 update =  [[-0.00137326]\n"," [-0.00137326]\n"," [ 0.00205785]]\n","Iter:  2965 loss =  0.011754939613248604 learning rate =  0.5 update =  [[-0.00137279]\n"," [-0.00137279]\n"," [ 0.00205716]]\n","Iter:  2966 loss =  0.011750939773910207 learning rate =  0.5 update =  [[-0.00137233]\n"," [-0.00137233]\n"," [ 0.00205646]]\n","Iter:  2967 loss =  0.011746942634969228 learning rate =  0.5 update =  [[-0.00137187]\n"," [-0.00137187]\n"," [ 0.00205577]]\n","Iter:  2968 loss =  0.011742948193705666 learning rate =  0.5 update =  [[-0.0013714 ]\n"," [-0.0013714 ]\n"," [ 0.00205508]]\n","Iter:  2969 loss =  0.011738956447403134 learning rate =  0.5 update =  [[-0.00137094]\n"," [-0.00137094]\n"," [ 0.00205438]]\n","Iter:  2970 loss =  0.011734967393348776 learning rate =  0.5 update =  [[-0.00137048]\n"," [-0.00137048]\n"," [ 0.00205369]]\n","Iter:  2971 loss =  0.011730981028833545 learning rate =  0.5 update =  [[-0.00137002]\n"," [-0.00137002]\n"," [ 0.002053  ]]\n","Iter:  2972 loss =  0.011726997351151812 learning rate =  0.5 update =  [[-0.00136955]\n"," [-0.00136955]\n"," [ 0.00205231]]\n","Iter:  2973 loss =  0.011723016357601649 learning rate =  0.5 update =  [[-0.00136909]\n"," [-0.00136909]\n"," [ 0.00205162]]\n","Iter:  2974 loss =  0.011719038045484798 learning rate =  0.5 update =  [[-0.00136863]\n"," [-0.00136863]\n"," [ 0.00205093]]\n","Iter:  2975 loss =  0.01171506241210658 learning rate =  0.5 update =  [[-0.00136817]\n"," [-0.00136817]\n"," [ 0.00205024]]\n","Iter:  2976 loss =  0.011711089454775852 learning rate =  0.5 update =  [[-0.00136771]\n"," [-0.00136771]\n"," [ 0.00204955]]\n","Iter:  2977 loss =  0.011707119170805136 learning rate =  0.5 update =  [[-0.00136725]\n"," [-0.00136725]\n"," [ 0.00204886]]\n","Iter:  2978 loss =  0.011703151557510373 learning rate =  0.5 update =  [[-0.00136679]\n"," [-0.00136679]\n"," [ 0.00204817]]\n","Iter:  2979 loss =  0.011699186612211313 learning rate =  0.5 update =  [[-0.00136633]\n"," [-0.00136633]\n"," [ 0.00204748]]\n","Iter:  2980 loss =  0.011695224332231118 learning rate =  0.5 update =  [[-0.00136587]\n"," [-0.00136587]\n"," [ 0.00204679]]\n","Iter:  2981 loss =  0.011691264714896569 learning rate =  0.5 update =  [[-0.00136541]\n"," [-0.00136541]\n"," [ 0.00204611]]\n","Iter:  2982 loss =  0.01168730775753802 learning rate =  0.5 update =  [[-0.00136495]\n"," [-0.00136495]\n"," [ 0.00204542]]\n","Iter:  2983 loss =  0.011683353457489394 learning rate =  0.5 update =  [[-0.00136449]\n"," [-0.00136449]\n"," [ 0.00204473]]\n","Iter:  2984 loss =  0.011679401812088132 learning rate =  0.5 update =  [[-0.00136404]\n"," [-0.00136404]\n"," [ 0.00204405]]\n","Iter:  2985 loss =  0.011675452818675176 learning rate =  0.5 update =  [[-0.00136358]\n"," [-0.00136358]\n"," [ 0.00204336]]\n","Iter:  2986 loss =  0.011671506474595052 learning rate =  0.5 update =  [[-0.00136312]\n"," [-0.00136312]\n"," [ 0.00204268]]\n","Iter:  2987 loss =  0.011667562777195842 learning rate =  0.5 update =  [[-0.00136266]\n"," [-0.00136266]\n"," [ 0.00204199]]\n","Iter:  2988 loss =  0.01166362172382912 learning rate =  0.5 update =  [[-0.00136221]\n"," [-0.00136221]\n"," [ 0.00204131]]\n","Iter:  2989 loss =  0.011659683311849967 learning rate =  0.5 update =  [[-0.00136175]\n"," [-0.00136175]\n"," [ 0.00204062]]\n","Iter:  2990 loss =  0.011655747538617067 learning rate =  0.5 update =  [[-0.00136129]\n"," [-0.00136129]\n"," [ 0.00203994]]\n","Iter:  2991 loss =  0.011651814401492438 learning rate =  0.5 update =  [[-0.00136084]\n"," [-0.00136084]\n"," [ 0.00203926]]\n","Iter:  2992 loss =  0.01164788389784176 learning rate =  0.5 update =  [[-0.00136038]\n"," [-0.00136038]\n"," [ 0.00203858]]\n","Iter:  2993 loss =  0.011643956025034131 learning rate =  0.5 update =  [[-0.00135992]\n"," [-0.00135992]\n"," [ 0.00203789]]\n","Iter:  2994 loss =  0.01164003078044217 learning rate =  0.5 update =  [[-0.00135947]\n"," [-0.00135947]\n"," [ 0.00203721]]\n","Iter:  2995 loss =  0.011636108161441937 learning rate =  0.5 update =  [[-0.00135901]\n"," [-0.00135901]\n"," [ 0.00203653]]\n","Iter:  2996 loss =  0.01163218816541313 learning rate =  0.5 update =  [[-0.00135856]\n"," [-0.00135856]\n"," [ 0.00203585]]\n","Iter:  2997 loss =  0.011628270789738633 learning rate =  0.5 update =  [[-0.00135811]\n"," [-0.00135811]\n"," [ 0.00203517]]\n","Iter:  2998 loss =  0.011624356031804997 learning rate =  0.5 update =  [[-0.00135765]\n"," [-0.00135765]\n"," [ 0.00203449]]\n","Iter:  2999 loss =  0.011620443889002297 learning rate =  0.5 update =  [[-0.0013572 ]\n"," [-0.0013572 ]\n"," [ 0.00203381]]\n","Iter:  3000 loss =  0.01161653435872382 learning rate =  0.5 update =  [[-0.00135674]\n"," [-0.00135674]\n"," [ 0.00203313]]\n","Iter:  3001 loss =  0.011612627438366522 learning rate =  0.5 update =  [[-0.00135629]\n"," [-0.00135629]\n"," [ 0.00203246]]\n","Iter:  3002 loss =  0.011608723125330766 learning rate =  0.5 update =  [[-0.00135584]\n"," [-0.00135584]\n"," [ 0.00203178]]\n","Iter:  3003 loss =  0.01160482141702027 learning rate =  0.5 update =  [[-0.00135539]\n"," [-0.00135539]\n"," [ 0.0020311 ]]\n","Iter:  3004 loss =  0.011600922310842164 learning rate =  0.5 update =  [[-0.00135493]\n"," [-0.00135493]\n"," [ 0.00203042]]\n","Iter:  3005 loss =  0.011597025804207184 learning rate =  0.5 update =  [[-0.00135448]\n"," [-0.00135448]\n"," [ 0.00202975]]\n","Iter:  3006 loss =  0.011593131894529339 learning rate =  0.5 update =  [[-0.00135403]\n"," [-0.00135403]\n"," [ 0.00202907]]\n","Iter:  3007 loss =  0.01158924057922607 learning rate =  0.5 update =  [[-0.00135358]\n"," [-0.00135358]\n"," [ 0.0020284 ]]\n","Iter:  3008 loss =  0.011585351855718264 learning rate =  0.5 update =  [[-0.00135313]\n"," [-0.00135313]\n"," [ 0.00202772]]\n","Iter:  3009 loss =  0.011581465721430158 learning rate =  0.5 update =  [[-0.00135268]\n"," [-0.00135268]\n"," [ 0.00202705]]\n","Iter:  3010 loss =  0.011577582173789503 learning rate =  0.5 update =  [[-0.00135223]\n"," [-0.00135223]\n"," [ 0.00202637]]\n","Iter:  3011 loss =  0.011573701210227362 learning rate =  0.5 update =  [[-0.00135178]\n"," [-0.00135178]\n"," [ 0.0020257 ]]\n","Iter:  3012 loss =  0.011569822828178138 learning rate =  0.5 update =  [[-0.00135133]\n"," [-0.00135133]\n"," [ 0.00202502]]\n","Iter:  3013 loss =  0.011565947025079706 learning rate =  0.5 update =  [[-0.00135088]\n"," [-0.00135088]\n"," [ 0.00202435]]\n","Iter:  3014 loss =  0.011562073798373262 learning rate =  0.5 update =  [[-0.00135043]\n"," [-0.00135043]\n"," [ 0.00202368]]\n","Iter:  3015 loss =  0.011558203145503416 learning rate =  0.5 update =  [[-0.00134998]\n"," [-0.00134998]\n"," [ 0.00202301]]\n","Iter:  3016 loss =  0.011554335063918092 learning rate =  0.5 update =  [[-0.00134953]\n"," [-0.00134953]\n"," [ 0.00202233]]\n","Iter:  3017 loss =  0.011550469551068717 learning rate =  0.5 update =  [[-0.00134908]\n"," [-0.00134908]\n"," [ 0.00202166]]\n","Iter:  3018 loss =  0.011546606604409867 learning rate =  0.5 update =  [[-0.00134864]\n"," [-0.00134864]\n"," [ 0.00202099]]\n","Iter:  3019 loss =  0.011542746221399525 learning rate =  0.5 update =  [[-0.00134819]\n"," [-0.00134819]\n"," [ 0.00202032]]\n","Iter:  3020 loss =  0.011538888399499115 learning rate =  0.5 update =  [[-0.00134774]\n"," [-0.00134774]\n"," [ 0.00201965]]\n","Iter:  3021 loss =  0.01153503313617332 learning rate =  0.5 update =  [[-0.00134729]\n"," [-0.00134729]\n"," [ 0.00201898]]\n","Iter:  3022 loss =  0.01153118042889024 learning rate =  0.5 update =  [[-0.00134685]\n"," [-0.00134685]\n"," [ 0.00201831]]\n","Iter:  3023 loss =  0.011527330275121175 learning rate =  0.5 update =  [[-0.0013464 ]\n"," [-0.0013464 ]\n"," [ 0.00201765]]\n","Iter:  3024 loss =  0.01152348267234084 learning rate =  0.5 update =  [[-0.00134595]\n"," [-0.00134595]\n"," [ 0.00201698]]\n","Iter:  3025 loss =  0.011519637618027241 learning rate =  0.5 update =  [[-0.00134551]\n"," [-0.00134551]\n"," [ 0.00201631]]\n","Iter:  3026 loss =  0.011515795109661647 learning rate =  0.5 update =  [[-0.00134506]\n"," [-0.00134506]\n"," [ 0.00201564]]\n","Iter:  3027 loss =  0.01151195514472874 learning rate =  0.5 update =  [[-0.00134462]\n"," [-0.00134462]\n"," [ 0.00201498]]\n","Iter:  3028 loss =  0.01150811772071637 learning rate =  0.5 update =  [[-0.00134417]\n"," [-0.00134417]\n"," [ 0.00201431]]\n","Iter:  3029 loss =  0.011504282835115814 learning rate =  0.5 update =  [[-0.00134373]\n"," [-0.00134373]\n"," [ 0.00201364]]\n","Iter:  3030 loss =  0.011500450485421563 learning rate =  0.5 update =  [[-0.00134328]\n"," [-0.00134328]\n"," [ 0.00201298]]\n","Iter:  3031 loss =  0.011496620669131333 learning rate =  0.5 update =  [[-0.00134284]\n"," [-0.00134284]\n"," [ 0.00201231]]\n","Iter:  3032 loss =  0.011492793383746324 learning rate =  0.5 update =  [[-0.00134239]\n"," [-0.00134239]\n"," [ 0.00201165]]\n","Iter:  3033 loss =  0.011488968626770802 learning rate =  0.5 update =  [[-0.00134195]\n"," [-0.00134195]\n"," [ 0.00201099]]\n","Iter:  3034 loss =  0.011485146395712412 learning rate =  0.5 update =  [[-0.00134151]\n"," [-0.00134151]\n"," [ 0.00201032]]\n","Iter:  3035 loss =  0.011481326688081923 learning rate =  0.5 update =  [[-0.00134106]\n"," [-0.00134106]\n"," [ 0.00200966]]\n","Iter:  3036 loss =  0.011477509501393618 learning rate =  0.5 update =  [[-0.00134062]\n"," [-0.00134062]\n"," [ 0.002009  ]]\n","Iter:  3037 loss =  0.011473694833164766 learning rate =  0.5 update =  [[-0.00134018]\n"," [-0.00134018]\n"," [ 0.00200833]]\n","Iter:  3038 loss =  0.01146988268091604 learning rate =  0.5 update =  [[-0.00133974]\n"," [-0.00133974]\n"," [ 0.00200767]]\n","Iter:  3039 loss =  0.011466073042171343 learning rate =  0.5 update =  [[-0.00133929]\n"," [-0.00133929]\n"," [ 0.00200701]]\n","Iter:  3040 loss =  0.011462265914457714 learning rate =  0.5 update =  [[-0.00133885]\n"," [-0.00133885]\n"," [ 0.00200635]]\n","Iter:  3041 loss =  0.01145846129530552 learning rate =  0.5 update =  [[-0.00133841]\n"," [-0.00133841]\n"," [ 0.00200569]]\n","Iter:  3042 loss =  0.011454659182248377 learning rate =  0.5 update =  [[-0.00133797]\n"," [-0.00133797]\n"," [ 0.00200503]]\n","Iter:  3043 loss =  0.011450859572822963 learning rate =  0.5 update =  [[-0.00133753]\n"," [-0.00133753]\n"," [ 0.00200437]]\n","Iter:  3044 loss =  0.011447062464569389 learning rate =  0.5 update =  [[-0.00133709]\n"," [-0.00133709]\n"," [ 0.00200371]]\n","Iter:  3045 loss =  0.011443267855030827 learning rate =  0.5 update =  [[-0.00133665]\n"," [-0.00133665]\n"," [ 0.00200305]]\n","Iter:  3046 loss =  0.011439475741753602 learning rate =  0.5 update =  [[-0.00133621]\n"," [-0.00133621]\n"," [ 0.00200239]]\n","Iter:  3047 loss =  0.011435686122287469 learning rate =  0.5 update =  [[-0.00133577]\n"," [-0.00133577]\n"," [ 0.00200173]]\n","Iter:  3048 loss =  0.01143189899418519 learning rate =  0.5 update =  [[-0.00133533]\n"," [-0.00133533]\n"," [ 0.00200108]]\n","Iter:  3049 loss =  0.011428114355002707 learning rate =  0.5 update =  [[-0.00133489]\n"," [-0.00133489]\n"," [ 0.00200042]]\n","Iter:  3050 loss =  0.011424332202299225 learning rate =  0.5 update =  [[-0.00133445]\n"," [-0.00133445]\n"," [ 0.00199976]]\n","Iter:  3051 loss =  0.011420552533637262 learning rate =  0.5 update =  [[-0.00133401]\n"," [-0.00133401]\n"," [ 0.00199911]]\n","Iter:  3052 loss =  0.01141677534658209 learning rate =  0.5 update =  [[-0.00133358]\n"," [-0.00133358]\n"," [ 0.00199845]]\n","Iter:  3053 loss =  0.011413000638702587 learning rate =  0.5 update =  [[-0.00133314]\n"," [-0.00133314]\n"," [ 0.00199779]]\n","Iter:  3054 loss =  0.011409228407570575 learning rate =  0.5 update =  [[-0.0013327 ]\n"," [-0.0013327 ]\n"," [ 0.00199714]]\n","Iter:  3055 loss =  0.011405458650761136 learning rate =  0.5 update =  [[-0.00133226]\n"," [-0.00133226]\n"," [ 0.00199648]]\n","Iter:  3056 loss =  0.011401691365852366 learning rate =  0.5 update =  [[-0.00133183]\n"," [-0.00133183]\n"," [ 0.00199583]]\n","Iter:  3057 loss =  0.01139792655042559 learning rate =  0.5 update =  [[-0.00133139]\n"," [-0.00133139]\n"," [ 0.00199518]]\n","Iter:  3058 loss =  0.011394164202065335 learning rate =  0.5 update =  [[-0.00133095]\n"," [-0.00133095]\n"," [ 0.00199452]]\n","Iter:  3059 loss =  0.011390404318359248 learning rate =  0.5 update =  [[-0.00133052]\n"," [-0.00133052]\n"," [ 0.00199387]]\n","Iter:  3060 loss =  0.01138664689689793 learning rate =  0.5 update =  [[-0.00133008]\n"," [-0.00133008]\n"," [ 0.00199322]]\n","Iter:  3061 loss =  0.011382891935275374 learning rate =  0.5 update =  [[-0.00132965]\n"," [-0.00132965]\n"," [ 0.00199257]]\n","Iter:  3062 loss =  0.01137913943108853 learning rate =  0.5 update =  [[-0.00132921]\n"," [-0.00132921]\n"," [ 0.00199191]]\n","Iter:  3063 loss =  0.011375389381937438 learning rate =  0.5 update =  [[-0.00132878]\n"," [-0.00132878]\n"," [ 0.00199126]]\n","Iter:  3064 loss =  0.011371641785425508 learning rate =  0.5 update =  [[-0.00132834]\n"," [-0.00132834]\n"," [ 0.00199061]]\n","Iter:  3065 loss =  0.01136789663915887 learning rate =  0.5 update =  [[-0.00132791]\n"," [-0.00132791]\n"," [ 0.00198996]]\n","Iter:  3066 loss =  0.01136415394074709 learning rate =  0.5 update =  [[-0.00132747]\n"," [-0.00132747]\n"," [ 0.00198931]]\n","Iter:  3067 loss =  0.011360413687802651 learning rate =  0.5 update =  [[-0.00132704]\n"," [-0.00132704]\n"," [ 0.00198866]]\n","Iter:  3068 loss =  0.01135667587794115 learning rate =  0.5 update =  [[-0.00132661]\n"," [-0.00132661]\n"," [ 0.00198801]]\n","Iter:  3069 loss =  0.011352940508781308 learning rate =  0.5 update =  [[-0.00132617]\n"," [-0.00132617]\n"," [ 0.00198736]]\n","Iter:  3070 loss =  0.011349207577944933 learning rate =  0.5 update =  [[-0.00132574]\n"," [-0.00132574]\n"," [ 0.00198672]]\n","Iter:  3071 loss =  0.011345477083057004 learning rate =  0.5 update =  [[-0.00132531]\n"," [-0.00132531]\n"," [ 0.00198607]]\n","Iter:  3072 loss =  0.01134174902174517 learning rate =  0.5 update =  [[-0.00132487]\n"," [-0.00132487]\n"," [ 0.00198542]]\n","Iter:  3073 loss =  0.011338023391640728 learning rate =  0.5 update =  [[-0.00132444]\n"," [-0.00132444]\n"," [ 0.00198477]]\n","Iter:  3074 loss =  0.011334300190377702 learning rate =  0.5 update =  [[-0.00132401]\n"," [-0.00132401]\n"," [ 0.00198413]]\n","Iter:  3075 loss =  0.011330579415593088 learning rate =  0.5 update =  [[-0.00132358]\n"," [-0.00132358]\n"," [ 0.00198348]]\n","Iter:  3076 loss =  0.011326861064927172 learning rate =  0.5 update =  [[-0.00132315]\n"," [-0.00132315]\n"," [ 0.00198284]]\n","Iter:  3077 loss =  0.01132314513602321 learning rate =  0.5 update =  [[-0.00132272]\n"," [-0.00132272]\n"," [ 0.00198219]]\n","Iter:  3078 loss =  0.011319431626527439 learning rate =  0.5 update =  [[-0.00132228]\n"," [-0.00132228]\n"," [ 0.00198154]]\n","Iter:  3079 loss =  0.011315720534089177 learning rate =  0.5 update =  [[-0.00132185]\n"," [-0.00132185]\n"," [ 0.0019809 ]]\n","Iter:  3080 loss =  0.011312011856360694 learning rate =  0.5 update =  [[-0.00132142]\n"," [-0.00132142]\n"," [ 0.00198026]]\n","Iter:  3081 loss =  0.011308305590997545 learning rate =  0.5 update =  [[-0.00132099]\n"," [-0.00132099]\n"," [ 0.00197961]]\n","Iter:  3082 loss =  0.011304601735658004 learning rate =  0.5 update =  [[-0.00132056]\n"," [-0.00132056]\n"," [ 0.00197897]]\n","Iter:  3083 loss =  0.01130090028800343 learning rate =  0.5 update =  [[-0.00132013]\n"," [-0.00132013]\n"," [ 0.00197833]]\n","Iter:  3084 loss =  0.01129720124569836 learning rate =  0.5 update =  [[-0.00131971]\n"," [-0.00131971]\n"," [ 0.00197768]]\n","Iter:  3085 loss =  0.0112935046064103 learning rate =  0.5 update =  [[-0.00131928]\n"," [-0.00131928]\n"," [ 0.00197704]]\n","Iter:  3086 loss =  0.01128981036780959 learning rate =  0.5 update =  [[-0.00131885]\n"," [-0.00131885]\n"," [ 0.0019764 ]]\n","Iter:  3087 loss =  0.011286118527569573 learning rate =  0.5 update =  [[-0.00131842]\n"," [-0.00131842]\n"," [ 0.00197576]]\n","Iter:  3088 loss =  0.011282429083366958 learning rate =  0.5 update =  [[-0.00131799]\n"," [-0.00131799]\n"," [ 0.00197512]]\n","Iter:  3089 loss =  0.011278742032880953 learning rate =  0.5 update =  [[-0.00131756]\n"," [-0.00131756]\n"," [ 0.00197448]]\n","Iter:  3090 loss =  0.011275057373794103 learning rate =  0.5 update =  [[-0.00131714]\n"," [-0.00131714]\n"," [ 0.00197384]]\n","Iter:  3091 loss =  0.011271375103791588 learning rate =  0.5 update =  [[-0.00131671]\n"," [-0.00131671]\n"," [ 0.0019732 ]]\n","Iter:  3092 loss =  0.011267695220562042 learning rate =  0.5 update =  [[-0.00131628]\n"," [-0.00131628]\n"," [ 0.00197256]]\n","Iter:  3093 loss =  0.01126401772179673 learning rate =  0.5 update =  [[-0.00131586]\n"," [-0.00131586]\n"," [ 0.00197192]]\n","Iter:  3094 loss =  0.01126034260518989 learning rate =  0.5 update =  [[-0.00131543]\n"," [-0.00131543]\n"," [ 0.00197128]]\n","Iter:  3095 loss =  0.011256669868438773 learning rate =  0.5 update =  [[-0.001315  ]\n"," [-0.001315  ]\n"," [ 0.00197064]]\n","Iter:  3096 loss =  0.011252999509243751 learning rate =  0.5 update =  [[-0.00131458]\n"," [-0.00131458]\n"," [ 0.00197001]]\n","Iter:  3097 loss =  0.011249331525307844 learning rate =  0.5 update =  [[-0.00131415]\n"," [-0.00131415]\n"," [ 0.00196937]]\n","Iter:  3098 loss =  0.011245665914337242 learning rate =  0.5 update =  [[-0.00131373]\n"," [-0.00131373]\n"," [ 0.00196873]]\n","Iter:  3099 loss =  0.011242002674040987 learning rate =  0.5 update =  [[-0.0013133]\n"," [-0.0013133]\n"," [ 0.0019681]]\n","Iter:  3100 loss =  0.011238341802131156 learning rate =  0.5 update =  [[-0.00131288]\n"," [-0.00131288]\n"," [ 0.00196746]]\n","Iter:  3101 loss =  0.011234683296322538 learning rate =  0.5 update =  [[-0.00131245]\n"," [-0.00131245]\n"," [ 0.00196683]]\n","Iter:  3102 loss =  0.01123102715433318 learning rate =  0.5 update =  [[-0.00131203]\n"," [-0.00131203]\n"," [ 0.00196619]]\n","Iter:  3103 loss =  0.011227373373883647 learning rate =  0.5 update =  [[-0.0013116 ]\n"," [-0.0013116 ]\n"," [ 0.00196556]]\n","Iter:  3104 loss =  0.011223721952697817 learning rate =  0.5 update =  [[-0.00131118]\n"," [-0.00131118]\n"," [ 0.00196492]]\n","Iter:  3105 loss =  0.011220072888502255 learning rate =  0.5 update =  [[-0.00131076]\n"," [-0.00131076]\n"," [ 0.00196429]]\n","Iter:  3106 loss =  0.011216426179026554 learning rate =  0.5 update =  [[-0.00131033]\n"," [-0.00131033]\n"," [ 0.00196365]]\n","Iter:  3107 loss =  0.011212781822003023 learning rate =  0.5 update =  [[-0.00130991]\n"," [-0.00130991]\n"," [ 0.00196302]]\n","Iter:  3108 loss =  0.011209139815167083 learning rate =  0.5 update =  [[-0.00130949]\n"," [-0.00130949]\n"," [ 0.00196239]]\n","Iter:  3109 loss =  0.011205500156256959 learning rate =  0.5 update =  [[-0.00130907]\n"," [-0.00130907]\n"," [ 0.00196176]]\n","Iter:  3110 loss =  0.0112018628430137 learning rate =  0.5 update =  [[-0.00130864]\n"," [-0.00130864]\n"," [ 0.00196112]]\n","Iter:  3111 loss =  0.011198227873181418 learning rate =  0.5 update =  [[-0.00130822]\n"," [-0.00130822]\n"," [ 0.00196049]]\n","Iter:  3112 loss =  0.011194595244506993 learning rate =  0.5 update =  [[-0.0013078 ]\n"," [-0.0013078 ]\n"," [ 0.00195986]]\n","Iter:  3113 loss =  0.011190964954740132 learning rate =  0.5 update =  [[-0.00130738]\n"," [-0.00130738]\n"," [ 0.00195923]]\n","Iter:  3114 loss =  0.011187337001633432 learning rate =  0.5 update =  [[-0.00130696]\n"," [-0.00130696]\n"," [ 0.0019586 ]]\n","Iter:  3115 loss =  0.011183711382942538 learning rate =  0.5 update =  [[-0.00130654]\n"," [-0.00130654]\n"," [ 0.00195797]]\n","Iter:  3116 loss =  0.011180088096425709 learning rate =  0.5 update =  [[-0.00130612]\n"," [-0.00130612]\n"," [ 0.00195734]]\n","Iter:  3117 loss =  0.011176467139844277 learning rate =  0.5 update =  [[-0.0013057 ]\n"," [-0.0013057 ]\n"," [ 0.00195671]]\n","Iter:  3118 loss =  0.01117284851096221 learning rate =  0.5 update =  [[-0.00130528]\n"," [-0.00130528]\n"," [ 0.00195608]]\n","Iter:  3119 loss =  0.011169232207546467 learning rate =  0.5 update =  [[-0.00130486]\n"," [-0.00130486]\n"," [ 0.00195546]]\n","Iter:  3120 loss =  0.01116561822736684 learning rate =  0.5 update =  [[-0.00130444]\n"," [-0.00130444]\n"," [ 0.00195483]]\n","Iter:  3121 loss =  0.011162006568196036 learning rate =  0.5 update =  [[-0.00130402]\n"," [-0.00130402]\n"," [ 0.0019542 ]]\n","Iter:  3122 loss =  0.011158397227809362 learning rate =  0.5 update =  [[-0.0013036 ]\n"," [-0.0013036 ]\n"," [ 0.00195357]]\n","Iter:  3123 loss =  0.011154790203985257 learning rate =  0.5 update =  [[-0.00130318]\n"," [-0.00130318]\n"," [ 0.00195295]]\n","Iter:  3124 loss =  0.011151185494504718 learning rate =  0.5 update =  [[-0.00130276]\n"," [-0.00130276]\n"," [ 0.00195232]]\n","Iter:  3125 loss =  0.011147583097151723 learning rate =  0.5 update =  [[-0.00130235]\n"," [-0.00130235]\n"," [ 0.0019517 ]]\n","Iter:  3126 loss =  0.011143983009713011 learning rate =  0.5 update =  [[-0.00130193]\n"," [-0.00130193]\n"," [ 0.00195107]]\n","Iter:  3127 loss =  0.011140385229978272 learning rate =  0.5 update =  [[-0.00130151]\n"," [-0.00130151]\n"," [ 0.00195045]]\n","Iter:  3128 loss =  0.011136789755739753 learning rate =  0.5 update =  [[-0.00130109]\n"," [-0.00130109]\n"," [ 0.00194982]]\n","Iter:  3129 loss =  0.011133196584792573 learning rate =  0.5 update =  [[-0.00130068]\n"," [-0.00130068]\n"," [ 0.0019492 ]]\n","Iter:  3130 loss =  0.011129605714934918 learning rate =  0.5 update =  [[-0.00130026]\n"," [-0.00130026]\n"," [ 0.00194857]]\n","Iter:  3131 loss =  0.01112601714396745 learning rate =  0.5 update =  [[-0.00129984]\n"," [-0.00129984]\n"," [ 0.00194795]]\n","Iter:  3132 loss =  0.011122430869693747 learning rate =  0.5 update =  [[-0.00129943]\n"," [-0.00129943]\n"," [ 0.00194733]]\n","Iter:  3133 loss =  0.011118846889920184 learning rate =  0.5 update =  [[-0.00129901]\n"," [-0.00129901]\n"," [ 0.0019467 ]]\n","Iter:  3134 loss =  0.01111526520245591 learning rate =  0.5 update =  [[-0.0012986 ]\n"," [-0.0012986 ]\n"," [ 0.00194608]]\n","Iter:  3135 loss =  0.011111685805112837 learning rate =  0.5 update =  [[-0.00129818]\n"," [-0.00129818]\n"," [ 0.00194546]]\n","Iter:  3136 loss =  0.011108108695705705 learning rate =  0.5 update =  [[-0.00129777]\n"," [-0.00129777]\n"," [ 0.00194484]]\n","Iter:  3137 loss =  0.011104533872051943 learning rate =  0.5 update =  [[-0.00129735]\n"," [-0.00129735]\n"," [ 0.00194422]]\n","Iter:  3138 loss =  0.011100961331971745 learning rate =  0.5 update =  [[-0.00129694]\n"," [-0.00129694]\n"," [ 0.0019436 ]]\n","Iter:  3139 loss =  0.011097391073288158 learning rate =  0.5 update =  [[-0.00129652]\n"," [-0.00129652]\n"," [ 0.00194298]]\n","Iter:  3140 loss =  0.011093823093826949 learning rate =  0.5 update =  [[-0.00129611]\n"," [-0.00129611]\n"," [ 0.00194236]]\n","Iter:  3141 loss =  0.011090257391416558 learning rate =  0.5 update =  [[-0.00129569]\n"," [-0.00129569]\n"," [ 0.00194174]]\n","Iter:  3142 loss =  0.011086693963888231 learning rate =  0.5 update =  [[-0.00129528]\n"," [-0.00129528]\n"," [ 0.00194112]]\n","Iter:  3143 loss =  0.011083132809076038 learning rate =  0.5 update =  [[-0.00129487]\n"," [-0.00129487]\n"," [ 0.0019405 ]]\n","Iter:  3144 loss =  0.011079573924816642 learning rate =  0.5 update =  [[-0.00129446]\n"," [-0.00129446]\n"," [ 0.00193988]]\n","Iter:  3145 loss =  0.011076017308949452 learning rate =  0.5 update =  [[-0.00129404]\n"," [-0.00129404]\n"," [ 0.00193926]]\n","Iter:  3146 loss =  0.011072462959316815 learning rate =  0.5 update =  [[-0.00129363]\n"," [-0.00129363]\n"," [ 0.00193865]]\n","Iter:  3147 loss =  0.011068910873763643 learning rate =  0.5 update =  [[-0.00129322]\n"," [-0.00129322]\n"," [ 0.00193803]]\n","Iter:  3148 loss =  0.011065361050137454 learning rate =  0.5 update =  [[-0.00129281]\n"," [-0.00129281]\n"," [ 0.00193741]]\n","Iter:  3149 loss =  0.01106181348628867 learning rate =  0.5 update =  [[-0.00129239]\n"," [-0.00129239]\n"," [ 0.0019368 ]]\n","Iter:  3150 loss =  0.0110582681800704 learning rate =  0.5 update =  [[-0.00129198]\n"," [-0.00129198]\n"," [ 0.00193618]]\n","Iter:  3151 loss =  0.011054725129338427 learning rate =  0.5 update =  [[-0.00129157]\n"," [-0.00129157]\n"," [ 0.00193556]]\n","Iter:  3152 loss =  0.011051184331951174 learning rate =  0.5 update =  [[-0.00129116]\n"," [-0.00129116]\n"," [ 0.00193495]]\n","Iter:  3153 loss =  0.011047645785769875 learning rate =  0.5 update =  [[-0.00129075]\n"," [-0.00129075]\n"," [ 0.00193433]]\n","Iter:  3154 loss =  0.011044109488658515 learning rate =  0.5 update =  [[-0.00129034]\n"," [-0.00129034]\n"," [ 0.00193372]]\n","Iter:  3155 loss =  0.011040575438483496 learning rate =  0.5 update =  [[-0.00128993]\n"," [-0.00128993]\n"," [ 0.00193311]]\n","Iter:  3156 loss =  0.01103704363311422 learning rate =  0.5 update =  [[-0.00128952]\n"," [-0.00128952]\n"," [ 0.00193249]]\n","Iter:  3157 loss =  0.011033514070422547 learning rate =  0.5 update =  [[-0.00128911]\n"," [-0.00128911]\n"," [ 0.00193188]]\n","Iter:  3158 loss =  0.011029986748283208 learning rate =  0.5 update =  [[-0.0012887 ]\n"," [-0.0012887 ]\n"," [ 0.00193127]]\n","Iter:  3159 loss =  0.011026461664573425 learning rate =  0.5 update =  [[-0.00128829]\n"," [-0.00128829]\n"," [ 0.00193065]]\n","Iter:  3160 loss =  0.011022938817173245 learning rate =  0.5 update =  [[-0.00128788]\n"," [-0.00128788]\n"," [ 0.00193004]]\n","Iter:  3161 loss =  0.011019418203965278 learning rate =  0.5 update =  [[-0.00128747]\n"," [-0.00128747]\n"," [ 0.00192943]]\n","Iter:  3162 loss =  0.01101589982283479 learning rate =  0.5 update =  [[-0.00128707]\n"," [-0.00128707]\n"," [ 0.00192882]]\n","Iter:  3163 loss =  0.011012383671669814 learning rate =  0.5 update =  [[-0.00128666]\n"," [-0.00128666]\n"," [ 0.00192821]]\n","Iter:  3164 loss =  0.01100886974836095 learning rate =  0.5 update =  [[-0.00128625]\n"," [-0.00128625]\n"," [ 0.0019276 ]]\n","Iter:  3165 loss =  0.011005358050801453 learning rate =  0.5 update =  [[-0.00128584]\n"," [-0.00128584]\n"," [ 0.00192699]]\n","Iter:  3166 loss =  0.011001848576887258 learning rate =  0.5 update =  [[-0.00128544]\n"," [-0.00128544]\n"," [ 0.00192638]]\n","Iter:  3167 loss =  0.010998341324516978 learning rate =  0.5 update =  [[-0.00128503]\n"," [-0.00128503]\n"," [ 0.00192577]]\n","Iter:  3168 loss =  0.010994836291591745 learning rate =  0.5 update =  [[-0.00128462]\n"," [-0.00128462]\n"," [ 0.00192516]]\n","Iter:  3169 loss =  0.010991333476015308 learning rate =  0.5 update =  [[-0.00128422]\n"," [-0.00128421]\n"," [ 0.00192455]]\n","Iter:  3170 loss =  0.01098783287569427 learning rate =  0.5 update =  [[-0.00128381]\n"," [-0.00128381]\n"," [ 0.00192394]]\n","Iter:  3171 loss =  0.010984334488537684 learning rate =  0.5 update =  [[-0.0012834 ]\n"," [-0.0012834 ]\n"," [ 0.00192333]]\n","Iter:  3172 loss =  0.010980838312457209 learning rate =  0.5 update =  [[-0.001283  ]\n"," [-0.001283  ]\n"," [ 0.00192273]]\n","Iter:  3173 loss =  0.010977344345367231 learning rate =  0.5 update =  [[-0.00128259]\n"," [-0.00128259]\n"," [ 0.00192212]]\n","Iter:  3174 loss =  0.010973852585184593 learning rate =  0.5 update =  [[-0.00128219]\n"," [-0.00128219]\n"," [ 0.00192151]]\n","Iter:  3175 loss =  0.010970363029828931 learning rate =  0.5 update =  [[-0.00128178]\n"," [-0.00128178]\n"," [ 0.00192091]]\n","Iter:  3176 loss =  0.010966875677222421 learning rate =  0.5 update =  [[-0.00128138]\n"," [-0.00128138]\n"," [ 0.0019203 ]]\n","Iter:  3177 loss =  0.010963390525289725 learning rate =  0.5 update =  [[-0.00128097]\n"," [-0.00128097]\n"," [ 0.00191969]]\n","Iter:  3178 loss =  0.01095990757195823 learning rate =  0.5 update =  [[-0.00128057]\n"," [-0.00128057]\n"," [ 0.00191909]]\n","Iter:  3179 loss =  0.01095642681515793 learning rate =  0.5 update =  [[-0.00128016]\n"," [-0.00128016]\n"," [ 0.00191848]]\n","Iter:  3180 loss =  0.010952948252821292 learning rate =  0.5 update =  [[-0.00127976]\n"," [-0.00127976]\n"," [ 0.00191788]]\n","Iter:  3181 loss =  0.010949471882883526 learning rate =  0.5 update =  [[-0.00127936]\n"," [-0.00127936]\n"," [ 0.00191728]]\n","Iter:  3182 loss =  0.010945997703282178 learning rate =  0.5 update =  [[-0.00127895]\n"," [-0.00127895]\n"," [ 0.00191667]]\n","Iter:  3183 loss =  0.010942525711957585 learning rate =  0.5 update =  [[-0.00127855]\n"," [-0.00127855]\n"," [ 0.00191607]]\n","Iter:  3184 loss =  0.010939055906852655 learning rate =  0.5 update =  [[-0.00127815]\n"," [-0.00127815]\n"," [ 0.00191547]]\n","Iter:  3185 loss =  0.010935588285912689 learning rate =  0.5 update =  [[-0.00127775]\n"," [-0.00127775]\n"," [ 0.00191486]]\n","Iter:  3186 loss =  0.010932122847085808 learning rate =  0.5 update =  [[-0.00127734]\n"," [-0.00127734]\n"," [ 0.00191426]]\n","Iter:  3187 loss =  0.010928659588322506 learning rate =  0.5 update =  [[-0.00127694]\n"," [-0.00127694]\n"," [ 0.00191366]]\n","Iter:  3188 loss =  0.010925198507575807 learning rate =  0.5 update =  [[-0.00127654]\n"," [-0.00127654]\n"," [ 0.00191306]]\n","Iter:  3189 loss =  0.010921739602801447 learning rate =  0.5 update =  [[-0.00127614]\n"," [-0.00127614]\n"," [ 0.00191246]]\n","Iter:  3190 loss =  0.010918282871957612 learning rate =  0.5 update =  [[-0.00127574]\n"," [-0.00127574]\n"," [ 0.00191186]]\n","Iter:  3191 loss =  0.010914828313004943 learning rate =  0.5 update =  [[-0.00127534]\n"," [-0.00127534]\n"," [ 0.00191126]]\n","Iter:  3192 loss =  0.010911375923906895 learning rate =  0.5 update =  [[-0.00127494]\n"," [-0.00127494]\n"," [ 0.00191066]]\n","Iter:  3193 loss =  0.010907925702629183 learning rate =  0.5 update =  [[-0.00127453]\n"," [-0.00127453]\n"," [ 0.00191006]]\n","Iter:  3194 loss =  0.010904477647140174 learning rate =  0.5 update =  [[-0.00127413]\n"," [-0.00127413]\n"," [ 0.00190946]]\n","Iter:  3195 loss =  0.010901031755410788 learning rate =  0.5 update =  [[-0.00127373]\n"," [-0.00127373]\n"," [ 0.00190886]]\n","Iter:  3196 loss =  0.01089758802541441 learning rate =  0.5 update =  [[-0.00127333]\n"," [-0.00127333]\n"," [ 0.00190826]]\n","Iter:  3197 loss =  0.010894146455126985 learning rate =  0.5 update =  [[-0.00127294]\n"," [-0.00127294]\n"," [ 0.00190766]]\n","Iter:  3198 loss =  0.010890707042526935 learning rate =  0.5 update =  [[-0.00127254]\n"," [-0.00127254]\n"," [ 0.00190706]]\n","Iter:  3199 loss =  0.01088726978559525 learning rate =  0.5 update =  [[-0.00127214]\n"," [-0.00127214]\n"," [ 0.00190647]]\n","Iter:  3200 loss =  0.010883834682315423 learning rate =  0.5 update =  [[-0.00127174]\n"," [-0.00127174]\n"," [ 0.00190587]]\n","Iter:  3201 loss =  0.010880401730673322 learning rate =  0.5 update =  [[-0.00127134]\n"," [-0.00127134]\n"," [ 0.00190527]]\n","Iter:  3202 loss =  0.010876970928657569 learning rate =  0.5 update =  [[-0.00127094]\n"," [-0.00127094]\n"," [ 0.00190468]]\n","Iter:  3203 loss =  0.010873542274259042 learning rate =  0.5 update =  [[-0.00127054]\n"," [-0.00127054]\n"," [ 0.00190408]]\n","Iter:  3204 loss =  0.010870115765471243 learning rate =  0.5 update =  [[-0.00127015]\n"," [-0.00127015]\n"," [ 0.00190349]]\n","Iter:  3205 loss =  0.010866691400290176 learning rate =  0.5 update =  [[-0.00126975]\n"," [-0.00126975]\n"," [ 0.00190289]]\n","Iter:  3206 loss =  0.010863269176714211 learning rate =  0.5 update =  [[-0.00126935]\n"," [-0.00126935]\n"," [ 0.0019023 ]]\n","Iter:  3207 loss =  0.010859849092744345 learning rate =  0.5 update =  [[-0.00126895]\n"," [-0.00126895]\n"," [ 0.0019017 ]]\n","Iter:  3208 loss =  0.010856431146383901 learning rate =  0.5 update =  [[-0.00126856]\n"," [-0.00126856]\n"," [ 0.00190111]]\n","Iter:  3209 loss =  0.010853015335638865 learning rate =  0.5 update =  [[-0.00126816]\n"," [-0.00126816]\n"," [ 0.00190051]]\n","Iter:  3210 loss =  0.010849601658517602 learning rate =  0.5 update =  [[-0.00126777]\n"," [-0.00126777]\n"," [ 0.00189992]]\n","Iter:  3211 loss =  0.01084619011303077 learning rate =  0.5 update =  [[-0.00126737]\n"," [-0.00126737]\n"," [ 0.00189933]]\n","Iter:  3212 loss =  0.010842780697191919 learning rate =  0.5 update =  [[-0.00126697]\n"," [-0.00126697]\n"," [ 0.00189874]]\n","Iter:  3213 loss =  0.010839373409016462 learning rate =  0.5 update =  [[-0.00126658]\n"," [-0.00126658]\n"," [ 0.00189814]]\n","Iter:  3214 loss =  0.010835968246522883 learning rate =  0.5 update =  [[-0.00126618]\n"," [-0.00126618]\n"," [ 0.00189755]]\n","Iter:  3215 loss =  0.01083256520773169 learning rate =  0.5 update =  [[-0.00126579]\n"," [-0.00126579]\n"," [ 0.00189696]]\n","Iter:  3216 loss =  0.010829164290666007 learning rate =  0.5 update =  [[-0.00126539]\n"," [-0.00126539]\n"," [ 0.00189637]]\n","Iter:  3217 loss =  0.010825765493351475 learning rate =  0.5 update =  [[-0.001265  ]\n"," [-0.001265  ]\n"," [ 0.00189578]]\n","Iter:  3218 loss =  0.010822368813815977 learning rate =  0.5 update =  [[-0.0012646 ]\n"," [-0.0012646 ]\n"," [ 0.00189519]]\n","Iter:  3219 loss =  0.010818974250089904 learning rate =  0.5 update =  [[-0.00126421]\n"," [-0.00126421]\n"," [ 0.0018946 ]]\n","Iter:  3220 loss =  0.010815581800206208 learning rate =  0.5 update =  [[-0.00126382]\n"," [-0.00126382]\n"," [ 0.00189401]]\n","Iter:  3221 loss =  0.010812191462200128 learning rate =  0.5 update =  [[-0.00126342]\n"," [-0.00126342]\n"," [ 0.00189342]]\n","Iter:  3222 loss =  0.01080880323410934 learning rate =  0.5 update =  [[-0.00126303]\n"," [-0.00126303]\n"," [ 0.00189283]]\n","Iter:  3223 loss =  0.010805417113974095 learning rate =  0.5 update =  [[-0.00126264]\n"," [-0.00126264]\n"," [ 0.00189224]]\n","Iter:  3224 loss =  0.010802033099836857 learning rate =  0.5 update =  [[-0.00126224]\n"," [-0.00126224]\n"," [ 0.00189165]]\n","Iter:  3225 loss =  0.010798651189742513 learning rate =  0.5 update =  [[-0.00126185]\n"," [-0.00126185]\n"," [ 0.00189107]]\n","Iter:  3226 loss =  0.010795271381738657 learning rate =  0.5 update =  [[-0.00126146]\n"," [-0.00126146]\n"," [ 0.00189048]]\n","Iter:  3227 loss =  0.010791893673874904 learning rate =  0.5 update =  [[-0.00126107]\n"," [-0.00126107]\n"," [ 0.00188989]]\n","Iter:  3228 loss =  0.010788518064203488 learning rate =  0.5 update =  [[-0.00126067]\n"," [-0.00126067]\n"," [ 0.0018893 ]]\n","Iter:  3229 loss =  0.01078514455077895 learning rate =  0.5 update =  [[-0.00126028]\n"," [-0.00126028]\n"," [ 0.00188872]]\n","Iter:  3230 loss =  0.01078177313165839 learning rate =  0.5 update =  [[-0.00125989]\n"," [-0.00125989]\n"," [ 0.00188813]]\n","Iter:  3231 loss =  0.010778403804901106 learning rate =  0.5 update =  [[-0.0012595 ]\n"," [-0.0012595 ]\n"," [ 0.00188755]]\n","Iter:  3232 loss =  0.010775036568568786 learning rate =  0.5 update =  [[-0.00125911]\n"," [-0.00125911]\n"," [ 0.00188696]]\n","Iter:  3233 loss =  0.010771671420725762 learning rate =  0.5 update =  [[-0.00125872]\n"," [-0.00125872]\n"," [ 0.00188638]]\n","Iter:  3234 loss =  0.010768308359438365 learning rate =  0.5 update =  [[-0.00125833]\n"," [-0.00125833]\n"," [ 0.00188579]]\n","Iter:  3235 loss =  0.010764947382775652 learning rate =  0.5 update =  [[-0.00125794]\n"," [-0.00125794]\n"," [ 0.00188521]]\n","Iter:  3236 loss =  0.01076158848880885 learning rate =  0.5 update =  [[-0.00125755]\n"," [-0.00125755]\n"," [ 0.00188462]]\n","Iter:  3237 loss =  0.010758231675611659 learning rate =  0.5 update =  [[-0.00125716]\n"," [-0.00125716]\n"," [ 0.00188404]]\n","Iter:  3238 loss =  0.01075487694126 learning rate =  0.5 update =  [[-0.00125677]\n"," [-0.00125677]\n"," [ 0.00188346]]\n","Iter:  3239 loss =  0.01075152428383233 learning rate =  0.5 update =  [[-0.00125638]\n"," [-0.00125638]\n"," [ 0.00188288]]\n","Iter:  3240 loss =  0.010748173701409347 learning rate =  0.5 update =  [[-0.00125599]\n"," [-0.00125599]\n"," [ 0.00188229]]\n","Iter:  3241 loss =  0.010744825192074264 learning rate =  0.5 update =  [[-0.0012556 ]\n"," [-0.0012556 ]\n"," [ 0.00188171]]\n","Iter:  3242 loss =  0.010741478753912365 learning rate =  0.5 update =  [[-0.00125521]\n"," [-0.00125521]\n"," [ 0.00188113]]\n","Iter:  3243 loss =  0.01073813438501153 learning rate =  0.5 update =  [[-0.00125483]\n"," [-0.00125483]\n"," [ 0.00188055]]\n","Iter:  3244 loss =  0.010734792083461932 learning rate =  0.5 update =  [[-0.00125444]\n"," [-0.00125444]\n"," [ 0.00187997]]\n","Iter:  3245 loss =  0.010731451847355978 learning rate =  0.5 update =  [[-0.00125405]\n"," [-0.00125405]\n"," [ 0.00187939]]\n","Iter:  3246 loss =  0.010728113674788601 learning rate =  0.5 update =  [[-0.00125366]\n"," [-0.00125366]\n"," [ 0.00187881]]\n","Iter:  3247 loss =  0.010724777563856897 learning rate =  0.5 update =  [[-0.00125328]\n"," [-0.00125328]\n"," [ 0.00187823]]\n","Iter:  3248 loss =  0.010721443512660328 learning rate =  0.5 update =  [[-0.00125289]\n"," [-0.00125289]\n"," [ 0.00187765]]\n","Iter:  3249 loss =  0.01071811151930074 learning rate =  0.5 update =  [[-0.0012525 ]\n"," [-0.0012525 ]\n"," [ 0.00187707]]\n","Iter:  3250 loss =  0.010714781581882346 learning rate =  0.5 update =  [[-0.00125211]\n"," [-0.00125211]\n"," [ 0.00187649]]\n","Iter:  3251 loss =  0.010711453698511404 learning rate =  0.5 update =  [[-0.00125173]\n"," [-0.00125173]\n"," [ 0.00187591]]\n","Iter:  3252 loss =  0.010708127867296937 learning rate =  0.5 update =  [[-0.00125134]\n"," [-0.00125134]\n"," [ 0.00187533]]\n","Iter:  3253 loss =  0.01070480408634981 learning rate =  0.5 update =  [[-0.00125096]\n"," [-0.00125096]\n"," [ 0.00187475]]\n","Iter:  3254 loss =  0.010701482353783626 learning rate =  0.5 update =  [[-0.00125057]\n"," [-0.00125057]\n"," [ 0.00187418]]\n","Iter:  3255 loss =  0.01069816266771395 learning rate =  0.5 update =  [[-0.00125019]\n"," [-0.00125019]\n"," [ 0.0018736 ]]\n","Iter:  3256 loss =  0.010694845026258906 learning rate =  0.5 update =  [[-0.0012498 ]\n"," [-0.0012498 ]\n"," [ 0.00187302]]\n","Iter:  3257 loss =  0.01069152942753861 learning rate =  0.5 update =  [[-0.00124942]\n"," [-0.00124942]\n"," [ 0.00187245]]\n","Iter:  3258 loss =  0.010688215869675879 learning rate =  0.5 update =  [[-0.00124903]\n"," [-0.00124903]\n"," [ 0.00187187]]\n","Iter:  3259 loss =  0.010684904350795497 learning rate =  0.5 update =  [[-0.00124865]\n"," [-0.00124865]\n"," [ 0.0018713 ]]\n","Iter:  3260 loss =  0.010681594869024664 learning rate =  0.5 update =  [[-0.00124826]\n"," [-0.00124826]\n"," [ 0.00187072]]\n","Iter:  3261 loss =  0.01067828742249288 learning rate =  0.5 update =  [[-0.00124788]\n"," [-0.00124788]\n"," [ 0.00187015]]\n","Iter:  3262 loss =  0.010674982009331814 learning rate =  0.5 update =  [[-0.00124749]\n"," [-0.00124749]\n"," [ 0.00186957]]\n","Iter:  3263 loss =  0.010671678627675606 learning rate =  0.5 update =  [[-0.00124711]\n"," [-0.00124711]\n"," [ 0.001869  ]]\n","Iter:  3264 loss =  0.010668377275660501 learning rate =  0.5 update =  [[-0.00124673]\n"," [-0.00124673]\n"," [ 0.00186842]]\n","Iter:  3265 loss =  0.010665077951425073 learning rate =  0.5 update =  [[-0.00124634]\n"," [-0.00124634]\n"," [ 0.00186785]]\n","Iter:  3266 loss =  0.010661780653110152 learning rate =  0.5 update =  [[-0.00124596]\n"," [-0.00124596]\n"," [ 0.00186728]]\n","Iter:  3267 loss =  0.010658485378858835 learning rate =  0.5 update =  [[-0.00124558]\n"," [-0.00124558]\n"," [ 0.0018667 ]]\n","Iter:  3268 loss =  0.010655192126816553 learning rate =  0.5 update =  [[-0.0012452 ]\n"," [-0.0012452 ]\n"," [ 0.00186613]]\n","Iter:  3269 loss =  0.010651900895130871 learning rate =  0.5 update =  [[-0.00124481]\n"," [-0.00124481]\n"," [ 0.00186556]]\n","Iter:  3270 loss =  0.010648611681951716 learning rate =  0.5 update =  [[-0.00124443]\n"," [-0.00124443]\n"," [ 0.00186499]]\n","Iter:  3271 loss =  0.0106453244854312 learning rate =  0.5 update =  [[-0.00124405]\n"," [-0.00124405]\n"," [ 0.00186442]]\n","Iter:  3272 loss =  0.01064203930372366 learning rate =  0.5 update =  [[-0.00124367]\n"," [-0.00124367]\n"," [ 0.00186384]]\n","Iter:  3273 loss =  0.010638756134985756 learning rate =  0.5 update =  [[-0.00124329]\n"," [-0.00124329]\n"," [ 0.00186327]]\n","Iter:  3274 loss =  0.010635474977376345 learning rate =  0.5 update =  [[-0.00124291]\n"," [-0.00124291]\n"," [ 0.0018627 ]]\n","Iter:  3275 loss =  0.010632195829056555 learning rate =  0.5 update =  [[-0.00124253]\n"," [-0.00124253]\n"," [ 0.00186213]]\n","Iter:  3276 loss =  0.010628918688189621 learning rate =  0.5 update =  [[-0.00124215]\n"," [-0.00124215]\n"," [ 0.00186156]]\n","Iter:  3277 loss =  0.01062564355294119 learning rate =  0.5 update =  [[-0.00124177]\n"," [-0.00124177]\n"," [ 0.00186099]]\n","Iter:  3278 loss =  0.0106223704214791 learning rate =  0.5 update =  [[-0.00124139]\n"," [-0.00124139]\n"," [ 0.00186042]]\n","Iter:  3279 loss =  0.01061909929197324 learning rate =  0.5 update =  [[-0.00124101]\n"," [-0.00124101]\n"," [ 0.00185986]]\n","Iter:  3280 loss =  0.010615830162595884 learning rate =  0.5 update =  [[-0.00124063]\n"," [-0.00124063]\n"," [ 0.00185929]]\n","Iter:  3281 loss =  0.010612563031521464 learning rate =  0.5 update =  [[-0.00124025]\n"," [-0.00124025]\n"," [ 0.00185872]]\n","Iter:  3282 loss =  0.010609297896926748 learning rate =  0.5 update =  [[-0.00123987]\n"," [-0.00123987]\n"," [ 0.00185815]]\n","Iter:  3283 loss =  0.010606034756990493 learning rate =  0.5 update =  [[-0.00123949]\n"," [-0.00123949]\n"," [ 0.00185759]]\n","Iter:  3284 loss =  0.010602773609893897 learning rate =  0.5 update =  [[-0.00123911]\n"," [-0.00123911]\n"," [ 0.00185702]]\n","Iter:  3285 loss =  0.010599514453820066 learning rate =  0.5 update =  [[-0.00123873]\n"," [-0.00123873]\n"," [ 0.00185645]]\n","Iter:  3286 loss =  0.010596257286954631 learning rate =  0.5 update =  [[-0.00123835]\n"," [-0.00123835]\n"," [ 0.00185589]]\n","Iter:  3287 loss =  0.010593002107485221 learning rate =  0.5 update =  [[-0.00123798]\n"," [-0.00123798]\n"," [ 0.00185532]]\n","Iter:  3288 loss =  0.010589748913601704 learning rate =  0.5 update =  [[-0.0012376 ]\n"," [-0.0012376 ]\n"," [ 0.00185475]]\n","Iter:  3289 loss =  0.010586497703496168 learning rate =  0.5 update =  [[-0.00123722]\n"," [-0.00123722]\n"," [ 0.00185419]]\n","Iter:  3290 loss =  0.010583248475362829 learning rate =  0.5 update =  [[-0.00123684]\n"," [-0.00123684]\n"," [ 0.00185362]]\n","Iter:  3291 loss =  0.010580001227398179 learning rate =  0.5 update =  [[-0.00123647]\n"," [-0.00123647]\n"," [ 0.00185306]]\n","Iter:  3292 loss =  0.010576755957800757 learning rate =  0.5 update =  [[-0.00123609]\n"," [-0.00123609]\n"," [ 0.00185249]]\n","Iter:  3293 loss =  0.010573512664771433 learning rate =  0.5 update =  [[-0.00123571]\n"," [-0.00123571]\n"," [ 0.00185193]]\n","Iter:  3294 loss =  0.010570271346513116 learning rate =  0.5 update =  [[-0.00123534]\n"," [-0.00123534]\n"," [ 0.00185137]]\n","Iter:  3295 loss =  0.010567032001230887 learning rate =  0.5 update =  [[-0.00123496]\n"," [-0.00123496]\n"," [ 0.0018508 ]]\n","Iter:  3296 loss =  0.010563794627132174 learning rate =  0.5 update =  [[-0.00123458]\n"," [-0.00123458]\n"," [ 0.00185024]]\n","Iter:  3297 loss =  0.010560559222426342 learning rate =  0.5 update =  [[-0.00123421]\n"," [-0.00123421]\n"," [ 0.00184968]]\n","Iter:  3298 loss =  0.010557325785325172 learning rate =  0.5 update =  [[-0.00123383]\n"," [-0.00123383]\n"," [ 0.00184912]]\n","Iter:  3299 loss =  0.010554094314042294 learning rate =  0.5 update =  [[-0.00123346]\n"," [-0.00123346]\n"," [ 0.00184856]]\n","Iter:  3300 loss =  0.01055086480679363 learning rate =  0.5 update =  [[-0.00123308]\n"," [-0.00123308]\n"," [ 0.00184799]]\n","Iter:  3301 loss =  0.010547637261797412 learning rate =  0.5 update =  [[-0.00123271]\n"," [-0.00123271]\n"," [ 0.00184743]]\n","Iter:  3302 loss =  0.01054441167727372 learning rate =  0.5 update =  [[-0.00123233]\n"," [-0.00123233]\n"," [ 0.00184687]]\n","Iter:  3303 loss =  0.010541188051445069 learning rate =  0.5 update =  [[-0.00123196]\n"," [-0.00123196]\n"," [ 0.00184631]]\n","Iter:  3304 loss =  0.01053796638253594 learning rate =  0.5 update =  [[-0.00123159]\n"," [-0.00123159]\n"," [ 0.00184575]]\n","Iter:  3305 loss =  0.010534746668772913 learning rate =  0.5 update =  [[-0.00123121]\n"," [-0.00123121]\n"," [ 0.00184519]]\n","Iter:  3306 loss =  0.010531528908384904 learning rate =  0.5 update =  [[-0.00123084]\n"," [-0.00123084]\n"," [ 0.00184463]]\n","Iter:  3307 loss =  0.01052831309960275 learning rate =  0.5 update =  [[-0.00123046]\n"," [-0.00123046]\n"," [ 0.00184407]]\n","Iter:  3308 loss =  0.010525099240659572 learning rate =  0.5 update =  [[-0.00123009]\n"," [-0.00123009]\n"," [ 0.00184351]]\n","Iter:  3309 loss =  0.010521887329790527 learning rate =  0.5 update =  [[-0.00122972]\n"," [-0.00122972]\n"," [ 0.00184296]]\n","Iter:  3310 loss =  0.010518677365232935 learning rate =  0.5 update =  [[-0.00122935]\n"," [-0.00122935]\n"," [ 0.0018424 ]]\n","Iter:  3311 loss =  0.010515469345226158 learning rate =  0.5 update =  [[-0.00122897]\n"," [-0.00122897]\n"," [ 0.00184184]]\n","Iter:  3312 loss =  0.010512263268011777 learning rate =  0.5 update =  [[-0.0012286 ]\n"," [-0.0012286 ]\n"," [ 0.00184128]]\n","Iter:  3313 loss =  0.010509059131833448 learning rate =  0.5 update =  [[-0.00122823]\n"," [-0.00122823]\n"," [ 0.00184073]]\n","Iter:  3314 loss =  0.010505856934936946 learning rate =  0.5 update =  [[-0.00122786]\n"," [-0.00122786]\n"," [ 0.00184017]]\n","Iter:  3315 loss =  0.010502656675570049 learning rate =  0.5 update =  [[-0.00122749]\n"," [-0.00122749]\n"," [ 0.00183961]]\n","Iter:  3316 loss =  0.010499458351982793 learning rate =  0.5 update =  [[-0.00122711]\n"," [-0.00122711]\n"," [ 0.00183906]]\n","Iter:  3317 loss =  0.010496261962427327 learning rate =  0.5 update =  [[-0.00122674]\n"," [-0.00122674]\n"," [ 0.0018385 ]]\n","Iter:  3318 loss =  0.010493067505157667 learning rate =  0.5 update =  [[-0.00122637]\n"," [-0.00122637]\n"," [ 0.00183794]]\n","Iter:  3319 loss =  0.010489874978430102 learning rate =  0.5 update =  [[-0.001226  ]\n"," [-0.001226  ]\n"," [ 0.00183739]]\n","Iter:  3320 loss =  0.01048668438050308 learning rate =  0.5 update =  [[-0.00122563]\n"," [-0.00122563]\n"," [ 0.00183683]]\n","Iter:  3321 loss =  0.010483495709636949 learning rate =  0.5 update =  [[-0.00122526]\n"," [-0.00122526]\n"," [ 0.00183628]]\n","Iter:  3322 loss =  0.01048030896409425 learning rate =  0.5 update =  [[-0.00122489]\n"," [-0.00122489]\n"," [ 0.00183573]]\n","Iter:  3323 loss =  0.010477124142139587 learning rate =  0.5 update =  [[-0.00122452]\n"," [-0.00122452]\n"," [ 0.00183517]]\n","Iter:  3324 loss =  0.010473941242039646 learning rate =  0.5 update =  [[-0.00122415]\n"," [-0.00122415]\n"," [ 0.00183462]]\n","Iter:  3325 loss =  0.010470760262063172 learning rate =  0.5 update =  [[-0.00122378]\n"," [-0.00122378]\n"," [ 0.00183407]]\n","Iter:  3326 loss =  0.010467581200481003 learning rate =  0.5 update =  [[-0.00122341]\n"," [-0.00122341]\n"," [ 0.00183351]]\n","Iter:  3327 loss =  0.010464404055566002 learning rate =  0.5 update =  [[-0.00122304]\n"," [-0.00122304]\n"," [ 0.00183296]]\n","Iter:  3328 loss =  0.010461228825593196 learning rate =  0.5 update =  [[-0.00122267]\n"," [-0.00122267]\n"," [ 0.00183241]]\n","Iter:  3329 loss =  0.010458055508839544 learning rate =  0.5 update =  [[-0.00122231]\n"," [-0.00122231]\n"," [ 0.00183186]]\n","Iter:  3330 loss =  0.010454884103584194 learning rate =  0.5 update =  [[-0.00122194]\n"," [-0.00122194]\n"," [ 0.00183131]]\n","Iter:  3331 loss =  0.01045171460810827 learning rate =  0.5 update =  [[-0.00122157]\n"," [-0.00122157]\n"," [ 0.00183075]]\n","Iter:  3332 loss =  0.010448547020694865 learning rate =  0.5 update =  [[-0.0012212]\n"," [-0.0012212]\n"," [ 0.0018302]]\n","Iter:  3333 loss =  0.010445381339629337 learning rate =  0.5 update =  [[-0.00122083]\n"," [-0.00122083]\n"," [ 0.00182965]]\n","Iter:  3334 loss =  0.010442217563198948 learning rate =  0.5 update =  [[-0.00122047]\n"," [-0.00122047]\n"," [ 0.0018291 ]]\n","Iter:  3335 loss =  0.010439055689693002 learning rate =  0.5 update =  [[-0.0012201 ]\n"," [-0.0012201 ]\n"," [ 0.00182855]]\n","Iter:  3336 loss =  0.010435895717402905 learning rate =  0.5 update =  [[-0.00121973]\n"," [-0.00121973]\n"," [ 0.001828  ]]\n","Iter:  3337 loss =  0.010432737644622075 learning rate =  0.5 update =  [[-0.00121937]\n"," [-0.00121937]\n"," [ 0.00182745]]\n","Iter:  3338 loss =  0.010429581469645947 learning rate =  0.5 update =  [[-0.001219  ]\n"," [-0.001219  ]\n"," [ 0.00182691]]\n","Iter:  3339 loss =  0.010426427190772001 learning rate =  0.5 update =  [[-0.00121863]\n"," [-0.00121863]\n"," [ 0.00182636]]\n","Iter:  3340 loss =  0.010423274806299784 learning rate =  0.5 update =  [[-0.00121827]\n"," [-0.00121827]\n"," [ 0.00182581]]\n","Iter:  3341 loss =  0.010420124314530776 learning rate =  0.5 update =  [[-0.0012179 ]\n"," [-0.0012179 ]\n"," [ 0.00182526]]\n","Iter:  3342 loss =  0.010416975713768553 learning rate =  0.5 update =  [[-0.00121754]\n"," [-0.00121754]\n"," [ 0.00182471]]\n","Iter:  3343 loss =  0.010413829002318738 learning rate =  0.5 update =  [[-0.00121717]\n"," [-0.00121717]\n"," [ 0.00182417]]\n","Iter:  3344 loss =  0.010410684178488883 learning rate =  0.5 update =  [[-0.00121681]\n"," [-0.00121681]\n"," [ 0.00182362]]\n","Iter:  3345 loss =  0.010407541240588524 learning rate =  0.5 update =  [[-0.00121644]\n"," [-0.00121644]\n"," [ 0.00182307]]\n","Iter:  3346 loss =  0.01040440018692947 learning rate =  0.5 update =  [[-0.00121608]\n"," [-0.00121608]\n"," [ 0.00182253]]\n","Iter:  3347 loss =  0.010401261015825253 learning rate =  0.5 update =  [[-0.00121571]\n"," [-0.00121571]\n"," [ 0.00182198]]\n","Iter:  3348 loss =  0.01039812372559143 learning rate =  0.5 update =  [[-0.00121535]\n"," [-0.00121535]\n"," [ 0.00182144]]\n","Iter:  3349 loss =  0.01039498831454576 learning rate =  0.5 update =  [[-0.00121498]\n"," [-0.00121498]\n"," [ 0.00182089]]\n","Iter:  3350 loss =  0.010391854781007799 learning rate =  0.5 update =  [[-0.00121462]\n"," [-0.00121462]\n"," [ 0.00182035]]\n","Iter:  3351 loss =  0.01038872312329928 learning rate =  0.5 update =  [[-0.00121425]\n"," [-0.00121425]\n"," [ 0.0018198 ]]\n","Iter:  3352 loss =  0.010385593339743725 learning rate =  0.5 update =  [[-0.00121389]\n"," [-0.00121389]\n"," [ 0.00181926]]\n","Iter:  3353 loss =  0.010382465428666755 learning rate =  0.5 update =  [[-0.00121353]\n"," [-0.00121353]\n"," [ 0.00181871]]\n","Iter:  3354 loss =  0.010379339388396011 learning rate =  0.5 update =  [[-0.00121316]\n"," [-0.00121316]\n"," [ 0.00181817]]\n","Iter:  3355 loss =  0.010376215217261106 learning rate =  0.5 update =  [[-0.0012128 ]\n"," [-0.0012128 ]\n"," [ 0.00181763]]\n","Iter:  3356 loss =  0.010373092913593568 learning rate =  0.5 update =  [[-0.00121244]\n"," [-0.00121244]\n"," [ 0.00181708]]\n","Iter:  3357 loss =  0.010369972475726956 learning rate =  0.5 update =  [[-0.00121208]\n"," [-0.00121208]\n"," [ 0.00181654]]\n","Iter:  3358 loss =  0.01036685390199679 learning rate =  0.5 update =  [[-0.00121171]\n"," [-0.00121171]\n"," [ 0.001816  ]]\n","Iter:  3359 loss =  0.010363737190740607 learning rate =  0.5 update =  [[-0.00121135]\n"," [-0.00121135]\n"," [ 0.00181546]]\n","Iter:  3360 loss =  0.010360622340297752 learning rate =  0.5 update =  [[-0.00121099]\n"," [-0.00121099]\n"," [ 0.00181491]]\n","Iter:  3361 loss =  0.010357509349009815 learning rate =  0.5 update =  [[-0.00121063]\n"," [-0.00121063]\n"," [ 0.00181437]]\n","Iter:  3362 loss =  0.010354398215220056 learning rate =  0.5 update =  [[-0.00121027]\n"," [-0.00121027]\n"," [ 0.00181383]]\n","Iter:  3363 loss =  0.010351288937273922 learning rate =  0.5 update =  [[-0.00120991]\n"," [-0.00120991]\n"," [ 0.00181329]]\n","Iter:  3364 loss =  0.010348181513518675 learning rate =  0.5 update =  [[-0.00120955]\n"," [-0.00120955]\n"," [ 0.00181275]]\n","Iter:  3365 loss =  0.01034507594230364 learning rate =  0.5 update =  [[-0.00120919]\n"," [-0.00120919]\n"," [ 0.00181221]]\n","Iter:  3366 loss =  0.010341972221980023 learning rate =  0.5 update =  [[-0.00120883]\n"," [-0.00120882]\n"," [ 0.00181167]]\n","Iter:  3367 loss =  0.010338870350900947 learning rate =  0.5 update =  [[-0.00120846]\n"," [-0.00120846]\n"," [ 0.00181113]]\n","Iter:  3368 loss =  0.010335770327421615 learning rate =  0.5 update =  [[-0.0012081 ]\n"," [-0.0012081 ]\n"," [ 0.00181059]]\n","Iter:  3369 loss =  0.010332672149899098 learning rate =  0.5 update =  [[-0.00120774]\n"," [-0.00120774]\n"," [ 0.00181005]]\n","Iter:  3370 loss =  0.010329575816692246 learning rate =  0.5 update =  [[-0.00120739]\n"," [-0.00120739]\n"," [ 0.00180951]]\n","Iter:  3371 loss =  0.010326481326162132 learning rate =  0.5 update =  [[-0.00120703]\n"," [-0.00120703]\n"," [ 0.00180898]]\n","Iter:  3372 loss =  0.010323388676671662 learning rate =  0.5 update =  [[-0.00120667]\n"," [-0.00120667]\n"," [ 0.00180844]]\n","Iter:  3373 loss =  0.010320297866585587 learning rate =  0.5 update =  [[-0.00120631]\n"," [-0.00120631]\n"," [ 0.0018079 ]]\n","Iter:  3374 loss =  0.010317208894270591 learning rate =  0.5 update =  [[-0.00120595]\n"," [-0.00120595]\n"," [ 0.00180736]]\n","Iter:  3375 loss =  0.010314121758095448 learning rate =  0.5 update =  [[-0.00120559]\n"," [-0.00120559]\n"," [ 0.00180683]]\n","Iter:  3376 loss =  0.01031103645643075 learning rate =  0.5 update =  [[-0.00120523]\n"," [-0.00120523]\n"," [ 0.00180629]]\n","Iter:  3377 loss =  0.010307952987648843 learning rate =  0.5 update =  [[-0.00120487]\n"," [-0.00120487]\n"," [ 0.00180575]]\n","Iter:  3378 loss =  0.010304871350124346 learning rate =  0.5 update =  [[-0.00120452]\n"," [-0.00120452]\n"," [ 0.00180522]]\n","Iter:  3379 loss =  0.010301791542233516 learning rate =  0.5 update =  [[-0.00120416]\n"," [-0.00120416]\n"," [ 0.00180468]]\n","Iter:  3380 loss =  0.01029871356235463 learning rate =  0.5 update =  [[-0.0012038 ]\n"," [-0.0012038 ]\n"," [ 0.00180415]]\n","Iter:  3381 loss =  0.010295637408867905 learning rate =  0.5 update =  [[-0.00120344]\n"," [-0.00120344]\n"," [ 0.00180361]]\n","Iter:  3382 loss =  0.010292563080155307 learning rate =  0.5 update =  [[-0.00120309]\n"," [-0.00120309]\n"," [ 0.00180308]]\n","Iter:  3383 loss =  0.010289490574600938 learning rate =  0.5 update =  [[-0.00120273]\n"," [-0.00120273]\n"," [ 0.00180254]]\n","Iter:  3384 loss =  0.010286419890590554 learning rate =  0.5 update =  [[-0.00120237]\n"," [-0.00120237]\n"," [ 0.00180201]]\n","Iter:  3385 loss =  0.010283351026511978 learning rate =  0.5 update =  [[-0.00120202]\n"," [-0.00120202]\n"," [ 0.00180148]]\n","Iter:  3386 loss =  0.010280283980754954 learning rate =  0.5 update =  [[-0.00120166]\n"," [-0.00120166]\n"," [ 0.00180094]]\n","Iter:  3387 loss =  0.010277218751710906 learning rate =  0.5 update =  [[-0.0012013 ]\n"," [-0.0012013 ]\n"," [ 0.00180041]]\n","Iter:  3388 loss =  0.01027415533777349 learning rate =  0.5 update =  [[-0.00120095]\n"," [-0.00120095]\n"," [ 0.00179988]]\n","Iter:  3389 loss =  0.010271093737337868 learning rate =  0.5 update =  [[-0.00120059]\n"," [-0.00120059]\n"," [ 0.00179934]]\n","Iter:  3390 loss =  0.01026803394880135 learning rate =  0.5 update =  [[-0.00120024]\n"," [-0.00120024]\n"," [ 0.00179881]]\n","Iter:  3391 loss =  0.010264975970563034 learning rate =  0.5 update =  [[-0.00119988]\n"," [-0.00119988]\n"," [ 0.00179828]]\n","Iter:  3392 loss =  0.010261919801024002 learning rate =  0.5 update =  [[-0.00119953]\n"," [-0.00119953]\n"," [ 0.00179775]]\n","Iter:  3393 loss =  0.01025886543858692 learning rate =  0.5 update =  [[-0.00119917]\n"," [-0.00119917]\n"," [ 0.00179722]]\n","Iter:  3394 loss =  0.010255812881656692 learning rate =  0.5 update =  [[-0.00119882]\n"," [-0.00119882]\n"," [ 0.00179669]]\n","Iter:  3395 loss =  0.010252762128639868 learning rate =  0.5 update =  [[-0.00119846]\n"," [-0.00119846]\n"," [ 0.00179616]]\n","Iter:  3396 loss =  0.010249713177945008 learning rate =  0.5 update =  [[-0.00119811]\n"," [-0.00119811]\n"," [ 0.00179563]]\n","Iter:  3397 loss =  0.01024666602798234 learning rate =  0.5 update =  [[-0.00119776]\n"," [-0.00119776]\n"," [ 0.0017951 ]]\n","Iter:  3398 loss =  0.010243620677164136 learning rate =  0.5 update =  [[-0.0011974 ]\n"," [-0.0011974 ]\n"," [ 0.00179457]]\n","Iter:  3399 loss =  0.0102405771239045 learning rate =  0.5 update =  [[-0.00119705]\n"," [-0.00119705]\n"," [ 0.00179404]]\n","Iter:  3400 loss =  0.010237535366619248 learning rate =  0.5 update =  [[-0.00119669]\n"," [-0.00119669]\n"," [ 0.00179351]]\n","Iter:  3401 loss =  0.010234495403726257 learning rate =  0.5 update =  [[-0.00119634]\n"," [-0.00119634]\n"," [ 0.00179298]]\n","Iter:  3402 loss =  0.0102314572336451 learning rate =  0.5 update =  [[-0.00119599]\n"," [-0.00119599]\n"," [ 0.00179245]]\n","Iter:  3403 loss =  0.010228420854797311 learning rate =  0.5 update =  [[-0.00119564]\n"," [-0.00119564]\n"," [ 0.00179192]]\n","Iter:  3404 loss =  0.0102253862656062 learning rate =  0.5 update =  [[-0.00119528]\n"," [-0.00119528]\n"," [ 0.00179139]]\n","Iter:  3405 loss =  0.010222353464496927 learning rate =  0.5 update =  [[-0.00119493]\n"," [-0.00119493]\n"," [ 0.00179087]]\n","Iter:  3406 loss =  0.010219322449896482 learning rate =  0.5 update =  [[-0.00119458]\n"," [-0.00119458]\n"," [ 0.00179034]]\n","Iter:  3407 loss =  0.010216293220233763 learning rate =  0.5 update =  [[-0.00119423]\n"," [-0.00119423]\n"," [ 0.00178981]]\n","Iter:  3408 loss =  0.010213265773939443 learning rate =  0.5 update =  [[-0.00119388]\n"," [-0.00119388]\n"," [ 0.00178929]]\n","Iter:  3409 loss =  0.010210240109446097 learning rate =  0.5 update =  [[-0.00119352]\n"," [-0.00119352]\n"," [ 0.00178876]]\n","Iter:  3410 loss =  0.010207216225187969 learning rate =  0.5 update =  [[-0.00119317]\n"," [-0.00119317]\n"," [ 0.00178823]]\n","Iter:  3411 loss =  0.01020419411960134 learning rate =  0.5 update =  [[-0.00119282]\n"," [-0.00119282]\n"," [ 0.00178771]]\n","Iter:  3412 loss =  0.010201173791124138 learning rate =  0.5 update =  [[-0.00119247]\n"," [-0.00119247]\n"," [ 0.00178718]]\n","Iter:  3413 loss =  0.01019815523819615 learning rate =  0.5 update =  [[-0.00119212]\n"," [-0.00119212]\n"," [ 0.00178666]]\n","Iter:  3414 loss =  0.010195138459259184 learning rate =  0.5 update =  [[-0.00119177]\n"," [-0.00119177]\n"," [ 0.00178613]]\n","Iter:  3415 loss =  0.01019212345275657 learning rate =  0.5 update =  [[-0.00119142]\n"," [-0.00119142]\n"," [ 0.00178561]]\n","Iter:  3416 loss =  0.010189110217133587 learning rate =  0.5 update =  [[-0.00119107]\n"," [-0.00119107]\n"," [ 0.00178508]]\n","Iter:  3417 loss =  0.010186098750837423 learning rate =  0.5 update =  [[-0.00119072]\n"," [-0.00119072]\n"," [ 0.00178456]]\n","Iter:  3418 loss =  0.0101830890523169 learning rate =  0.5 update =  [[-0.00119037]\n"," [-0.00119037]\n"," [ 0.00178404]]\n","Iter:  3419 loss =  0.010180081120022704 learning rate =  0.5 update =  [[-0.00119002]\n"," [-0.00119002]\n"," [ 0.00178351]]\n","Iter:  3420 loss =  0.010177074952407399 learning rate =  0.5 update =  [[-0.00118967]\n"," [-0.00118967]\n"," [ 0.00178299]]\n","Iter:  3421 loss =  0.010174070547925296 learning rate =  0.5 update =  [[-0.00118932]\n"," [-0.00118932]\n"," [ 0.00178247]]\n","Iter:  3422 loss =  0.010171067905032463 learning rate =  0.5 update =  [[-0.00118897]\n"," [-0.00118897]\n"," [ 0.00178195]]\n","Iter:  3423 loss =  0.010168067022186865 learning rate =  0.5 update =  [[-0.00118862]\n"," [-0.00118862]\n"," [ 0.00178142]]\n","Iter:  3424 loss =  0.010165067897848134 learning rate =  0.5 update =  [[-0.00118828]\n"," [-0.00118828]\n"," [ 0.0017809 ]]\n","Iter:  3425 loss =  0.010162070530477803 learning rate =  0.5 update =  [[-0.00118793]\n"," [-0.00118793]\n"," [ 0.00178038]]\n","Iter:  3426 loss =  0.010159074918539155 learning rate =  0.5 update =  [[-0.00118758]\n"," [-0.00118758]\n"," [ 0.00177986]]\n","Iter:  3427 loss =  0.010156081060497198 learning rate =  0.5 update =  [[-0.00118723]\n"," [-0.00118723]\n"," [ 0.00177934]]\n","Iter:  3428 loss =  0.0101530889548189 learning rate =  0.5 update =  [[-0.00118688]\n"," [-0.00118688]\n"," [ 0.00177882]]\n","Iter:  3429 loss =  0.010150098599972748 learning rate =  0.5 update =  [[-0.00118654]\n"," [-0.00118654]\n"," [ 0.0017783 ]]\n","Iter:  3430 loss =  0.01014710999442927 learning rate =  0.5 update =  [[-0.00118619]\n"," [-0.00118619]\n"," [ 0.00177778]]\n","Iter:  3431 loss =  0.010144123136660552 learning rate =  0.5 update =  [[-0.00118584]\n"," [-0.00118584]\n"," [ 0.00177726]]\n","Iter:  3432 loss =  0.010141138025140571 learning rate =  0.5 update =  [[-0.0011855 ]\n"," [-0.0011855 ]\n"," [ 0.00177674]]\n","Iter:  3433 loss =  0.01013815465834509 learning rate =  0.5 update =  [[-0.00118515]\n"," [-0.00118515]\n"," [ 0.00177622]]\n","Iter:  3434 loss =  0.010135173034751598 learning rate =  0.5 update =  [[-0.0011848]\n"," [-0.0011848]\n"," [ 0.0017757]]\n","Iter:  3435 loss =  0.010132193152839392 learning rate =  0.5 update =  [[-0.00118446]\n"," [-0.00118446]\n"," [ 0.00177518]]\n","Iter:  3436 loss =  0.01012921501108938 learning rate =  0.5 update =  [[-0.00118411]\n"," [-0.00118411]\n"," [ 0.00177466]]\n","Iter:  3437 loss =  0.010126238607984346 learning rate =  0.5 update =  [[-0.00118377]\n"," [-0.00118377]\n"," [ 0.00177415]]\n","Iter:  3438 loss =  0.010123263942008954 learning rate =  0.5 update =  [[-0.00118342]\n"," [-0.00118342]\n"," [ 0.00177363]]\n","Iter:  3439 loss =  0.010120291011649407 learning rate =  0.5 update =  [[-0.00118307]\n"," [-0.00118307]\n"," [ 0.00177311]]\n","Iter:  3440 loss =  0.010117319815393832 learning rate =  0.5 update =  [[-0.00118273]\n"," [-0.00118273]\n"," [ 0.0017726 ]]\n","Iter:  3441 loss =  0.010114350351731933 learning rate =  0.5 update =  [[-0.00118238]\n"," [-0.00118238]\n"," [ 0.00177208]]\n","Iter:  3442 loss =  0.010111382619155226 learning rate =  0.5 update =  [[-0.00118204]\n"," [-0.00118204]\n"," [ 0.00177156]]\n","Iter:  3443 loss =  0.01010841661615707 learning rate =  0.5 update =  [[-0.0011817 ]\n"," [-0.0011817 ]\n"," [ 0.00177105]]\n","Iter:  3444 loss =  0.010105452341232543 learning rate =  0.5 update =  [[-0.00118135]\n"," [-0.00118135]\n"," [ 0.00177053]]\n","Iter:  3445 loss =  0.010102489792878366 learning rate =  0.5 update =  [[-0.00118101]\n"," [-0.00118101]\n"," [ 0.00177002]]\n","Iter:  3446 loss =  0.010099528969592987 learning rate =  0.5 update =  [[-0.00118066]\n"," [-0.00118066]\n"," [ 0.0017695 ]]\n","Iter:  3447 loss =  0.010096569869876771 learning rate =  0.5 update =  [[-0.00118032]\n"," [-0.00118032]\n"," [ 0.00176899]]\n","Iter:  3448 loss =  0.010093612492231587 learning rate =  0.5 update =  [[-0.00117998]\n"," [-0.00117998]\n"," [ 0.00176847]]\n","Iter:  3449 loss =  0.01009065683516117 learning rate =  0.5 update =  [[-0.00117963]\n"," [-0.00117963]\n"," [ 0.00176796]]\n","Iter:  3450 loss =  0.010087702897170921 learning rate =  0.5 update =  [[-0.00117929]\n"," [-0.00117929]\n"," [ 0.00176744]]\n","Iter:  3451 loss =  0.010084750676768162 learning rate =  0.5 update =  [[-0.00117895]\n"," [-0.00117895]\n"," [ 0.00176693]]\n","Iter:  3452 loss =  0.010081800172461578 learning rate =  0.5 update =  [[-0.0011786 ]\n"," [-0.0011786 ]\n"," [ 0.00176642]]\n","Iter:  3453 loss =  0.010078851382761798 learning rate =  0.5 update =  [[-0.00117826]\n"," [-0.00117826]\n"," [ 0.0017659 ]]\n","Iter:  3454 loss =  0.010075904306181264 learning rate =  0.5 update =  [[-0.00117792]\n"," [-0.00117792]\n"," [ 0.00176539]]\n","Iter:  3455 loss =  0.010072958941233862 learning rate =  0.5 update =  [[-0.00117758]\n"," [-0.00117758]\n"," [ 0.00176488]]\n","Iter:  3456 loss =  0.01007001528643548 learning rate =  0.5 update =  [[-0.00117723]\n"," [-0.00117723]\n"," [ 0.00176437]]\n","Iter:  3457 loss =  0.010067073340303392 learning rate =  0.5 update =  [[-0.00117689]\n"," [-0.00117689]\n"," [ 0.00176385]]\n","Iter:  3458 loss =  0.010064133101356943 learning rate =  0.5 update =  [[-0.00117655]\n"," [-0.00117655]\n"," [ 0.00176334]]\n","Iter:  3459 loss =  0.010061194568116934 learning rate =  0.5 update =  [[-0.00117621]\n"," [-0.00117621]\n"," [ 0.00176283]]\n","Iter:  3460 loss =  0.010058257739105902 learning rate =  0.5 update =  [[-0.00117587]\n"," [-0.00117587]\n"," [ 0.00176232]]\n","Iter:  3461 loss =  0.010055322612848104 learning rate =  0.5 update =  [[-0.00117553]\n"," [-0.00117553]\n"," [ 0.00176181]]\n","Iter:  3462 loss =  0.010052389187869548 learning rate =  0.5 update =  [[-0.00117519]\n"," [-0.00117519]\n"," [ 0.0017613 ]]\n","Iter:  3463 loss =  0.010049457462697886 learning rate =  0.5 update =  [[-0.00117485]\n"," [-0.00117485]\n"," [ 0.00176079]]\n","Iter:  3464 loss =  0.010046527435862544 learning rate =  0.5 update =  [[-0.0011745 ]\n"," [-0.0011745 ]\n"," [ 0.00176028]]\n","Iter:  3465 loss =  0.010043599105894505 learning rate =  0.5 update =  [[-0.00117416]\n"," [-0.00117416]\n"," [ 0.00175977]]\n","Iter:  3466 loss =  0.010040672471326534 learning rate =  0.5 update =  [[-0.00117382]\n"," [-0.00117382]\n"," [ 0.00175926]]\n","Iter:  3467 loss =  0.010037747530692975 learning rate =  0.5 update =  [[-0.00117348]\n"," [-0.00117348]\n"," [ 0.00175875]]\n","Iter:  3468 loss =  0.010034824282529988 learning rate =  0.5 update =  [[-0.00117315]\n"," [-0.00117315]\n"," [ 0.00175824]]\n","Iter:  3469 loss =  0.01003190272537539 learning rate =  0.5 update =  [[-0.00117281]\n"," [-0.00117281]\n"," [ 0.00175774]]\n","Iter:  3470 loss =  0.010028982857768615 learning rate =  0.5 update =  [[-0.00117247]\n"," [-0.00117247]\n"," [ 0.00175723]]\n","Iter:  3471 loss =  0.01002606467825082 learning rate =  0.5 update =  [[-0.00117213]\n"," [-0.00117213]\n"," [ 0.00175672]]\n","Iter:  3472 loss =  0.01002314818536474 learning rate =  0.5 update =  [[-0.00117179]\n"," [-0.00117179]\n"," [ 0.00175621]]\n","Iter:  3473 loss =  0.01002023337765498 learning rate =  0.5 update =  [[-0.00117145]\n"," [-0.00117145]\n"," [ 0.00175571]]\n","Iter:  3474 loss =  0.010017320253667694 learning rate =  0.5 update =  [[-0.00117111]\n"," [-0.00117111]\n"," [ 0.0017552 ]]\n","Iter:  3475 loss =  0.010014408811950568 learning rate =  0.5 update =  [[-0.00117077]\n"," [-0.00117077]\n"," [ 0.00175469]]\n","Iter:  3476 loss =  0.010011499051053172 learning rate =  0.5 update =  [[-0.00117043]\n"," [-0.00117043]\n"," [ 0.00175419]]\n","Iter:  3477 loss =  0.010008590969526669 learning rate =  0.5 update =  [[-0.0011701 ]\n"," [-0.0011701 ]\n"," [ 0.00175368]]\n","Iter:  3478 loss =  0.01000568456592385 learning rate =  0.5 update =  [[-0.00116976]\n"," [-0.00116976]\n"," [ 0.00175317]]\n","Iter:  3479 loss =  0.010002779838799148 learning rate =  0.5 update =  [[-0.00116942]\n"," [-0.00116942]\n"," [ 0.00175267]]\n","Iter:  3480 loss =  0.009999876786708776 learning rate =  0.5 update =  [[-0.00116908]\n"," [-0.00116908]\n"," [ 0.00175216]]\n","Iter:  3481 loss =  0.009996975408210334 learning rate =  0.5 update =  [[-0.00116875]\n"," [-0.00116875]\n"," [ 0.00175166]]\n","Iter:  3482 loss =  0.009994075701863364 learning rate =  0.5 update =  [[-0.00116841]\n"," [-0.00116841]\n"," [ 0.00175115]]\n","Iter:  3483 loss =  0.009991177666228939 learning rate =  0.5 update =  [[-0.00116807]\n"," [-0.00116807]\n"," [ 0.00175065]]\n","Iter:  3484 loss =  0.009988281299869766 learning rate =  0.5 update =  [[-0.00116774]\n"," [-0.00116774]\n"," [ 0.00175015]]\n","Iter:  3485 loss =  0.009985386601350194 learning rate =  0.5 update =  [[-0.0011674 ]\n"," [-0.0011674 ]\n"," [ 0.00174964]]\n","Iter:  3486 loss =  0.00998249356923615 learning rate =  0.5 update =  [[-0.00116706]\n"," [-0.00116706]\n"," [ 0.00174914]]\n","Iter:  3487 loss =  0.009979602202095343 learning rate =  0.5 update =  [[-0.00116673]\n"," [-0.00116673]\n"," [ 0.00174864]]\n","Iter:  3488 loss =  0.009976712498497086 learning rate =  0.5 update =  [[-0.00116639]\n"," [-0.00116639]\n"," [ 0.00174813]]\n","Iter:  3489 loss =  0.009973824457012169 learning rate =  0.5 update =  [[-0.00116606]\n"," [-0.00116606]\n"," [ 0.00174763]]\n","Iter:  3490 loss =  0.009970938076213264 learning rate =  0.5 update =  [[-0.00116572]\n"," [-0.00116572]\n"," [ 0.00174713]]\n","Iter:  3491 loss =  0.00996805335467435 learning rate =  0.5 update =  [[-0.00116539]\n"," [-0.00116539]\n"," [ 0.00174663]]\n","Iter:  3492 loss =  0.009965170290971352 learning rate =  0.5 update =  [[-0.00116505]\n"," [-0.00116505]\n"," [ 0.00174612]]\n","Iter:  3493 loss =  0.009962288883681687 learning rate =  0.5 update =  [[-0.00116472]\n"," [-0.00116472]\n"," [ 0.00174562]]\n","Iter:  3494 loss =  0.00995940913138436 learning rate =  0.5 update =  [[-0.00116438]\n"," [-0.00116438]\n"," [ 0.00174512]]\n","Iter:  3495 loss =  0.009956531032659977 learning rate =  0.5 update =  [[-0.00116405]\n"," [-0.00116405]\n"," [ 0.00174462]]\n","Iter:  3496 loss =  0.009953654586090903 learning rate =  0.5 update =  [[-0.00116371]\n"," [-0.00116371]\n"," [ 0.00174412]]\n","Iter:  3497 loss =  0.009950779790260992 learning rate =  0.5 update =  [[-0.00116338]\n"," [-0.00116338]\n"," [ 0.00174362]]\n","Iter:  3498 loss =  0.009947906643755824 learning rate =  0.5 update =  [[-0.00116305]\n"," [-0.00116305]\n"," [ 0.00174312]]\n","Iter:  3499 loss =  0.009945035145162278 learning rate =  0.5 update =  [[-0.00116271]\n"," [-0.00116271]\n"," [ 0.00174262]]\n","Iter:  3500 loss =  0.009942165293069346 learning rate =  0.5 update =  [[-0.00116238]\n"," [-0.00116238]\n"," [ 0.00174212]]\n","Iter:  3501 loss =  0.00993929708606715 learning rate =  0.5 update =  [[-0.00116205]\n"," [-0.00116205]\n"," [ 0.00174162]]\n","Iter:  3502 loss =  0.00993643052274779 learning rate =  0.5 update =  [[-0.00116171]\n"," [-0.00116171]\n"," [ 0.00174112]]\n","Iter:  3503 loss =  0.009933565601704693 learning rate =  0.5 update =  [[-0.00116138]\n"," [-0.00116138]\n"," [ 0.00174063]]\n","Iter:  3504 loss =  0.009930702321533006 learning rate =  0.5 update =  [[-0.00116105]\n"," [-0.00116105]\n"," [ 0.00174013]]\n","Iter:  3505 loss =  0.009927840680829428 learning rate =  0.5 update =  [[-0.00116071]\n"," [-0.00116071]\n"," [ 0.00173963]]\n","Iter:  3506 loss =  0.009924980678192348 learning rate =  0.5 update =  [[-0.00116038]\n"," [-0.00116038]\n"," [ 0.00173913]]\n","Iter:  3507 loss =  0.009922122312221657 learning rate =  0.5 update =  [[-0.00116005]\n"," [-0.00116005]\n"," [ 0.00173863]]\n","Iter:  3508 loss =  0.009919265581518823 learning rate =  0.5 update =  [[-0.00115972]\n"," [-0.00115972]\n"," [ 0.00173814]]\n","Iter:  3509 loss =  0.009916410484687036 learning rate =  0.5 update =  [[-0.00115939]\n"," [-0.00115939]\n"," [ 0.00173764]]\n","Iter:  3510 loss =  0.009913557020330838 learning rate =  0.5 update =  [[-0.00115905]\n"," [-0.00115905]\n"," [ 0.00173714]]\n","Iter:  3511 loss =  0.009910705187056525 learning rate =  0.5 update =  [[-0.00115872]\n"," [-0.00115872]\n"," [ 0.00173665]]\n","Iter:  3512 loss =  0.009907854983472108 learning rate =  0.5 update =  [[-0.00115839]\n"," [-0.00115839]\n"," [ 0.00173615]]\n","Iter:  3513 loss =  0.009905006408186732 learning rate =  0.5 update =  [[-0.00115806]\n"," [-0.00115806]\n"," [ 0.00173566]]\n","Iter:  3514 loss =  0.009902159459811627 learning rate =  0.5 update =  [[-0.00115773]\n"," [-0.00115773]\n"," [ 0.00173516]]\n","Iter:  3515 loss =  0.009899314136959235 learning rate =  0.5 update =  [[-0.0011574 ]\n"," [-0.0011574 ]\n"," [ 0.00173467]]\n","Iter:  3516 loss =  0.009896470438243809 learning rate =  0.5 update =  [[-0.00115707]\n"," [-0.00115707]\n"," [ 0.00173417]]\n","Iter:  3517 loss =  0.009893628362280863 learning rate =  0.5 update =  [[-0.00115674]\n"," [-0.00115674]\n"," [ 0.00173368]]\n","Iter:  3518 loss =  0.009890787907687864 learning rate =  0.5 update =  [[-0.00115641]\n"," [-0.00115641]\n"," [ 0.00173318]]\n","Iter:  3519 loss =  0.009887949073083688 learning rate =  0.5 update =  [[-0.00115608]\n"," [-0.00115608]\n"," [ 0.00173269]]\n","Iter:  3520 loss =  0.009885111857088565 learning rate =  0.5 update =  [[-0.00115575]\n"," [-0.00115575]\n"," [ 0.00173219]]\n","Iter:  3521 loss =  0.009882276258324638 learning rate =  0.5 update =  [[-0.00115542]\n"," [-0.00115542]\n"," [ 0.0017317 ]]\n","Iter:  3522 loss =  0.0098794422754153 learning rate =  0.5 update =  [[-0.00115509]\n"," [-0.00115509]\n"," [ 0.00173121]]\n","Iter:  3523 loss =  0.00987660990698574 learning rate =  0.5 update =  [[-0.00115476]\n"," [-0.00115476]\n"," [ 0.00173071]]\n","Iter:  3524 loss =  0.009873779151662593 learning rate =  0.5 update =  [[-0.00115443]\n"," [-0.00115443]\n"," [ 0.00173022]]\n","Iter:  3525 loss =  0.009870950008073955 learning rate =  0.5 update =  [[-0.0011541 ]\n"," [-0.0011541 ]\n"," [ 0.00172973]]\n","Iter:  3526 loss =  0.009868122474849698 learning rate =  0.5 update =  [[-0.00115377]\n"," [-0.00115377]\n"," [ 0.00172924]]\n","Iter:  3527 loss =  0.009865296550621027 learning rate =  0.5 update =  [[-0.00115345]\n"," [-0.00115345]\n"," [ 0.00172875]]\n","Iter:  3528 loss =  0.009862472234020833 learning rate =  0.5 update =  [[-0.00115312]\n"," [-0.00115312]\n"," [ 0.00172825]]\n","Iter:  3529 loss =  0.009859649523683497 learning rate =  0.5 update =  [[-0.00115279]\n"," [-0.00115279]\n"," [ 0.00172776]]\n","Iter:  3530 loss =  0.009856828418244824 learning rate =  0.5 update =  [[-0.00115246]\n"," [-0.00115246]\n"," [ 0.00172727]]\n","Iter:  3531 loss =  0.009854008916342465 learning rate =  0.5 update =  [[-0.00115213]\n"," [-0.00115213]\n"," [ 0.00172678]]\n","Iter:  3532 loss =  0.009851191016615232 learning rate =  0.5 update =  [[-0.00115181]\n"," [-0.00115181]\n"," [ 0.00172629]]\n","Iter:  3533 loss =  0.009848374717703858 learning rate =  0.5 update =  [[-0.00115148]\n"," [-0.00115148]\n"," [ 0.0017258 ]]\n","Iter:  3534 loss =  0.009845560018250231 learning rate =  0.5 update =  [[-0.00115115]\n"," [-0.00115115]\n"," [ 0.00172531]]\n","Iter:  3535 loss =  0.009842746916898064 learning rate =  0.5 update =  [[-0.00115083]\n"," [-0.00115083]\n"," [ 0.00172482]]\n","Iter:  3536 loss =  0.009839935412292488 learning rate =  0.5 update =  [[-0.0011505 ]\n"," [-0.0011505 ]\n"," [ 0.00172433]]\n","Iter:  3537 loss =  0.009837125503080014 learning rate =  0.5 update =  [[-0.00115017]\n"," [-0.00115017]\n"," [ 0.00172384]]\n","Iter:  3538 loss =  0.009834317187908951 learning rate =  0.5 update =  [[-0.00114985]\n"," [-0.00114985]\n"," [ 0.00172335]]\n","Iter:  3539 loss =  0.009831510465429027 learning rate =  0.5 update =  [[-0.00114952]\n"," [-0.00114952]\n"," [ 0.00172287]]\n","Iter:  3540 loss =  0.009828705334291313 learning rate =  0.5 update =  [[-0.00114919]\n"," [-0.00114919]\n"," [ 0.00172238]]\n","Iter:  3541 loss =  0.009825901793148635 learning rate =  0.5 update =  [[-0.00114887]\n"," [-0.00114887]\n"," [ 0.00172189]]\n","Iter:  3542 loss =  0.009823099840655252 learning rate =  0.5 update =  [[-0.00114854]\n"," [-0.00114854]\n"," [ 0.0017214 ]]\n","Iter:  3543 loss =  0.009820299475466976 learning rate =  0.5 update =  [[-0.00114822]\n"," [-0.00114822]\n"," [ 0.00172091]]\n","Iter:  3544 loss =  0.009817500696240971 learning rate =  0.5 update =  [[-0.00114789]\n"," [-0.00114789]\n"," [ 0.00172043]]\n","Iter:  3545 loss =  0.00981470350163606 learning rate =  0.5 update =  [[-0.00114757]\n"," [-0.00114757]\n"," [ 0.00171994]]\n","Iter:  3546 loss =  0.009811907890312567 learning rate =  0.5 update =  [[-0.00114724]\n"," [-0.00114724]\n"," [ 0.00171945]]\n","Iter:  3547 loss =  0.009809113860932243 learning rate =  0.5 update =  [[-0.00114692]\n"," [-0.00114692]\n"," [ 0.00171897]]\n","Iter:  3548 loss =  0.009806321412158429 learning rate =  0.5 update =  [[-0.00114659]\n"," [-0.00114659]\n"," [ 0.00171848]]\n","Iter:  3549 loss =  0.009803530542655951 learning rate =  0.5 update =  [[-0.00114627]\n"," [-0.00114627]\n"," [ 0.001718  ]]\n","Iter:  3550 loss =  0.009800741251091044 learning rate =  0.5 update =  [[-0.00114594]\n"," [-0.00114594]\n"," [ 0.00171751]]\n","Iter:  3551 loss =  0.00979795353613158 learning rate =  0.5 update =  [[-0.00114562]\n"," [-0.00114562]\n"," [ 0.00171703]]\n","Iter:  3552 loss =  0.009795167396446786 learning rate =  0.5 update =  [[-0.0011453 ]\n"," [-0.0011453 ]\n"," [ 0.00171654]]\n","Iter:  3553 loss =  0.009792382830707456 learning rate =  0.5 update =  [[-0.00114497]\n"," [-0.00114497]\n"," [ 0.00171606]]\n","Iter:  3554 loss =  0.00978959983758592 learning rate =  0.5 update =  [[-0.00114465]\n"," [-0.00114465]\n"," [ 0.00171557]]\n","Iter:  3555 loss =  0.009786818415755849 learning rate =  0.5 update =  [[-0.00114433]\n"," [-0.00114433]\n"," [ 0.00171509]]\n","Iter:  3556 loss =  0.009784038563892608 learning rate =  0.5 update =  [[-0.001144 ]\n"," [-0.001144 ]\n"," [ 0.0017146]]\n","Iter:  3557 loss =  0.009781260280672777 learning rate =  0.5 update =  [[-0.00114368]\n"," [-0.00114368]\n"," [ 0.00171412]]\n","Iter:  3558 loss =  0.009778483564774672 learning rate =  0.5 update =  [[-0.00114336]\n"," [-0.00114336]\n"," [ 0.00171364]]\n","Iter:  3559 loss =  0.009775708414878001 learning rate =  0.5 update =  [[-0.00114303]\n"," [-0.00114303]\n"," [ 0.00171315]]\n","Iter:  3560 loss =  0.009772934829663876 learning rate =  0.5 update =  [[-0.00114271]\n"," [-0.00114271]\n"," [ 0.00171267]]\n","Iter:  3561 loss =  0.009770162807815036 learning rate =  0.5 update =  [[-0.00114239]\n"," [-0.00114239]\n"," [ 0.00171219]]\n","Iter:  3562 loss =  0.0097673923480155 learning rate =  0.5 update =  [[-0.00114207]\n"," [-0.00114207]\n"," [ 0.00171171]]\n","Iter:  3563 loss =  0.009764623448950963 learning rate =  0.5 update =  [[-0.00114175]\n"," [-0.00114175]\n"," [ 0.00171122]]\n","Iter:  3564 loss =  0.0097618561093084 learning rate =  0.5 update =  [[-0.00114142]\n"," [-0.00114142]\n"," [ 0.00171074]]\n","Iter:  3565 loss =  0.009759090327776384 learning rate =  0.5 update =  [[-0.0011411 ]\n"," [-0.0011411 ]\n"," [ 0.00171026]]\n","Iter:  3566 loss =  0.00975632610304495 learning rate =  0.5 update =  [[-0.00114078]\n"," [-0.00114078]\n"," [ 0.00170978]]\n","Iter:  3567 loss =  0.009753563433805438 learning rate =  0.5 update =  [[-0.00114046]\n"," [-0.00114046]\n"," [ 0.0017093 ]]\n","Iter:  3568 loss =  0.009750802318750865 learning rate =  0.5 update =  [[-0.00114014]\n"," [-0.00114014]\n"," [ 0.00170882]]\n","Iter:  3569 loss =  0.009748042756575585 learning rate =  0.5 update =  [[-0.00113982]\n"," [-0.00113982]\n"," [ 0.00170834]]\n","Iter:  3570 loss =  0.009745284745975547 learning rate =  0.5 update =  [[-0.0011395 ]\n"," [-0.0011395 ]\n"," [ 0.00170786]]\n","Iter:  3571 loss =  0.009742528285647825 learning rate =  0.5 update =  [[-0.00113918]\n"," [-0.00113918]\n"," [ 0.00170738]]\n","Iter:  3572 loss =  0.009739773374291332 learning rate =  0.5 update =  [[-0.00113886]\n"," [-0.00113886]\n"," [ 0.0017069 ]]\n","Iter:  3573 loss =  0.009737020010606213 learning rate =  0.5 update =  [[-0.00113854]\n"," [-0.00113854]\n"," [ 0.00170642]]\n","Iter:  3574 loss =  0.009734268193294163 learning rate =  0.5 update =  [[-0.00113822]\n"," [-0.00113822]\n"," [ 0.00170594]]\n","Iter:  3575 loss =  0.009731517921058228 learning rate =  0.5 update =  [[-0.0011379 ]\n"," [-0.0011379 ]\n"," [ 0.00170546]]\n","Iter:  3576 loss =  0.009728769192602973 learning rate =  0.5 update =  [[-0.00113758]\n"," [-0.00113758]\n"," [ 0.00170498]]\n","Iter:  3577 loss =  0.009726022006634404 learning rate =  0.5 update =  [[-0.00113726]\n"," [-0.00113726]\n"," [ 0.00170451]]\n","Iter:  3578 loss =  0.009723276361859871 learning rate =  0.5 update =  [[-0.00113694]\n"," [-0.00113694]\n"," [ 0.00170403]]\n","Iter:  3579 loss =  0.00972053225698822 learning rate =  0.5 update =  [[-0.00113662]\n"," [-0.00113662]\n"," [ 0.00170355]]\n","Iter:  3580 loss =  0.009717789690729926 learning rate =  0.5 update =  [[-0.0011363 ]\n"," [-0.0011363 ]\n"," [ 0.00170307]]\n","Iter:  3581 loss =  0.009715048661796538 learning rate =  0.5 update =  [[-0.00113598]\n"," [-0.00113598]\n"," [ 0.0017026 ]]\n","Iter:  3582 loss =  0.009712309168901376 learning rate =  0.5 update =  [[-0.00113567]\n"," [-0.00113567]\n"," [ 0.00170212]]\n","Iter:  3583 loss =  0.00970957121075897 learning rate =  0.5 update =  [[-0.00113535]\n"," [-0.00113535]\n"," [ 0.00170164]]\n","Iter:  3584 loss =  0.009706834786085312 learning rate =  0.5 update =  [[-0.00113503]\n"," [-0.00113503]\n"," [ 0.00170117]]\n","Iter:  3585 loss =  0.00970409989359797 learning rate =  0.5 update =  [[-0.00113471]\n"," [-0.00113471]\n"," [ 0.00170069]]\n","Iter:  3586 loss =  0.00970136653201567 learning rate =  0.5 update =  [[-0.00113439]\n"," [-0.00113439]\n"," [ 0.00170021]]\n","Iter:  3587 loss =  0.009698634700058908 learning rate =  0.5 update =  [[-0.00113408]\n"," [-0.00113408]\n"," [ 0.00169974]]\n","Iter:  3588 loss =  0.009695904396449247 learning rate =  0.5 update =  [[-0.00113376]\n"," [-0.00113376]\n"," [ 0.00169926]]\n","Iter:  3589 loss =  0.009693175619909993 learning rate =  0.5 update =  [[-0.00113344]\n"," [-0.00113344]\n"," [ 0.00169879]]\n","Iter:  3590 loss =  0.009690448369165532 learning rate =  0.5 update =  [[-0.00113312]\n"," [-0.00113312]\n"," [ 0.00169831]]\n","Iter:  3591 loss =  0.009687722642942036 learning rate =  0.5 update =  [[-0.00113281]\n"," [-0.00113281]\n"," [ 0.00169784]]\n","Iter:  3592 loss =  0.009684998439966785 learning rate =  0.5 update =  [[-0.00113249]\n"," [-0.00113249]\n"," [ 0.00169737]]\n","Iter:  3593 loss =  0.009682275758968576 learning rate =  0.5 update =  [[-0.00113217]\n"," [-0.00113217]\n"," [ 0.00169689]]\n","Iter:  3594 loss =  0.009679554598677661 learning rate =  0.5 update =  [[-0.00113186]\n"," [-0.00113186]\n"," [ 0.00169642]]\n","Iter:  3595 loss =  0.00967683495782571 learning rate =  0.5 update =  [[-0.00113154]\n"," [-0.00113154]\n"," [ 0.00169594]]\n","Iter:  3596 loss =  0.009674116835145623 learning rate =  0.5 update =  [[-0.00113123]\n"," [-0.00113123]\n"," [ 0.00169547]]\n","Iter:  3597 loss =  0.00967140022937199 learning rate =  0.5 update =  [[-0.00113091]\n"," [-0.00113091]\n"," [ 0.001695  ]]\n","Iter:  3598 loss =  0.009668685139240556 learning rate =  0.5 update =  [[-0.0011306 ]\n"," [-0.0011306 ]\n"," [ 0.00169453]]\n","Iter:  3599 loss =  0.009665971563488583 learning rate =  0.5 update =  [[-0.00113028]\n"," [-0.00113028]\n"," [ 0.00169405]]\n","Iter:  3600 loss =  0.009663259500854626 learning rate =  0.5 update =  [[-0.00112996]\n"," [-0.00112996]\n"," [ 0.00169358]]\n","Iter:  3601 loss =  0.009660548950078899 learning rate =  0.5 update =  [[-0.00112965]\n"," [-0.00112965]\n"," [ 0.00169311]]\n","Iter:  3602 loss =  0.009657839909902714 learning rate =  0.5 update =  [[-0.00112933]\n"," [-0.00112933]\n"," [ 0.00169264]]\n","Iter:  3603 loss =  0.00965513237906879 learning rate =  0.5 update =  [[-0.00112902]\n"," [-0.00112902]\n"," [ 0.00169217]]\n","Iter:  3604 loss =  0.009652426356321504 learning rate =  0.5 update =  [[-0.00112871]\n"," [-0.00112871]\n"," [ 0.0016917 ]]\n","Iter:  3605 loss =  0.009649721840406387 learning rate =  0.5 update =  [[-0.00112839]\n"," [-0.00112839]\n"," [ 0.00169122]]\n","Iter:  3606 loss =  0.009647018830070315 learning rate =  0.5 update =  [[-0.00112808]\n"," [-0.00112808]\n"," [ 0.00169075]]\n","Iter:  3607 loss =  0.009644317324061832 learning rate =  0.5 update =  [[-0.00112776]\n"," [-0.00112776]\n"," [ 0.00169028]]\n","Iter:  3608 loss =  0.009641617321130624 learning rate =  0.5 update =  [[-0.00112745]\n"," [-0.00112745]\n"," [ 0.00168981]]\n","Iter:  3609 loss =  0.009638918820027792 learning rate =  0.5 update =  [[-0.00112714]\n"," [-0.00112714]\n"," [ 0.00168934]]\n","Iter:  3610 loss =  0.009636221819505837 learning rate =  0.5 update =  [[-0.00112682]\n"," [-0.00112682]\n"," [ 0.00168887]]\n","Iter:  3611 loss =  0.009633526318318698 learning rate =  0.5 update =  [[-0.00112651]\n"," [-0.00112651]\n"," [ 0.00168841]]\n","Iter:  3612 loss =  0.009630832315221601 learning rate =  0.5 update =  [[-0.0011262 ]\n"," [-0.0011262 ]\n"," [ 0.00168794]]\n","Iter:  3613 loss =  0.009628139808971186 learning rate =  0.5 update =  [[-0.00112588]\n"," [-0.00112588]\n"," [ 0.00168747]]\n","Iter:  3614 loss =  0.009625448798325434 learning rate =  0.5 update =  [[-0.00112557]\n"," [-0.00112557]\n"," [ 0.001687  ]]\n","Iter:  3615 loss =  0.009622759282043765 learning rate =  0.5 update =  [[-0.00112526]\n"," [-0.00112526]\n"," [ 0.00168653]]\n","Iter:  3616 loss =  0.009620071258886916 learning rate =  0.5 update =  [[-0.00112494]\n"," [-0.00112494]\n"," [ 0.00168606]]\n","Iter:  3617 loss =  0.009617384727616925 learning rate =  0.5 update =  [[-0.00112463]\n"," [-0.00112463]\n"," [ 0.0016856 ]]\n","Iter:  3618 loss =  0.009614699686997335 learning rate =  0.5 update =  [[-0.00112432]\n"," [-0.00112432]\n"," [ 0.00168513]]\n","Iter:  3619 loss =  0.009612016135792992 learning rate =  0.5 update =  [[-0.00112401]\n"," [-0.00112401]\n"," [ 0.00168466]]\n","Iter:  3620 loss =  0.009609334072770006 learning rate =  0.5 update =  [[-0.0011237 ]\n"," [-0.0011237 ]\n"," [ 0.00168419]]\n","Iter:  3621 loss =  0.009606653496696056 learning rate =  0.5 update =  [[-0.00112338]\n"," [-0.00112338]\n"," [ 0.00168373]]\n","Iter:  3622 loss =  0.009603974406339946 learning rate =  0.5 update =  [[-0.00112307]\n"," [-0.00112307]\n"," [ 0.00168326]]\n","Iter:  3623 loss =  0.009601296800471998 learning rate =  0.5 update =  [[-0.00112276]\n"," [-0.00112276]\n"," [ 0.00168279]]\n","Iter:  3624 loss =  0.009598620677863802 learning rate =  0.5 update =  [[-0.00112245]\n"," [-0.00112245]\n"," [ 0.00168233]]\n","Iter:  3625 loss =  0.009595946037288295 learning rate =  0.5 update =  [[-0.00112214]\n"," [-0.00112214]\n"," [ 0.00168186]]\n","Iter:  3626 loss =  0.009593272877519815 learning rate =  0.5 update =  [[-0.00112183]\n"," [-0.00112183]\n"," [ 0.0016814 ]]\n","Iter:  3627 loss =  0.009590601197334053 learning rate =  0.5 update =  [[-0.00112152]\n"," [-0.00112152]\n"," [ 0.00168093]]\n","Iter:  3628 loss =  0.00958793099550799 learning rate =  0.5 update =  [[-0.00112121]\n"," [-0.00112121]\n"," [ 0.00168047]]\n","Iter:  3629 loss =  0.009585262270819985 learning rate =  0.5 update =  [[-0.0011209]\n"," [-0.0011209]\n"," [ 0.00168  ]]\n","Iter:  3630 loss =  0.009582595022049717 learning rate =  0.5 update =  [[-0.00112059]\n"," [-0.00112059]\n"," [ 0.00167954]]\n","Iter:  3631 loss =  0.009579929247978184 learning rate =  0.5 update =  [[-0.00112028]\n"," [-0.00112028]\n"," [ 0.00167908]]\n","Iter:  3632 loss =  0.009577264947387906 learning rate =  0.5 update =  [[-0.00111997]\n"," [-0.00111997]\n"," [ 0.00167861]]\n","Iter:  3633 loss =  0.009574602119062404 learning rate =  0.5 update =  [[-0.00111966]\n"," [-0.00111966]\n"," [ 0.00167815]]\n","Iter:  3634 loss =  0.009571940761786812 learning rate =  0.5 update =  [[-0.00111935]\n"," [-0.00111935]\n"," [ 0.00167768]]\n","Iter:  3635 loss =  0.009569280874347462 learning rate =  0.5 update =  [[-0.00111904]\n"," [-0.00111904]\n"," [ 0.00167722]]\n","Iter:  3636 loss =  0.00956662245553207 learning rate =  0.5 update =  [[-0.00111873]\n"," [-0.00111873]\n"," [ 0.00167676]]\n","Iter:  3637 loss =  0.009563965504129808 learning rate =  0.5 update =  [[-0.00111842]\n"," [-0.00111842]\n"," [ 0.0016763 ]]\n","Iter:  3638 loss =  0.009561310018930777 learning rate =  0.5 update =  [[-0.00111811]\n"," [-0.00111811]\n"," [ 0.00167583]]\n","Iter:  3639 loss =  0.009558655998726788 learning rate =  0.5 update =  [[-0.0011178 ]\n"," [-0.0011178 ]\n"," [ 0.00167537]]\n","Iter:  3640 loss =  0.009556003442310945 learning rate =  0.5 update =  [[-0.0011175 ]\n"," [-0.0011175 ]\n"," [ 0.00167491]]\n","Iter:  3641 loss =  0.009553352348477392 learning rate =  0.5 update =  [[-0.00111719]\n"," [-0.00111719]\n"," [ 0.00167445]]\n","Iter:  3642 loss =  0.009550702716021875 learning rate =  0.5 update =  [[-0.00111688]\n"," [-0.00111688]\n"," [ 0.00167399]]\n","Iter:  3643 loss =  0.0095480545437414 learning rate =  0.5 update =  [[-0.00111657]\n"," [-0.00111657]\n"," [ 0.00167353]]\n","Iter:  3644 loss =  0.009545407830434158 learning rate =  0.5 update =  [[-0.00111626]\n"," [-0.00111626]\n"," [ 0.00167307]]\n","Iter:  3645 loss =  0.00954276257489976 learning rate =  0.5 update =  [[-0.00111596]\n"," [-0.00111596]\n"," [ 0.0016726 ]]\n","Iter:  3646 loss =  0.009540118775939209 learning rate =  0.5 update =  [[-0.00111565]\n"," [-0.00111565]\n"," [ 0.00167214]]\n","Iter:  3647 loss =  0.009537476432354619 learning rate =  0.5 update =  [[-0.00111534]\n"," [-0.00111534]\n"," [ 0.00167168]]\n","Iter:  3648 loss =  0.009534835542949595 learning rate =  0.5 update =  [[-0.00111504]\n"," [-0.00111504]\n"," [ 0.00167122]]\n","Iter:  3649 loss =  0.009532196106528869 learning rate =  0.5 update =  [[-0.00111473]\n"," [-0.00111473]\n"," [ 0.00167076]]\n","Iter:  3650 loss =  0.00952955812189871 learning rate =  0.5 update =  [[-0.00111442]\n"," [-0.00111442]\n"," [ 0.00167031]]\n","Iter:  3651 loss =  0.009526921587866433 learning rate =  0.5 update =  [[-0.00111412]\n"," [-0.00111412]\n"," [ 0.00166985]]\n","Iter:  3652 loss =  0.009524286503240918 learning rate =  0.5 update =  [[-0.00111381]\n"," [-0.00111381]\n"," [ 0.00166939]]\n","Iter:  3653 loss =  0.009521652866832115 learning rate =  0.5 update =  [[-0.0011135 ]\n"," [-0.0011135 ]\n"," [ 0.00166893]]\n","Iter:  3654 loss =  0.00951902067745138 learning rate =  0.5 update =  [[-0.0011132 ]\n"," [-0.0011132 ]\n"," [ 0.00166847]]\n","Iter:  3655 loss =  0.009516389933911407 learning rate =  0.5 update =  [[-0.00111289]\n"," [-0.00111289]\n"," [ 0.00166801]]\n","Iter:  3656 loss =  0.00951376063502608 learning rate =  0.5 update =  [[-0.00111259]\n"," [-0.00111259]\n"," [ 0.00166756]]\n","Iter:  3657 loss =  0.009511132779610625 learning rate =  0.5 update =  [[-0.00111228]\n"," [-0.00111228]\n"," [ 0.0016671 ]]\n","Iter:  3658 loss =  0.009508506366481588 learning rate =  0.5 update =  [[-0.00111197]\n"," [-0.00111197]\n"," [ 0.00166664]]\n","Iter:  3659 loss =  0.009505881394456801 learning rate =  0.5 update =  [[-0.00111167]\n"," [-0.00111167]\n"," [ 0.00166618]]\n","Iter:  3660 loss =  0.009503257862355338 learning rate =  0.5 update =  [[-0.00111136]\n"," [-0.00111136]\n"," [ 0.00166573]]\n","Iter:  3661 loss =  0.00950063576899756 learning rate =  0.5 update =  [[-0.00111106]\n"," [-0.00111106]\n"," [ 0.00166527]]\n","Iter:  3662 loss =  0.009498015113205145 learning rate =  0.5 update =  [[-0.00111075]\n"," [-0.00111075]\n"," [ 0.00166481]]\n","Iter:  3663 loss =  0.00949539589380101 learning rate =  0.5 update =  [[-0.00111045]\n"," [-0.00111045]\n"," [ 0.00166436]]\n","Iter:  3664 loss =  0.009492778109609475 learning rate =  0.5 update =  [[-0.00111015]\n"," [-0.00111015]\n"," [ 0.0016639 ]]\n","Iter:  3665 loss =  0.009490161759456 learning rate =  0.5 update =  [[-0.00110984]\n"," [-0.00110984]\n"," [ 0.00166345]]\n","Iter:  3666 loss =  0.009487546842167379 learning rate =  0.5 update =  [[-0.00110954]\n"," [-0.00110954]\n"," [ 0.00166299]]\n","Iter:  3667 loss =  0.009484933356571655 learning rate =  0.5 update =  [[-0.00110923]\n"," [-0.00110923]\n"," [ 0.00166254]]\n","Iter:  3668 loss =  0.009482321301498144 learning rate =  0.5 update =  [[-0.00110893]\n"," [-0.00110893]\n"," [ 0.00166208]]\n","Iter:  3669 loss =  0.00947971067577753 learning rate =  0.5 update =  [[-0.00110863]\n"," [-0.00110863]\n"," [ 0.00166163]]\n","Iter:  3670 loss =  0.00947710147824165 learning rate =  0.5 update =  [[-0.00110832]\n"," [-0.00110832]\n"," [ 0.00166117]]\n","Iter:  3671 loss =  0.009474493707723571 learning rate =  0.5 update =  [[-0.00110802]\n"," [-0.00110802]\n"," [ 0.00166072]]\n","Iter:  3672 loss =  0.009471887363057877 learning rate =  0.5 update =  [[-0.00110772]\n"," [-0.00110772]\n"," [ 0.00166026]]\n","Iter:  3673 loss =  0.009469282443080082 learning rate =  0.5 update =  [[-0.00110741]\n"," [-0.00110741]\n"," [ 0.00165981]]\n","Iter:  3674 loss =  0.009466678946627238 learning rate =  0.5 update =  [[-0.00110711]\n"," [-0.00110711]\n"," [ 0.00165936]]\n","Iter:  3675 loss =  0.009464076872537513 learning rate =  0.5 update =  [[-0.00110681]\n"," [-0.00110681]\n"," [ 0.0016589 ]]\n","Iter:  3676 loss =  0.009461476219650324 learning rate =  0.5 update =  [[-0.00110651]\n"," [-0.00110651]\n"," [ 0.00165845]]\n","Iter:  3677 loss =  0.009458876986806503 learning rate =  0.5 update =  [[-0.0011062]\n"," [-0.0011062]\n"," [ 0.001658 ]]\n","Iter:  3678 loss =  0.009456279172847987 learning rate =  0.5 update =  [[-0.0011059 ]\n"," [-0.0011059 ]\n"," [ 0.00165755]]\n","Iter:  3679 loss =  0.009453682776617938 learning rate =  0.5 update =  [[-0.0011056 ]\n"," [-0.0011056 ]\n"," [ 0.00165709]]\n","Iter:  3680 loss =  0.009451087796960899 learning rate =  0.5 update =  [[-0.0011053 ]\n"," [-0.0011053 ]\n"," [ 0.00165664]]\n","Iter:  3681 loss =  0.00944849423272264 learning rate =  0.5 update =  [[-0.001105  ]\n"," [-0.001105  ]\n"," [ 0.00165619]]\n","Iter:  3682 loss =  0.009445902082750056 learning rate =  0.5 update =  [[-0.0011047 ]\n"," [-0.0011047 ]\n"," [ 0.00165574]]\n","Iter:  3683 loss =  0.009443311345891478 learning rate =  0.5 update =  [[-0.00110439]\n"," [-0.00110439]\n"," [ 0.00165529]]\n","Iter:  3684 loss =  0.009440722020996427 learning rate =  0.5 update =  [[-0.00110409]\n"," [-0.00110409]\n"," [ 0.00165484]]\n","Iter:  3685 loss =  0.009438134106915505 learning rate =  0.5 update =  [[-0.00110379]\n"," [-0.00110379]\n"," [ 0.00165439]]\n","Iter:  3686 loss =  0.009435547602500764 learning rate =  0.5 update =  [[-0.00110349]\n"," [-0.00110349]\n"," [ 0.00165394]]\n","Iter:  3687 loss =  0.009432962506605375 learning rate =  0.5 update =  [[-0.00110319]\n"," [-0.00110319]\n"," [ 0.00165349]]\n","Iter:  3688 loss =  0.009430378818083809 learning rate =  0.5 update =  [[-0.00110289]\n"," [-0.00110289]\n"," [ 0.00165304]]\n","Iter:  3689 loss =  0.009427796535791753 learning rate =  0.5 update =  [[-0.00110259]\n"," [-0.00110259]\n"," [ 0.00165259]]\n","Iter:  3690 loss =  0.009425215658586148 learning rate =  0.5 update =  [[-0.00110229]\n"," [-0.00110229]\n"," [ 0.00165214]]\n","Iter:  3691 loss =  0.00942263618532518 learning rate =  0.5 update =  [[-0.00110199]\n"," [-0.00110199]\n"," [ 0.00165169]]\n","Iter:  3692 loss =  0.00942005811486822 learning rate =  0.5 update =  [[-0.00110169]\n"," [-0.00110169]\n"," [ 0.00165124]]\n","Iter:  3693 loss =  0.009417481446075921 learning rate =  0.5 update =  [[-0.00110139]\n"," [-0.00110139]\n"," [ 0.00165079]]\n","Iter:  3694 loss =  0.00941490617781001 learning rate =  0.5 update =  [[-0.00110109]\n"," [-0.00110109]\n"," [ 0.00165034]]\n","Iter:  3695 loss =  0.009412332308933725 learning rate =  0.5 update =  [[-0.00110079]\n"," [-0.00110079]\n"," [ 0.00164989]]\n","Iter:  3696 loss =  0.009409759838311335 learning rate =  0.5 update =  [[-0.00110049]\n"," [-0.00110049]\n"," [ 0.00164945]]\n","Iter:  3697 loss =  0.009407188764808345 learning rate =  0.5 update =  [[-0.00110019]\n"," [-0.00110019]\n"," [ 0.001649  ]]\n","Iter:  3698 loss =  0.009404619087291611 learning rate =  0.5 update =  [[-0.0010999 ]\n"," [-0.0010999 ]\n"," [ 0.00164855]]\n","Iter:  3699 loss =  0.009402050804628932 learning rate =  0.5 update =  [[-0.0010996]\n"," [-0.0010996]\n"," [ 0.0016481]]\n","Iter:  3700 loss =  0.009399483915689708 learning rate =  0.5 update =  [[-0.0010993 ]\n"," [-0.0010993 ]\n"," [ 0.00164766]]\n","Iter:  3701 loss =  0.009396918419344263 learning rate =  0.5 update =  [[-0.001099  ]\n"," [-0.001099  ]\n"," [ 0.00164721]]\n","Iter:  3702 loss =  0.009394354314464186 learning rate =  0.5 update =  [[-0.0010987 ]\n"," [-0.0010987 ]\n"," [ 0.00164676]]\n","Iter:  3703 loss =  0.009391791599922407 learning rate =  0.5 update =  [[-0.0010984 ]\n"," [-0.0010984 ]\n"," [ 0.00164632]]\n","Iter:  3704 loss =  0.009389230274593001 learning rate =  0.5 update =  [[-0.00109811]\n"," [-0.00109811]\n"," [ 0.00164587]]\n","Iter:  3705 loss =  0.009386670337351171 learning rate =  0.5 update =  [[-0.00109781]\n"," [-0.00109781]\n"," [ 0.00164543]]\n","Iter:  3706 loss =  0.009384111787073483 learning rate =  0.5 update =  [[-0.00109751]\n"," [-0.00109751]\n"," [ 0.00164498]]\n","Iter:  3707 loss =  0.009381554622637525 learning rate =  0.5 update =  [[-0.00109721]\n"," [-0.00109721]\n"," [ 0.00164453]]\n","Iter:  3708 loss =  0.0093789988429223 learning rate =  0.5 update =  [[-0.00109692]\n"," [-0.00109692]\n"," [ 0.00164409]]\n","Iter:  3709 loss =  0.00937644444680788 learning rate =  0.5 update =  [[-0.00109662]\n"," [-0.00109662]\n"," [ 0.00164365]]\n","Iter:  3710 loss =  0.009373891433175657 learning rate =  0.5 update =  [[-0.00109632]\n"," [-0.00109632]\n"," [ 0.0016432 ]]\n","Iter:  3711 loss =  0.009371339800908027 learning rate =  0.5 update =  [[-0.00109603]\n"," [-0.00109603]\n"," [ 0.00164276]]\n","Iter:  3712 loss =  0.009368789548888645 learning rate =  0.5 update =  [[-0.00109573]\n"," [-0.00109573]\n"," [ 0.00164231]]\n","Iter:  3713 loss =  0.009366240676002599 learning rate =  0.5 update =  [[-0.00109543]\n"," [-0.00109543]\n"," [ 0.00164187]]\n","Iter:  3714 loss =  0.009363693181135928 learning rate =  0.5 update =  [[-0.00109514]\n"," [-0.00109514]\n"," [ 0.00164142]]\n","Iter:  3715 loss =  0.009361147063175842 learning rate =  0.5 update =  [[-0.00109484]\n"," [-0.00109484]\n"," [ 0.00164098]]\n","Iter:  3716 loss =  0.00935860232101101 learning rate =  0.5 update =  [[-0.00109454]\n"," [-0.00109454]\n"," [ 0.00164054]]\n","Iter:  3717 loss =  0.009356058953531058 learning rate =  0.5 update =  [[-0.00109425]\n"," [-0.00109425]\n"," [ 0.0016401 ]]\n","Iter:  3718 loss =  0.009353516959626777 learning rate =  0.5 update =  [[-0.00109395]\n"," [-0.00109395]\n"," [ 0.00163965]]\n","Iter:  3719 loss =  0.00935097633819037 learning rate =  0.5 update =  [[-0.00109366]\n"," [-0.00109366]\n"," [ 0.00163921]]\n","Iter:  3720 loss =  0.009348437088115011 learning rate =  0.5 update =  [[-0.00109336]\n"," [-0.00109336]\n"," [ 0.00163877]]\n","Iter:  3721 loss =  0.009345899208295193 learning rate =  0.5 update =  [[-0.00109307]\n"," [-0.00109307]\n"," [ 0.00163833]]\n","Iter:  3722 loss =  0.009343362697626527 learning rate =  0.5 update =  [[-0.00109277]\n"," [-0.00109277]\n"," [ 0.00163788]]\n","Iter:  3723 loss =  0.009340827555005803 learning rate =  0.5 update =  [[-0.00109248]\n"," [-0.00109248]\n"," [ 0.00163744]]\n","Iter:  3724 loss =  0.009338293779331038 learning rate =  0.5 update =  [[-0.00109218]\n"," [-0.00109218]\n"," [ 0.001637  ]]\n","Iter:  3725 loss =  0.009335761369501457 learning rate =  0.5 update =  [[-0.00109189]\n"," [-0.00109189]\n"," [ 0.00163656]]\n","Iter:  3726 loss =  0.009333230324417317 learning rate =  0.5 update =  [[-0.00109159]\n"," [-0.00109159]\n"," [ 0.00163612]]\n","Iter:  3727 loss =  0.009330700642980222 learning rate =  0.5 update =  [[-0.0010913 ]\n"," [-0.0010913 ]\n"," [ 0.00163568]]\n","Iter:  3728 loss =  0.009328172324092809 learning rate =  0.5 update =  [[-0.00109101]\n"," [-0.00109101]\n"," [ 0.00163524]]\n","Iter:  3729 loss =  0.009325645366658917 learning rate =  0.5 update =  [[-0.00109071]\n"," [-0.00109071]\n"," [ 0.0016348 ]]\n","Iter:  3730 loss =  0.009323119769583789 learning rate =  0.5 update =  [[-0.00109042]\n"," [-0.00109042]\n"," [ 0.00163436]]\n","Iter:  3731 loss =  0.0093205955317735 learning rate =  0.5 update =  [[-0.00109013]\n"," [-0.00109013]\n"," [ 0.00163392]]\n","Iter:  3732 loss =  0.009318072652135419 learning rate =  0.5 update =  [[-0.00108983]\n"," [-0.00108983]\n"," [ 0.00163348]]\n","Iter:  3733 loss =  0.009315551129578143 learning rate =  0.5 update =  [[-0.00108954]\n"," [-0.00108954]\n"," [ 0.00163304]]\n","Iter:  3734 loss =  0.009313030963011419 learning rate =  0.5 update =  [[-0.00108925]\n"," [-0.00108925]\n"," [ 0.0016326 ]]\n","Iter:  3735 loss =  0.009310512151346107 learning rate =  0.5 update =  [[-0.00108895]\n"," [-0.00108895]\n"," [ 0.00163216]]\n","Iter:  3736 loss =  0.00930799469349425 learning rate =  0.5 update =  [[-0.00108866]\n"," [-0.00108866]\n"," [ 0.00163172]]\n","Iter:  3737 loss =  0.00930547858836906 learning rate =  0.5 update =  [[-0.00108837]\n"," [-0.00108837]\n"," [ 0.00163129]]\n","Iter:  3738 loss =  0.009302963834884841 learning rate =  0.5 update =  [[-0.00108808]\n"," [-0.00108807]\n"," [ 0.00163085]]\n","Iter:  3739 loss =  0.00930045043195727 learning rate =  0.5 update =  [[-0.00108778]\n"," [-0.00108778]\n"," [ 0.00163041]]\n","Iter:  3740 loss =  0.009297938378502917 learning rate =  0.5 update =  [[-0.00108749]\n"," [-0.00108749]\n"," [ 0.00162997]]\n","Iter:  3741 loss =  0.009295427673439612 learning rate =  0.5 update =  [[-0.0010872 ]\n"," [-0.0010872 ]\n"," [ 0.00162954]]\n","Iter:  3742 loss =  0.00929291831568644 learning rate =  0.5 update =  [[-0.00108691]\n"," [-0.00108691]\n"," [ 0.0016291 ]]\n","Iter:  3743 loss =  0.0092904103041635 learning rate =  0.5 update =  [[-0.00108662]\n"," [-0.00108662]\n"," [ 0.00162866]]\n","Iter:  3744 loss =  0.009287903637792033 learning rate =  0.5 update =  [[-0.00108632]\n"," [-0.00108632]\n"," [ 0.00162823]]\n","Iter:  3745 loss =  0.009285398315494481 learning rate =  0.5 update =  [[-0.00108603]\n"," [-0.00108603]\n"," [ 0.00162779]]\n","Iter:  3746 loss =  0.009282894336194437 learning rate =  0.5 update =  [[-0.00108574]\n"," [-0.00108574]\n"," [ 0.00162735]]\n","Iter:  3747 loss =  0.009280391698816663 learning rate =  0.5 update =  [[-0.00108545]\n"," [-0.00108545]\n"," [ 0.00162692]]\n","Iter:  3748 loss =  0.009277890402287102 learning rate =  0.5 update =  [[-0.00108516]\n"," [-0.00108516]\n"," [ 0.00162648]]\n","Iter:  3749 loss =  0.00927539044553267 learning rate =  0.5 update =  [[-0.00108487]\n"," [-0.00108487]\n"," [ 0.00162605]]\n","Iter:  3750 loss =  0.009272891827481599 learning rate =  0.5 update =  [[-0.00108458]\n"," [-0.00108458]\n"," [ 0.00162561]]\n","Iter:  3751 loss =  0.009270394547063132 learning rate =  0.5 update =  [[-0.00108429]\n"," [-0.00108429]\n"," [ 0.00162518]]\n","Iter:  3752 loss =  0.009267898603207762 learning rate =  0.5 update =  [[-0.001084  ]\n"," [-0.001084  ]\n"," [ 0.00162474]]\n","Iter:  3753 loss =  0.009265403994847021 learning rate =  0.5 update =  [[-0.00108371]\n"," [-0.00108371]\n"," [ 0.00162431]]\n","Iter:  3754 loss =  0.009262910720913644 learning rate =  0.5 update =  [[-0.00108342]\n"," [-0.00108342]\n"," [ 0.00162387]]\n","Iter:  3755 loss =  0.009260418780341428 learning rate =  0.5 update =  [[-0.00108313]\n"," [-0.00108313]\n"," [ 0.00162344]]\n","Iter:  3756 loss =  0.009257928172065453 learning rate =  0.5 update =  [[-0.00108284]\n"," [-0.00108284]\n"," [ 0.001623  ]]\n","Iter:  3757 loss =  0.009255438895021778 learning rate =  0.5 update =  [[-0.00108255]\n"," [-0.00108255]\n"," [ 0.00162257]]\n","Iter:  3758 loss =  0.009252950948147555 learning rate =  0.5 update =  [[-0.00108226]\n"," [-0.00108226]\n"," [ 0.00162214]]\n","Iter:  3759 loss =  0.009250464330381219 learning rate =  0.5 update =  [[-0.00108197]\n"," [-0.00108197]\n"," [ 0.0016217 ]]\n","Iter:  3760 loss =  0.009247979040662296 learning rate =  0.5 update =  [[-0.00108168]\n"," [-0.00108168]\n"," [ 0.00162127]]\n","Iter:  3761 loss =  0.009245495077931463 learning rate =  0.5 update =  [[-0.00108139]\n"," [-0.00108139]\n"," [ 0.00162084]]\n","Iter:  3762 loss =  0.009243012441130249 learning rate =  0.5 update =  [[-0.0010811 ]\n"," [-0.0010811 ]\n"," [ 0.00162041]]\n","Iter:  3763 loss =  0.009240531129201595 learning rate =  0.5 update =  [[-0.00108081]\n"," [-0.00108081]\n"," [ 0.00161997]]\n","Iter:  3764 loss =  0.009238051141089637 learning rate =  0.5 update =  [[-0.00108053]\n"," [-0.00108053]\n"," [ 0.00161954]]\n","Iter:  3765 loss =  0.00923557247573924 learning rate =  0.5 update =  [[-0.00108024]\n"," [-0.00108024]\n"," [ 0.00161911]]\n","Iter:  3766 loss =  0.009233095132096775 learning rate =  0.5 update =  [[-0.00107995]\n"," [-0.00107995]\n"," [ 0.00161868]]\n","Iter:  3767 loss =  0.009230619109109536 learning rate =  0.5 update =  [[-0.00107966]\n"," [-0.00107966]\n"," [ 0.00161825]]\n","Iter:  3768 loss =  0.009228144405725938 learning rate =  0.5 update =  [[-0.00107937]\n"," [-0.00107937]\n"," [ 0.00161782]]\n","Iter:  3769 loss =  0.00922567102089559 learning rate =  0.5 update =  [[-0.00107909]\n"," [-0.00107909]\n"," [ 0.00161739]]\n","Iter:  3770 loss =  0.009223198953569114 learning rate =  0.5 update =  [[-0.0010788 ]\n"," [-0.0010788 ]\n"," [ 0.00161696]]\n","Iter:  3771 loss =  0.009220728202698331 learning rate =  0.5 update =  [[-0.00107851]\n"," [-0.00107851]\n"," [ 0.00161653]]\n","Iter:  3772 loss =  0.009218258767236068 learning rate =  0.5 update =  [[-0.00107822]\n"," [-0.00107822]\n"," [ 0.0016161 ]]\n","Iter:  3773 loss =  0.009215790646136377 learning rate =  0.5 update =  [[-0.00107794]\n"," [-0.00107794]\n"," [ 0.00161567]]\n","Iter:  3774 loss =  0.009213323838354376 learning rate =  0.5 update =  [[-0.00107765]\n"," [-0.00107765]\n"," [ 0.00161524]]\n","Iter:  3775 loss =  0.009210858342846219 learning rate =  0.5 update =  [[-0.00107736]\n"," [-0.00107736]\n"," [ 0.00161481]]\n","Iter:  3776 loss =  0.009208394158569194 learning rate =  0.5 update =  [[-0.00107708]\n"," [-0.00107708]\n"," [ 0.00161438]]\n","Iter:  3777 loss =  0.009205931284481748 learning rate =  0.5 update =  [[-0.00107679]\n"," [-0.00107679]\n"," [ 0.00161395]]\n","Iter:  3778 loss =  0.009203469719543406 learning rate =  0.5 update =  [[-0.0010765 ]\n"," [-0.0010765 ]\n"," [ 0.00161352]]\n","Iter:  3779 loss =  0.009201009462714708 learning rate =  0.5 update =  [[-0.00107622]\n"," [-0.00107622]\n"," [ 0.00161309]]\n","Iter:  3780 loss =  0.00919855051295747 learning rate =  0.5 update =  [[-0.00107593]\n"," [-0.00107593]\n"," [ 0.00161266]]\n","Iter:  3781 loss =  0.00919609286923435 learning rate =  0.5 update =  [[-0.00107565]\n"," [-0.00107565]\n"," [ 0.00161223]]\n","Iter:  3782 loss =  0.009193636530509346 learning rate =  0.5 update =  [[-0.00107536]\n"," [-0.00107536]\n"," [ 0.00161181]]\n","Iter:  3783 loss =  0.009191181495747377 learning rate =  0.5 update =  [[-0.00107508]\n"," [-0.00107508]\n"," [ 0.00161138]]\n","Iter:  3784 loss =  0.009188727763914648 learning rate =  0.5 update =  [[-0.00107479]\n"," [-0.00107479]\n"," [ 0.00161095]]\n","Iter:  3785 loss =  0.009186275333978058 learning rate =  0.5 update =  [[-0.0010745 ]\n"," [-0.0010745 ]\n"," [ 0.00161052]]\n","Iter:  3786 loss =  0.00918382420490613 learning rate =  0.5 update =  [[-0.00107422]\n"," [-0.00107422]\n"," [ 0.0016101 ]]\n","Iter:  3787 loss =  0.009181374375668109 learning rate =  0.5 update =  [[-0.00107393]\n"," [-0.00107393]\n"," [ 0.00160967]]\n","Iter:  3788 loss =  0.009178925845234387 learning rate =  0.5 update =  [[-0.00107365]\n"," [-0.00107365]\n"," [ 0.00160924]]\n","Iter:  3789 loss =  0.009176478612576472 learning rate =  0.5 update =  [[-0.00107337]\n"," [-0.00107336]\n"," [ 0.00160882]]\n","Iter:  3790 loss =  0.009174032676666987 learning rate =  0.5 update =  [[-0.00107308]\n"," [-0.00107308]\n"," [ 0.00160839]]\n","Iter:  3791 loss =  0.009171588036479592 learning rate =  0.5 update =  [[-0.0010728 ]\n"," [-0.0010728 ]\n"," [ 0.00160797]]\n","Iter:  3792 loss =  0.009169144690988975 learning rate =  0.5 update =  [[-0.00107251]\n"," [-0.00107251]\n"," [ 0.00160754]]\n","Iter:  3793 loss =  0.009166702639171194 learning rate =  0.5 update =  [[-0.00107223]\n"," [-0.00107223]\n"," [ 0.00160712]]\n","Iter:  3794 loss =  0.009164261880002866 learning rate =  0.5 update =  [[-0.00107194]\n"," [-0.00107194]\n"," [ 0.00160669]]\n","Iter:  3795 loss =  0.009161822412462167 learning rate =  0.5 update =  [[-0.00107166]\n"," [-0.00107166]\n"," [ 0.00160626]]\n","Iter:  3796 loss =  0.009159384235528086 learning rate =  0.5 update =  [[-0.00107138]\n"," [-0.00107138]\n"," [ 0.00160584]]\n","Iter:  3797 loss =  0.009156947348180755 learning rate =  0.5 update =  [[-0.00107109]\n"," [-0.00107109]\n"," [ 0.00160542]]\n","Iter:  3798 loss =  0.009154511749401342 learning rate =  0.5 update =  [[-0.00107081]\n"," [-0.00107081]\n"," [ 0.00160499]]\n","Iter:  3799 loss =  0.00915207743817215 learning rate =  0.5 update =  [[-0.00107053]\n"," [-0.00107053]\n"," [ 0.00160457]]\n","Iter:  3800 loss =  0.009149644413476465 learning rate =  0.5 update =  [[-0.00107024]\n"," [-0.00107024]\n"," [ 0.00160414]]\n","Iter:  3801 loss =  0.009147212674298803 learning rate =  0.5 update =  [[-0.00106996]\n"," [-0.00106996]\n"," [ 0.00160372]]\n","Iter:  3802 loss =  0.009144782219624595 learning rate =  0.5 update =  [[-0.00106968]\n"," [-0.00106968]\n"," [ 0.0016033 ]]\n","Iter:  3803 loss =  0.00914235304844031 learning rate =  0.5 update =  [[-0.0010694 ]\n"," [-0.0010694 ]\n"," [ 0.00160287]]\n","Iter:  3804 loss =  0.009139925159733553 learning rate =  0.5 update =  [[-0.00106911]\n"," [-0.00106911]\n"," [ 0.00160245]]\n","Iter:  3805 loss =  0.00913749855249308 learning rate =  0.5 update =  [[-0.00106883]\n"," [-0.00106883]\n"," [ 0.00160203]]\n","Iter:  3806 loss =  0.009135073225708452 learning rate =  0.5 update =  [[-0.00106855]\n"," [-0.00106855]\n"," [ 0.00160161]]\n","Iter:  3807 loss =  0.009132649178370655 learning rate =  0.5 update =  [[-0.00106827]\n"," [-0.00106827]\n"," [ 0.00160118]]\n","Iter:  3808 loss =  0.009130226409471287 learning rate =  0.5 update =  [[-0.00106799]\n"," [-0.00106799]\n"," [ 0.00160076]]\n","Iter:  3809 loss =  0.009127804918003482 learning rate =  0.5 update =  [[-0.0010677 ]\n"," [-0.0010677 ]\n"," [ 0.00160034]]\n","Iter:  3810 loss =  0.009125384702961039 learning rate =  0.5 update =  [[-0.00106742]\n"," [-0.00106742]\n"," [ 0.00159992]]\n","Iter:  3811 loss =  0.009122965763338878 learning rate =  0.5 update =  [[-0.00106714]\n"," [-0.00106714]\n"," [ 0.0015995 ]]\n","Iter:  3812 loss =  0.009120548098133197 learning rate =  0.5 update =  [[-0.00106686]\n"," [-0.00106686]\n"," [ 0.00159908]]\n","Iter:  3813 loss =  0.009118131706341091 learning rate =  0.5 update =  [[-0.00106658]\n"," [-0.00106658]\n"," [ 0.00159865]]\n","Iter:  3814 loss =  0.009115716586960629 learning rate =  0.5 update =  [[-0.0010663 ]\n"," [-0.0010663 ]\n"," [ 0.00159823]]\n","Iter:  3815 loss =  0.009113302738990962 learning rate =  0.5 update =  [[-0.00106602]\n"," [-0.00106602]\n"," [ 0.00159781]]\n","Iter:  3816 loss =  0.009110890161432565 learning rate =  0.5 update =  [[-0.00106574]\n"," [-0.00106574]\n"," [ 0.00159739]]\n","Iter:  3817 loss =  0.009108478853286427 learning rate =  0.5 update =  [[-0.00106546]\n"," [-0.00106546]\n"," [ 0.00159697]]\n","Iter:  3818 loss =  0.00910606881355511 learning rate =  0.5 update =  [[-0.00106518]\n"," [-0.00106518]\n"," [ 0.00159655]]\n","Iter:  3819 loss =  0.009103660041241897 learning rate =  0.5 update =  [[-0.0010649 ]\n"," [-0.0010649 ]\n"," [ 0.00159613]]\n","Iter:  3820 loss =  0.009101252535351153 learning rate =  0.5 update =  [[-0.00106462]\n"," [-0.00106462]\n"," [ 0.00159571]]\n","Iter:  3821 loss =  0.009098846294888368 learning rate =  0.5 update =  [[-0.00106434]\n"," [-0.00106434]\n"," [ 0.00159529]]\n","Iter:  3822 loss =  0.009096441318860114 learning rate =  0.5 update =  [[-0.00106406]\n"," [-0.00106406]\n"," [ 0.00159488]]\n","Iter:  3823 loss =  0.009094037606273853 learning rate =  0.5 update =  [[-0.00106378]\n"," [-0.00106378]\n"," [ 0.00159446]]\n","Iter:  3824 loss =  0.009091635156138108 learning rate =  0.5 update =  [[-0.0010635 ]\n"," [-0.0010635 ]\n"," [ 0.00159404]]\n","Iter:  3825 loss =  0.009089233967462548 learning rate =  0.5 update =  [[-0.00106322]\n"," [-0.00106322]\n"," [ 0.00159362]]\n","Iter:  3826 loss =  0.009086834039257728 learning rate =  0.5 update =  [[-0.00106294]\n"," [-0.00106294]\n"," [ 0.0015932 ]]\n","Iter:  3827 loss =  0.009084435370535349 learning rate =  0.5 update =  [[-0.00106266]\n"," [-0.00106266]\n"," [ 0.00159278]]\n","Iter:  3828 loss =  0.009082037960308117 learning rate =  0.5 update =  [[-0.00106238]\n"," [-0.00106238]\n"," [ 0.00159237]]\n","Iter:  3829 loss =  0.009079641807589765 learning rate =  0.5 update =  [[-0.0010621 ]\n"," [-0.0010621 ]\n"," [ 0.00159195]]\n","Iter:  3830 loss =  0.009077246911394948 learning rate =  0.5 update =  [[-0.00106182]\n"," [-0.00106182]\n"," [ 0.00159153]]\n","Iter:  3831 loss =  0.00907485327073955 learning rate =  0.5 update =  [[-0.00106154]\n"," [-0.00106154]\n"," [ 0.00159111]]\n","Iter:  3832 loss =  0.009072460884640282 learning rate =  0.5 update =  [[-0.00106127]\n"," [-0.00106127]\n"," [ 0.0015907 ]]\n","Iter:  3833 loss =  0.009070069752115092 learning rate =  0.5 update =  [[-0.00106099]\n"," [-0.00106099]\n"," [ 0.00159028]]\n","Iter:  3834 loss =  0.009067679872182642 learning rate =  0.5 update =  [[-0.00106071]\n"," [-0.00106071]\n"," [ 0.00158986]]\n","Iter:  3835 loss =  0.009065291243862912 learning rate =  0.5 update =  [[-0.00106043]\n"," [-0.00106043]\n"," [ 0.00158945]]\n","Iter:  3836 loss =  0.00906290386617677 learning rate =  0.5 update =  [[-0.00106015]\n"," [-0.00106015]\n"," [ 0.00158903]]\n","Iter:  3837 loss =  0.009060517738146139 learning rate =  0.5 update =  [[-0.00105988]\n"," [-0.00105988]\n"," [ 0.00158862]]\n","Iter:  3838 loss =  0.009058132858793925 learning rate =  0.5 update =  [[-0.0010596]\n"," [-0.0010596]\n"," [ 0.0015882]]\n","Iter:  3839 loss =  0.009055749227143943 learning rate =  0.5 update =  [[-0.00105932]\n"," [-0.00105932]\n"," [ 0.00158779]]\n","Iter:  3840 loss =  0.00905336684222133 learning rate =  0.5 update =  [[-0.00105905]\n"," [-0.00105905]\n"," [ 0.00158737]]\n","Iter:  3841 loss =  0.009050985703051906 learning rate =  0.5 update =  [[-0.00105877]\n"," [-0.00105877]\n"," [ 0.00158696]]\n","Iter:  3842 loss =  0.00904860580866267 learning rate =  0.5 update =  [[-0.00105849]\n"," [-0.00105849]\n"," [ 0.00158654]]\n","Iter:  3843 loss =  0.009046227158081677 learning rate =  0.5 update =  [[-0.00105821]\n"," [-0.00105821]\n"," [ 0.00158613]]\n","Iter:  3844 loss =  0.009043849750337896 learning rate =  0.5 update =  [[-0.00105794]\n"," [-0.00105794]\n"," [ 0.00158571]]\n","Iter:  3845 loss =  0.009041473584461309 learning rate =  0.5 update =  [[-0.00105766]\n"," [-0.00105766]\n"," [ 0.0015853 ]]\n","Iter:  3846 loss =  0.009039098659482834 learning rate =  0.5 update =  [[-0.00105739]\n"," [-0.00105739]\n"," [ 0.00158489]]\n","Iter:  3847 loss =  0.009036724974434536 learning rate =  0.5 update =  [[-0.00105711]\n"," [-0.00105711]\n"," [ 0.00158447]]\n","Iter:  3848 loss =  0.009034352528349457 learning rate =  0.5 update =  [[-0.00105683]\n"," [-0.00105683]\n"," [ 0.00158406]]\n","Iter:  3849 loss =  0.009031981320261662 learning rate =  0.5 update =  [[-0.00105656]\n"," [-0.00105656]\n"," [ 0.00158365]]\n","Iter:  3850 loss =  0.00902961134920606 learning rate =  0.5 update =  [[-0.00105628]\n"," [-0.00105628]\n"," [ 0.00158323]]\n","Iter:  3851 loss =  0.009027242614218691 learning rate =  0.5 update =  [[-0.00105601]\n"," [-0.00105601]\n"," [ 0.00158282]]\n","Iter:  3852 loss =  0.009024875114336604 learning rate =  0.5 update =  [[-0.00105573]\n"," [-0.00105573]\n"," [ 0.00158241]]\n","Iter:  3853 loss =  0.009022508848597772 learning rate =  0.5 update =  [[-0.00105546]\n"," [-0.00105546]\n"," [ 0.001582  ]]\n","Iter:  3854 loss =  0.00902014381604127 learning rate =  0.5 update =  [[-0.00105518]\n"," [-0.00105518]\n"," [ 0.00158158]]\n","Iter:  3855 loss =  0.009017780015707097 learning rate =  0.5 update =  [[-0.00105491]\n"," [-0.00105491]\n"," [ 0.00158117]]\n","Iter:  3856 loss =  0.009015417446636115 learning rate =  0.5 update =  [[-0.00105463]\n"," [-0.00105463]\n"," [ 0.00158076]]\n","Iter:  3857 loss =  0.009013056107870464 learning rate =  0.5 update =  [[-0.00105436]\n"," [-0.00105436]\n"," [ 0.00158035]]\n","Iter:  3858 loss =  0.009010695998452958 learning rate =  0.5 update =  [[-0.00105408]\n"," [-0.00105408]\n"," [ 0.00157994]]\n","Iter:  3859 loss =  0.009008337117427769 learning rate =  0.5 update =  [[-0.00105381]\n"," [-0.00105381]\n"," [ 0.00157953]]\n","Iter:  3860 loss =  0.009005979463839766 learning rate =  0.5 update =  [[-0.00105353]\n"," [-0.00105353]\n"," [ 0.00157912]]\n","Iter:  3861 loss =  0.009003623036734763 learning rate =  0.5 update =  [[-0.00105326]\n"," [-0.00105326]\n"," [ 0.0015787 ]]\n","Iter:  3862 loss =  0.00900126783515989 learning rate =  0.5 update =  [[-0.00105298]\n"," [-0.00105298]\n"," [ 0.00157829]]\n","Iter:  3863 loss =  0.008998913858162899 learning rate =  0.5 update =  [[-0.00105271]\n"," [-0.00105271]\n"," [ 0.00157788]]\n","Iter:  3864 loss =  0.00899656110479279 learning rate =  0.5 update =  [[-0.00105244]\n"," [-0.00105244]\n"," [ 0.00157747]]\n","Iter:  3865 loss =  0.008994209574099375 learning rate =  0.5 update =  [[-0.00105216]\n"," [-0.00105216]\n"," [ 0.00157706]]\n","Iter:  3866 loss =  0.008991859265133625 learning rate =  0.5 update =  [[-0.00105189]\n"," [-0.00105189]\n"," [ 0.00157666]]\n","Iter:  3867 loss =  0.008989510176947222 learning rate =  0.5 update =  [[-0.00105162]\n"," [-0.00105162]\n"," [ 0.00157625]]\n","Iter:  3868 loss =  0.008987162308593106 learning rate =  0.5 update =  [[-0.00105134]\n"," [-0.00105134]\n"," [ 0.00157584]]\n","Iter:  3869 loss =  0.008984815659124939 learning rate =  0.5 update =  [[-0.00105107]\n"," [-0.00105107]\n"," [ 0.00157543]]\n","Iter:  3870 loss =  0.008982470227597662 learning rate =  0.5 update =  [[-0.0010508 ]\n"," [-0.0010508 ]\n"," [ 0.00157502]]\n","Iter:  3871 loss =  0.008980126013066825 learning rate =  0.5 update =  [[-0.00105053]\n"," [-0.00105053]\n"," [ 0.00157461]]\n","Iter:  3872 loss =  0.008977783014589267 learning rate =  0.5 update =  [[-0.00105025]\n"," [-0.00105025]\n"," [ 0.0015742 ]]\n","Iter:  3873 loss =  0.008975441231222648 learning rate =  0.5 update =  [[-0.00104998]\n"," [-0.00104998]\n"," [ 0.00157379]]\n","Iter:  3874 loss =  0.008973100662025672 learning rate =  0.5 update =  [[-0.00104971]\n"," [-0.00104971]\n"," [ 0.00157339]]\n","Iter:  3875 loss =  0.008970761306057833 learning rate =  0.5 update =  [[-0.00104944]\n"," [-0.00104944]\n"," [ 0.00157298]]\n","Iter:  3876 loss =  0.008968423162379809 learning rate =  0.5 update =  [[-0.00104916]\n"," [-0.00104916]\n"," [ 0.00157257]]\n","Iter:  3877 loss =  0.008966086230053185 learning rate =  0.5 update =  [[-0.00104889]\n"," [-0.00104889]\n"," [ 0.00157216]]\n","Iter:  3878 loss =  0.008963750508140494 learning rate =  0.5 update =  [[-0.00104862]\n"," [-0.00104862]\n"," [ 0.00157176]]\n","Iter:  3879 loss =  0.008961415995705192 learning rate =  0.5 update =  [[-0.00104835]\n"," [-0.00104835]\n"," [ 0.00157135]]\n","Iter:  3880 loss =  0.008959082691811754 learning rate =  0.5 update =  [[-0.00104808]\n"," [-0.00104808]\n"," [ 0.00157094]]\n","Iter:  3881 loss =  0.008956750595525552 learning rate =  0.5 update =  [[-0.00104781]\n"," [-0.00104781]\n"," [ 0.00157054]]\n","Iter:  3882 loss =  0.008954419705912955 learning rate =  0.5 update =  [[-0.00104753]\n"," [-0.00104753]\n"," [ 0.00157013]]\n","Iter:  3883 loss =  0.008952090022041309 learning rate =  0.5 update =  [[-0.00104726]\n"," [-0.00104726]\n"," [ 0.00156973]]\n","Iter:  3884 loss =  0.008949761542978989 learning rate =  0.5 update =  [[-0.00104699]\n"," [-0.00104699]\n"," [ 0.00156932]]\n","Iter:  3885 loss =  0.008947434267795196 learning rate =  0.5 update =  [[-0.00104672]\n"," [-0.00104672]\n"," [ 0.00156891]]\n","Iter:  3886 loss =  0.00894510819556014 learning rate =  0.5 update =  [[-0.00104645]\n"," [-0.00104645]\n"," [ 0.00156851]]\n","Iter:  3887 loss =  0.008942783325344917 learning rate =  0.5 update =  [[-0.00104618]\n"," [-0.00104618]\n"," [ 0.0015681 ]]\n","Iter:  3888 loss =  0.008940459656221762 learning rate =  0.5 update =  [[-0.00104591]\n"," [-0.00104591]\n"," [ 0.0015677 ]]\n","Iter:  3889 loss =  0.008938137187263637 learning rate =  0.5 update =  [[-0.00104564]\n"," [-0.00104564]\n"," [ 0.00156729]]\n","Iter:  3890 loss =  0.00893581591754461 learning rate =  0.5 update =  [[-0.00104537]\n"," [-0.00104537]\n"," [ 0.00156689]]\n","Iter:  3891 loss =  0.008933495846139639 learning rate =  0.5 update =  [[-0.0010451 ]\n"," [-0.0010451 ]\n"," [ 0.00156649]]\n","Iter:  3892 loss =  0.008931176972124606 learning rate =  0.5 update =  [[-0.00104483]\n"," [-0.00104483]\n"," [ 0.00156608]]\n","Iter:  3893 loss =  0.008928859294576439 learning rate =  0.5 update =  [[-0.00104456]\n"," [-0.00104456]\n"," [ 0.00156568]]\n","Iter:  3894 loss =  0.008926542812572907 learning rate =  0.5 update =  [[-0.00104429]\n"," [-0.00104429]\n"," [ 0.00156527]]\n","Iter:  3895 loss =  0.008924227525192719 learning rate =  0.5 update =  [[-0.00104402]\n"," [-0.00104402]\n"," [ 0.00156487]]\n","Iter:  3896 loss =  0.008921913431515646 learning rate =  0.5 update =  [[-0.00104375]\n"," [-0.00104375]\n"," [ 0.00156447]]\n","Iter:  3897 loss =  0.008919600530622341 learning rate =  0.5 update =  [[-0.00104348]\n"," [-0.00104348]\n"," [ 0.00156406]]\n","Iter:  3898 loss =  0.008917288821594293 learning rate =  0.5 update =  [[-0.00104321]\n"," [-0.00104321]\n"," [ 0.00156366]]\n","Iter:  3899 loss =  0.00891497830351416 learning rate =  0.5 update =  [[-0.00104295]\n"," [-0.00104295]\n"," [ 0.00156326]]\n","Iter:  3900 loss =  0.008912668975465237 learning rate =  0.5 update =  [[-0.00104268]\n"," [-0.00104268]\n"," [ 0.00156286]]\n","Iter:  3901 loss =  0.008910360836531985 learning rate =  0.5 update =  [[-0.00104241]\n"," [-0.00104241]\n"," [ 0.00156245]]\n","Iter:  3902 loss =  0.00890805388579981 learning rate =  0.5 update =  [[-0.00104214]\n"," [-0.00104214]\n"," [ 0.00156205]]\n","Iter:  3903 loss =  0.008905748122354841 learning rate =  0.5 update =  [[-0.00104187]\n"," [-0.00104187]\n"," [ 0.00156165]]\n","Iter:  3904 loss =  0.00890344354528446 learning rate =  0.5 update =  [[-0.0010416 ]\n"," [-0.0010416 ]\n"," [ 0.00156125]]\n","Iter:  3905 loss =  0.00890114015367665 learning rate =  0.5 update =  [[-0.00104134]\n"," [-0.00104134]\n"," [ 0.00156085]]\n","Iter:  3906 loss =  0.008898837946620514 learning rate =  0.5 update =  [[-0.00104107]\n"," [-0.00104107]\n"," [ 0.00156045]]\n","Iter:  3907 loss =  0.008896536923206072 learning rate =  0.5 update =  [[-0.0010408 ]\n"," [-0.0010408 ]\n"," [ 0.00156005]]\n","Iter:  3908 loss =  0.008894237082524254 learning rate =  0.5 update =  [[-0.00104053]\n"," [-0.00104053]\n"," [ 0.00155964]]\n","Iter:  3909 loss =  0.008891938423666907 learning rate =  0.5 update =  [[-0.00104027]\n"," [-0.00104027]\n"," [ 0.00155924]]\n","Iter:  3910 loss =  0.008889640945726775 learning rate =  0.5 update =  [[-0.00104   ]\n"," [-0.00104   ]\n"," [ 0.00155884]]\n","Iter:  3911 loss =  0.00888734464779759 learning rate =  0.5 update =  [[-0.00103973]\n"," [-0.00103973]\n"," [ 0.00155844]]\n","Iter:  3912 loss =  0.008885049528974065 learning rate =  0.5 update =  [[-0.00103946]\n"," [-0.00103946]\n"," [ 0.00155804]]\n","Iter:  3913 loss =  0.008882755588351544 learning rate =  0.5 update =  [[-0.0010392 ]\n"," [-0.0010392 ]\n"," [ 0.00155764]]\n","Iter:  3914 loss =  0.008880462825026712 learning rate =  0.5 update =  [[-0.00103893]\n"," [-0.00103893]\n"," [ 0.00155724]]\n","Iter:  3915 loss =  0.00887817123809697 learning rate =  0.5 update =  [[-0.00103866]\n"," [-0.00103866]\n"," [ 0.00155685]]\n","Iter:  3916 loss =  0.008875880826660439 learning rate =  0.5 update =  [[-0.0010384 ]\n"," [-0.0010384 ]\n"," [ 0.00155645]]\n","Iter:  3917 loss =  0.008873591589816494 learning rate =  0.5 update =  [[-0.00103813]\n"," [-0.00103813]\n"," [ 0.00155605]]\n","Iter:  3918 loss =  0.00887130352666526 learning rate =  0.5 update =  [[-0.00103786]\n"," [-0.00103786]\n"," [ 0.00155565]]\n","Iter:  3919 loss =  0.008869016636307799 learning rate =  0.5 update =  [[-0.0010376 ]\n"," [-0.0010376 ]\n"," [ 0.00155525]]\n","Iter:  3920 loss =  0.008866730917846233 learning rate =  0.5 update =  [[-0.00103733]\n"," [-0.00103733]\n"," [ 0.00155485]]\n","Iter:  3921 loss =  0.008864446370383212 learning rate =  0.5 update =  [[-0.00103707]\n"," [-0.00103707]\n"," [ 0.00155445]]\n","Iter:  3922 loss =  0.00886216299302263 learning rate =  0.5 update =  [[-0.0010368 ]\n"," [-0.0010368 ]\n"," [ 0.00155406]]\n","Iter:  3923 loss =  0.00885988078486936 learning rate =  0.5 update =  [[-0.00103654]\n"," [-0.00103654]\n"," [ 0.00155366]]\n","Iter:  3924 loss =  0.008857599745028873 learning rate =  0.5 update =  [[-0.00103627]\n"," [-0.00103627]\n"," [ 0.00155326]]\n","Iter:  3925 loss =  0.008855319872607736 learning rate =  0.5 update =  [[-0.001036  ]\n"," [-0.001036  ]\n"," [ 0.00155286]]\n","Iter:  3926 loss =  0.00885304116671348 learning rate =  0.5 update =  [[-0.00103574]\n"," [-0.00103574]\n"," [ 0.00155247]]\n","Iter:  3927 loss =  0.008850763626454471 learning rate =  0.5 update =  [[-0.00103547]\n"," [-0.00103547]\n"," [ 0.00155207]]\n","Iter:  3928 loss =  0.008848487250939818 learning rate =  0.5 update =  [[-0.00103521]\n"," [-0.00103521]\n"," [ 0.00155167]]\n","Iter:  3929 loss =  0.008846212039279856 learning rate =  0.5 update =  [[-0.00103494]\n"," [-0.00103494]\n"," [ 0.00155128]]\n","Iter:  3930 loss =  0.008843937990585506 learning rate =  0.5 update =  [[-0.00103468]\n"," [-0.00103468]\n"," [ 0.00155088]]\n","Iter:  3931 loss =  0.008841665103968872 learning rate =  0.5 update =  [[-0.00103442]\n"," [-0.00103442]\n"," [ 0.00155048]]\n","Iter:  3932 loss =  0.008839393378542787 learning rate =  0.5 update =  [[-0.00103415]\n"," [-0.00103415]\n"," [ 0.00155009]]\n","Iter:  3933 loss =  0.008837122813421024 learning rate =  0.5 update =  [[-0.00103389]\n"," [-0.00103389]\n"," [ 0.00154969]]\n","Iter:  3934 loss =  0.008834853407718217 learning rate =  0.5 update =  [[-0.00103362]\n"," [-0.00103362]\n"," [ 0.0015493 ]]\n","Iter:  3935 loss =  0.008832585160549973 learning rate =  0.5 update =  [[-0.00103336]\n"," [-0.00103336]\n"," [ 0.0015489 ]]\n","Iter:  3936 loss =  0.008830318071032836 learning rate =  0.5 update =  [[-0.0010331 ]\n"," [-0.0010331 ]\n"," [ 0.00154851]]\n","Iter:  3937 loss =  0.008828052138284073 learning rate =  0.5 update =  [[-0.00103283]\n"," [-0.00103283]\n"," [ 0.00154811]]\n","Iter:  3938 loss =  0.00882578736142201 learning rate =  0.5 update =  [[-0.00103257]\n"," [-0.00103257]\n"," [ 0.00154772]]\n","Iter:  3939 loss =  0.008823523739565658 learning rate =  0.5 update =  [[-0.00103231]\n"," [-0.00103231]\n"," [ 0.00154732]]\n","Iter:  3940 loss =  0.008821261271835313 learning rate =  0.5 update =  [[-0.00103204]\n"," [-0.00103204]\n"," [ 0.00154693]]\n","Iter:  3941 loss =  0.008818999957351676 learning rate =  0.5 update =  [[-0.00103178]\n"," [-0.00103178]\n"," [ 0.00154653]]\n","Iter:  3942 loss =  0.008816739795236707 learning rate =  0.5 update =  [[-0.00103152]\n"," [-0.00103152]\n"," [ 0.00154614]]\n","Iter:  3943 loss =  0.008814480784613048 learning rate =  0.5 update =  [[-0.00103125]\n"," [-0.00103125]\n"," [ 0.00154575]]\n","Iter:  3944 loss =  0.008812222924604318 learning rate =  0.5 update =  [[-0.00103099]\n"," [-0.00103099]\n"," [ 0.00154535]]\n","Iter:  3945 loss =  0.00880996621433508 learning rate =  0.5 update =  [[-0.00103073]\n"," [-0.00103073]\n"," [ 0.00154496]]\n","Iter:  3946 loss =  0.008807710652930614 learning rate =  0.5 update =  [[-0.00103047]\n"," [-0.00103047]\n"," [ 0.00154457]]\n","Iter:  3947 loss =  0.008805456239517225 learning rate =  0.5 update =  [[-0.0010302 ]\n"," [-0.0010302 ]\n"," [ 0.00154417]]\n","Iter:  3948 loss =  0.008803202973222051 learning rate =  0.5 update =  [[-0.00102994]\n"," [-0.00102994]\n"," [ 0.00154378]]\n","Iter:  3949 loss =  0.008800950853173147 learning rate =  0.5 update =  [[-0.00102968]\n"," [-0.00102968]\n"," [ 0.00154339]]\n","Iter:  3950 loss =  0.008798699878499397 learning rate =  0.5 update =  [[-0.00102942]\n"," [-0.00102942]\n"," [ 0.001543  ]]\n","Iter:  3951 loss =  0.008796450048330486 learning rate =  0.5 update =  [[-0.00102915]\n"," [-0.00102915]\n"," [ 0.0015426 ]]\n","Iter:  3952 loss =  0.00879420136179727 learning rate =  0.5 update =  [[-0.00102889]\n"," [-0.00102889]\n"," [ 0.00154221]]\n","Iter:  3953 loss =  0.008791953818031125 learning rate =  0.5 update =  [[-0.00102863]\n"," [-0.00102863]\n"," [ 0.00154182]]\n","Iter:  3954 loss =  0.008789707416164565 learning rate =  0.5 update =  [[-0.00102837]\n"," [-0.00102837]\n"," [ 0.00154143]]\n","Iter:  3955 loss =  0.00878746215533088 learning rate =  0.5 update =  [[-0.00102811]\n"," [-0.00102811]\n"," [ 0.00154104]]\n","Iter:  3956 loss =  0.008785218034664194 learning rate =  0.5 update =  [[-0.00102785]\n"," [-0.00102785]\n"," [ 0.00154065]]\n","Iter:  3957 loss =  0.00878297505329957 learning rate =  0.5 update =  [[-0.00102759]\n"," [-0.00102759]\n"," [ 0.00154026]]\n","Iter:  3958 loss =  0.008780733210372984 learning rate =  0.5 update =  [[-0.00102733]\n"," [-0.00102733]\n"," [ 0.00153986]]\n","Iter:  3959 loss =  0.00877849250502108 learning rate =  0.5 update =  [[-0.00102707]\n"," [-0.00102707]\n"," [ 0.00153947]]\n","Iter:  3960 loss =  0.008776252936381635 learning rate =  0.5 update =  [[-0.0010268 ]\n"," [-0.0010268 ]\n"," [ 0.00153908]]\n","Iter:  3961 loss =  0.008774014503593122 learning rate =  0.5 update =  [[-0.00102654]\n"," [-0.00102654]\n"," [ 0.00153869]]\n","Iter:  3962 loss =  0.008771777205794926 learning rate =  0.5 update =  [[-0.00102628]\n"," [-0.00102628]\n"," [ 0.0015383 ]]\n","Iter:  3963 loss =  0.008769541042127427 learning rate =  0.5 update =  [[-0.00102602]\n"," [-0.00102602]\n"," [ 0.00153791]]\n","Iter:  3964 loss =  0.008767306011731592 learning rate =  0.5 update =  [[-0.00102576]\n"," [-0.00102576]\n"," [ 0.00153752]]\n","Iter:  3965 loss =  0.008765072113749462 learning rate =  0.5 update =  [[-0.0010255 ]\n"," [-0.0010255 ]\n"," [ 0.00153713]]\n","Iter:  3966 loss =  0.008762839347323914 learning rate =  0.5 update =  [[-0.00102524]\n"," [-0.00102524]\n"," [ 0.00153675]]\n","Iter:  3967 loss =  0.008760607711598624 learning rate =  0.5 update =  [[-0.00102498]\n"," [-0.00102498]\n"," [ 0.00153636]]\n","Iter:  3968 loss =  0.008758377205718183 learning rate =  0.5 update =  [[-0.00102472]\n"," [-0.00102472]\n"," [ 0.00153597]]\n","Iter:  3969 loss =  0.008756147828828073 learning rate =  0.5 update =  [[-0.00102447]\n"," [-0.00102447]\n"," [ 0.00153558]]\n","Iter:  3970 loss =  0.008753919580074543 learning rate =  0.5 update =  [[-0.00102421]\n"," [-0.00102421]\n"," [ 0.00153519]]\n","Iter:  3971 loss =  0.00875169245860476 learning rate =  0.5 update =  [[-0.00102395]\n"," [-0.00102395]\n"," [ 0.0015348 ]]\n","Iter:  3972 loss =  0.008749466463566752 learning rate =  0.5 update =  [[-0.00102369]\n"," [-0.00102369]\n"," [ 0.00153441]]\n","Iter:  3973 loss =  0.008747241594109344 learning rate =  0.5 update =  [[-0.00102343]\n"," [-0.00102343]\n"," [ 0.00153403]]\n","Iter:  3974 loss =  0.008745017849382253 learning rate =  0.5 update =  [[-0.00102317]\n"," [-0.00102317]\n"," [ 0.00153364]]\n","Iter:  3975 loss =  0.008742795228536156 learning rate =  0.5 update =  [[-0.00102291]\n"," [-0.00102291]\n"," [ 0.00153325]]\n","Iter:  3976 loss =  0.008740573730722349 learning rate =  0.5 update =  [[-0.00102265]\n"," [-0.00102265]\n"," [ 0.00153286]]\n","Iter:  3977 loss =  0.008738353355093174 learning rate =  0.5 update =  [[-0.00102239]\n"," [-0.00102239]\n"," [ 0.00153248]]\n","Iter:  3978 loss =  0.008736134100801835 learning rate =  0.5 update =  [[-0.00102214]\n"," [-0.00102214]\n"," [ 0.00153209]]\n","Iter:  3979 loss =  0.008733915967002164 learning rate =  0.5 update =  [[-0.00102188]\n"," [-0.00102188]\n"," [ 0.0015317 ]]\n","Iter:  3980 loss =  0.008731698952849086 learning rate =  0.5 update =  [[-0.00102162]\n"," [-0.00102162]\n"," [ 0.00153132]]\n","Iter:  3981 loss =  0.00872948305749826 learning rate =  0.5 update =  [[-0.00102136]\n"," [-0.00102136]\n"," [ 0.00153093]]\n","Iter:  3982 loss =  0.008727268280106181 learning rate =  0.5 update =  [[-0.0010211 ]\n"," [-0.0010211 ]\n"," [ 0.00153055]]\n","Iter:  3983 loss =  0.00872505461983026 learning rate =  0.5 update =  [[-0.00102085]\n"," [-0.00102085]\n"," [ 0.00153016]]\n","Iter:  3984 loss =  0.008722842075828704 learning rate =  0.5 update =  [[-0.00102059]\n"," [-0.00102059]\n"," [ 0.00152977]]\n","Iter:  3985 loss =  0.008720630647260528 learning rate =  0.5 update =  [[-0.00102033]\n"," [-0.00102033]\n"," [ 0.00152939]]\n","Iter:  3986 loss =  0.00871842033328563 learning rate =  0.5 update =  [[-0.00102007]\n"," [-0.00102007]\n"," [ 0.001529  ]]\n","Iter:  3987 loss =  0.00871621113306482 learning rate =  0.5 update =  [[-0.00101982]\n"," [-0.00101982]\n"," [ 0.00152862]]\n","Iter:  3988 loss =  0.008714003045759543 learning rate =  0.5 update =  [[-0.00101956]\n"," [-0.00101956]\n"," [ 0.00152823]]\n","Iter:  3989 loss =  0.008711796070532359 learning rate =  0.5 update =  [[-0.0010193 ]\n"," [-0.0010193 ]\n"," [ 0.00152785]]\n","Iter:  3990 loss =  0.008709590206546509 learning rate =  0.5 update =  [[-0.00101905]\n"," [-0.00101905]\n"," [ 0.00152746]]\n","Iter:  3991 loss =  0.008707385452965986 learning rate =  0.5 update =  [[-0.00101879]\n"," [-0.00101879]\n"," [ 0.00152708]]\n","Iter:  3992 loss =  0.00870518180895575 learning rate =  0.5 update =  [[-0.00101853]\n"," [-0.00101853]\n"," [ 0.0015267 ]]\n","Iter:  3993 loss =  0.008702979273681676 learning rate =  0.5 update =  [[-0.00101828]\n"," [-0.00101828]\n"," [ 0.00152631]]\n","Iter:  3994 loss =  0.008700777846310216 learning rate =  0.5 update =  [[-0.00101802]\n"," [-0.00101802]\n"," [ 0.00152593]]\n","Iter:  3995 loss =  0.008698577526008874 learning rate =  0.5 update =  [[-0.00101777]\n"," [-0.00101777]\n"," [ 0.00152554]]\n","Iter:  3996 loss =  0.008696378311945878 learning rate =  0.5 update =  [[-0.00101751]\n"," [-0.00101751]\n"," [ 0.00152516]]\n","Iter:  3997 loss =  0.008694180203290332 learning rate =  0.5 update =  [[-0.00101725]\n"," [-0.00101725]\n"," [ 0.00152478]]\n","Iter:  3998 loss =  0.008691983199212175 learning rate =  0.5 update =  [[-0.001017 ]\n"," [-0.001017 ]\n"," [ 0.0015244]]\n","Iter:  3999 loss =  0.00868978729888212 learning rate =  0.5 update =  [[-0.00101674]\n"," [-0.00101674]\n"," [ 0.00152401]]\n","Iter:  4000 loss =  0.008687592501471737 learning rate =  0.5 update =  [[-0.00101649]\n"," [-0.00101649]\n"," [ 0.00152363]]\n","Iter:  4001 loss =  0.008685398806153424 learning rate =  0.5 update =  [[-0.00101623]\n"," [-0.00101623]\n"," [ 0.00152325]]\n","Iter:  4002 loss =  0.00868320621210052 learning rate =  0.5 update =  [[-0.00101598]\n"," [-0.00101598]\n"," [ 0.00152287]]\n","Iter:  4003 loss =  0.008681014718487002 learning rate =  0.5 update =  [[-0.00101572]\n"," [-0.00101572]\n"," [ 0.00152248]]\n","Iter:  4004 loss =  0.008678824324487699 learning rate =  0.5 update =  [[-0.00101547]\n"," [-0.00101547]\n"," [ 0.0015221 ]]\n","Iter:  4005 loss =  0.008676635029278408 learning rate =  0.5 update =  [[-0.00101521]\n"," [-0.00101521]\n"," [ 0.00152172]]\n","Iter:  4006 loss =  0.008674446832035568 learning rate =  0.5 update =  [[-0.00101496]\n"," [-0.00101496]\n"," [ 0.00152134]]\n","Iter:  4007 loss =  0.008672259731936553 learning rate =  0.5 update =  [[-0.0010147 ]\n"," [-0.0010147 ]\n"," [ 0.00152096]]\n","Iter:  4008 loss =  0.008670073728159509 learning rate =  0.5 update =  [[-0.00101445]\n"," [-0.00101445]\n"," [ 0.00152058]]\n","Iter:  4009 loss =  0.00866788881988347 learning rate =  0.5 update =  [[-0.00101419]\n"," [-0.00101419]\n"," [ 0.0015202 ]]\n","Iter:  4010 loss =  0.008665705006288132 learning rate =  0.5 update =  [[-0.00101394]\n"," [-0.00101394]\n"," [ 0.00151982]]\n","Iter:  4011 loss =  0.008663522286554159 learning rate =  0.5 update =  [[-0.00101369]\n"," [-0.00101369]\n"," [ 0.00151943]]\n","Iter:  4012 loss =  0.00866134065986304 learning rate =  0.5 update =  [[-0.00101343]\n"," [-0.00101343]\n"," [ 0.00151905]]\n","Iter:  4013 loss =  0.008659160125396935 learning rate =  0.5 update =  [[-0.00101318]\n"," [-0.00101318]\n"," [ 0.00151867]]\n","Iter:  4014 loss =  0.008656980682338961 learning rate =  0.5 update =  [[-0.00101292]\n"," [-0.00101292]\n"," [ 0.00151829]]\n","Iter:  4015 loss =  0.008654802329872938 learning rate =  0.5 update =  [[-0.00101267]\n"," [-0.00101267]\n"," [ 0.00151791]]\n","Iter:  4016 loss =  0.008652625067183527 learning rate =  0.5 update =  [[-0.00101242]\n"," [-0.00101242]\n"," [ 0.00151753]]\n","Iter:  4017 loss =  0.008650448893456203 learning rate =  0.5 update =  [[-0.00101216]\n"," [-0.00101216]\n"," [ 0.00151716]]\n","Iter:  4018 loss =  0.00864827380787732 learning rate =  0.5 update =  [[-0.00101191]\n"," [-0.00101191]\n"," [ 0.00151678]]\n","Iter:  4019 loss =  0.00864609980963409 learning rate =  0.5 update =  [[-0.00101166]\n"," [-0.00101166]\n"," [ 0.0015164 ]]\n","Iter:  4020 loss =  0.008643926897914222 learning rate =  0.5 update =  [[-0.00101141]\n"," [-0.00101141]\n"," [ 0.00151602]]\n","Iter:  4021 loss =  0.008641755071906494 learning rate =  0.5 update =  [[-0.00101115]\n"," [-0.00101115]\n"," [ 0.00151564]]\n","Iter:  4022 loss =  0.0086395843308005 learning rate =  0.5 update =  [[-0.0010109 ]\n"," [-0.0010109 ]\n"," [ 0.00151526]]\n","Iter:  4023 loss =  0.00863741467378657 learning rate =  0.5 update =  [[-0.00101065]\n"," [-0.00101065]\n"," [ 0.00151488]]\n","Iter:  4024 loss =  0.008635246100055751 learning rate =  0.5 update =  [[-0.0010104 ]\n"," [-0.0010104 ]\n"," [ 0.00151451]]\n","Iter:  4025 loss =  0.008633078608800006 learning rate =  0.5 update =  [[-0.00101014]\n"," [-0.00101014]\n"," [ 0.00151413]]\n","Iter:  4026 loss =  0.008630912199212123 learning rate =  0.5 update =  [[-0.00100989]\n"," [-0.00100989]\n"," [ 0.00151375]]\n","Iter:  4027 loss =  0.008628746870485586 learning rate =  0.5 update =  [[-0.00100964]\n"," [-0.00100964]\n"," [ 0.00151337]]\n","Iter:  4028 loss =  0.008626582621814781 learning rate =  0.5 update =  [[-0.00100939]\n"," [-0.00100939]\n"," [ 0.001513  ]]\n","Iter:  4029 loss =  0.008624419452394762 learning rate =  0.5 update =  [[-0.00100914]\n"," [-0.00100914]\n"," [ 0.00151262]]\n","Iter:  4030 loss =  0.008622257361421522 learning rate =  0.5 update =  [[-0.00100888]\n"," [-0.00100888]\n"," [ 0.00151224]]\n","Iter:  4031 loss =  0.008620096348091773 learning rate =  0.5 update =  [[-0.00100863]\n"," [-0.00100863]\n"," [ 0.00151186]]\n","Iter:  4032 loss =  0.008617936411603052 learning rate =  0.5 update =  [[-0.00100838]\n"," [-0.00100838]\n"," [ 0.00151149]]\n","Iter:  4033 loss =  0.008615777551153638 learning rate =  0.5 update =  [[-0.00100813]\n"," [-0.00100813]\n"," [ 0.00151111]]\n","Iter:  4034 loss =  0.008613619765942705 learning rate =  0.5 update =  [[-0.00100788]\n"," [-0.00100788]\n"," [ 0.00151074]]\n","Iter:  4035 loss =  0.008611463055170077 learning rate =  0.5 update =  [[-0.00100763]\n"," [-0.00100763]\n"," [ 0.00151036]]\n","Iter:  4036 loss =  0.00860930741803652 learning rate =  0.5 update =  [[-0.00100738]\n"," [-0.00100738]\n"," [ 0.00150998]]\n","Iter:  4037 loss =  0.00860715285374344 learning rate =  0.5 update =  [[-0.00100713]\n"," [-0.00100713]\n"," [ 0.00150961]]\n","Iter:  4038 loss =  0.008604999361493176 learning rate =  0.5 update =  [[-0.00100687]\n"," [-0.00100687]\n"," [ 0.00150923]]\n","Iter:  4039 loss =  0.008602846940488785 learning rate =  0.5 update =  [[-0.00100662]\n"," [-0.00100662]\n"," [ 0.00150886]]\n","Iter:  4040 loss =  0.008600695589934097 learning rate =  0.5 update =  [[-0.00100637]\n"," [-0.00100637]\n"," [ 0.00150848]]\n","Iter:  4041 loss =  0.008598545309033803 learning rate =  0.5 update =  [[-0.00100612]\n"," [-0.00100612]\n"," [ 0.00150811]]\n","Iter:  4042 loss =  0.008596396096993243 learning rate =  0.5 update =  [[-0.00100587]\n"," [-0.00100587]\n"," [ 0.00150773]]\n","Iter:  4043 loss =  0.00859424795301868 learning rate =  0.5 update =  [[-0.00100562]\n"," [-0.00100562]\n"," [ 0.00150736]]\n","Iter:  4044 loss =  0.008592100876317007 learning rate =  0.5 update =  [[-0.00100537]\n"," [-0.00100537]\n"," [ 0.00150698]]\n","Iter:  4045 loss =  0.008589954866096193 learning rate =  0.5 update =  [[-0.00100512]\n"," [-0.00100512]\n"," [ 0.00150661]]\n","Iter:  4046 loss =  0.008587809921564563 learning rate =  0.5 update =  [[-0.00100487]\n"," [-0.00100487]\n"," [ 0.00150624]]\n","Iter:  4047 loss =  0.008585666041931605 learning rate =  0.5 update =  [[-0.00100462]\n"," [-0.00100462]\n"," [ 0.00150586]]\n","Iter:  4048 loss =  0.008583523226407468 learning rate =  0.5 update =  [[-0.00100438]\n"," [-0.00100438]\n"," [ 0.00150549]]\n","Iter:  4049 loss =  0.008581381474202876 learning rate =  0.5 update =  [[-0.00100413]\n"," [-0.00100413]\n"," [ 0.00150512]]\n","Iter:  4050 loss =  0.008579240784529662 learning rate =  0.5 update =  [[-0.00100388]\n"," [-0.00100388]\n"," [ 0.00150474]]\n","Iter:  4051 loss =  0.008577101156600151 learning rate =  0.5 update =  [[-0.00100363]\n"," [-0.00100363]\n"," [ 0.00150437]]\n","Iter:  4052 loss =  0.00857496258962772 learning rate =  0.5 update =  [[-0.00100338]\n"," [-0.00100338]\n"," [ 0.001504  ]]\n","Iter:  4053 loss =  0.008572825082826209 learning rate =  0.5 update =  [[-0.00100313]\n"," [-0.00100313]\n"," [ 0.00150362]]\n","Iter:  4054 loss =  0.008570688635410483 learning rate =  0.5 update =  [[-0.00100288]\n"," [-0.00100288]\n"," [ 0.00150325]]\n","Iter:  4055 loss =  0.008568553246596042 learning rate =  0.5 update =  [[-0.00100263]\n"," [-0.00100263]\n"," [ 0.00150288]]\n","Iter:  4056 loss =  0.00856641891559929 learning rate =  0.5 update =  [[-0.00100238]\n"," [-0.00100238]\n"," [ 0.00150251]]\n","Iter:  4057 loss =  0.008564285641637161 learning rate =  0.5 update =  [[-0.00100214]\n"," [-0.00100214]\n"," [ 0.00150214]]\n","Iter:  4058 loss =  0.008562153423927691 learning rate =  0.5 update =  [[-0.00100189]\n"," [-0.00100189]\n"," [ 0.00150176]]\n","Iter:  4059 loss =  0.00856002226168946 learning rate =  0.5 update =  [[-0.00100164]\n"," [-0.00100164]\n"," [ 0.00150139]]\n","Iter:  4060 loss =  0.008557892154141747 learning rate =  0.5 update =  [[-0.00100139]\n"," [-0.00100139]\n"," [ 0.00150102]]\n","Iter:  4061 loss =  0.008555763100504874 learning rate =  0.5 update =  [[-0.00100114]\n"," [-0.00100114]\n"," [ 0.00150065]]\n","Iter:  4062 loss =  0.008553635099999686 learning rate =  0.5 update =  [[-0.0010009 ]\n"," [-0.0010009 ]\n"," [ 0.00150028]]\n","Iter:  4063 loss =  0.008551508151847938 learning rate =  0.5 update =  [[-0.00100065]\n"," [-0.00100065]\n"," [ 0.00149991]]\n","Iter:  4064 loss =  0.008549382255272048 learning rate =  0.5 update =  [[-0.0010004 ]\n"," [-0.0010004 ]\n"," [ 0.00149954]]\n","Iter:  4065 loss =  0.008547257409495258 learning rate =  0.5 update =  [[-0.00100015]\n"," [-0.00100015]\n"," [ 0.00149917]]\n","Iter:  4066 loss =  0.008545133613741585 learning rate =  0.5 update =  [[-0.00099991]\n"," [-0.00099991]\n"," [ 0.0014988 ]]\n","Iter:  4067 loss =  0.008543010867235731 learning rate =  0.5 update =  [[-0.00099966]\n"," [-0.00099966]\n"," [ 0.00149843]]\n","Iter:  4068 loss =  0.0085408891692032 learning rate =  0.5 update =  [[-0.00099941]\n"," [-0.00099941]\n"," [ 0.00149806]]\n","Iter:  4069 loss =  0.008538768518870265 learning rate =  0.5 update =  [[-0.00099917]\n"," [-0.00099917]\n"," [ 0.00149769]]\n","Iter:  4070 loss =  0.00853664891546406 learning rate =  0.5 update =  [[-0.00099892]\n"," [-0.00099892]\n"," [ 0.00149732]]\n","Iter:  4071 loss =  0.008534530358212231 learning rate =  0.5 update =  [[-0.00099867]\n"," [-0.00099867]\n"," [ 0.00149695]]\n","Iter:  4072 loss =  0.008532412846343452 learning rate =  0.5 update =  [[-0.00099843]\n"," [-0.00099843]\n"," [ 0.00149658]]\n","Iter:  4073 loss =  0.008530296379086946 learning rate =  0.5 update =  [[-0.00099818]\n"," [-0.00099818]\n"," [ 0.00149621]]\n","Iter:  4074 loss =  0.008528180955672787 learning rate =  0.5 update =  [[-0.00099793]\n"," [-0.00099793]\n"," [ 0.00149584]]\n","Iter:  4075 loss =  0.00852606657533177 learning rate =  0.5 update =  [[-0.00099769]\n"," [-0.00099769]\n"," [ 0.00149547]]\n","Iter:  4076 loss =  0.008523953237295484 learning rate =  0.5 update =  [[-0.00099744]\n"," [-0.00099744]\n"," [ 0.0014951 ]]\n","Iter:  4077 loss =  0.008521840940796183 learning rate =  0.5 update =  [[-0.0009972 ]\n"," [-0.0009972 ]\n"," [ 0.00149474]]\n","Iter:  4078 loss =  0.00851972968506714 learning rate =  0.5 update =  [[-0.00099695]\n"," [-0.00099695]\n"," [ 0.00149437]]\n","Iter:  4079 loss =  0.008517619469341924 learning rate =  0.5 update =  [[-0.0009967]\n"," [-0.0009967]\n"," [ 0.001494 ]]\n","Iter:  4080 loss =  0.00851551029285524 learning rate =  0.5 update =  [[-0.00099646]\n"," [-0.00099646]\n"," [ 0.00149363]]\n","Iter:  4081 loss =  0.008513402154842371 learning rate =  0.5 update =  [[-0.00099621]\n"," [-0.00099621]\n"," [ 0.00149326]]\n","Iter:  4082 loss =  0.008511295054539422 learning rate =  0.5 update =  [[-0.00099597]\n"," [-0.00099597]\n"," [ 0.0014929 ]]\n","Iter:  4083 loss =  0.008509188991183136 learning rate =  0.5 update =  [[-0.00099572]\n"," [-0.00099572]\n"," [ 0.00149253]]\n","Iter:  4084 loss =  0.008507083964011189 learning rate =  0.5 update =  [[-0.00099548]\n"," [-0.00099548]\n"," [ 0.00149216]]\n","Iter:  4085 loss =  0.008504979972261751 learning rate =  0.5 update =  [[-0.00099523]\n"," [-0.00099523]\n"," [ 0.0014918 ]]\n","Iter:  4086 loss =  0.008502877015173904 learning rate =  0.5 update =  [[-0.00099499]\n"," [-0.00099499]\n"," [ 0.00149143]]\n","Iter:  4087 loss =  0.008500775091987557 learning rate =  0.5 update =  [[-0.00099474]\n"," [-0.00099474]\n"," [ 0.00149106]]\n","Iter:  4088 loss =  0.00849867420194311 learning rate =  0.5 update =  [[-0.0009945]\n"," [-0.0009945]\n"," [ 0.0014907]]\n","Iter:  4089 loss =  0.008496574344281898 learning rate =  0.5 update =  [[-0.00099425]\n"," [-0.00099425]\n"," [ 0.00149033]]\n","Iter:  4090 loss =  0.008494475518245951 learning rate =  0.5 update =  [[-0.00099401]\n"," [-0.00099401]\n"," [ 0.00148996]]\n","Iter:  4091 loss =  0.00849237772307801 learning rate =  0.5 update =  [[-0.00099377]\n"," [-0.00099377]\n"," [ 0.0014896 ]]\n","Iter:  4092 loss =  0.008490280958021528 learning rate =  0.5 update =  [[-0.00099352]\n"," [-0.00099352]\n"," [ 0.00148923]]\n","Iter:  4093 loss =  0.008488185222320848 learning rate =  0.5 update =  [[-0.00099328]\n"," [-0.00099328]\n"," [ 0.00148887]]\n","Iter:  4094 loss =  0.008486090515220824 learning rate =  0.5 update =  [[-0.00099303]\n"," [-0.00099303]\n"," [ 0.0014885 ]]\n","Iter:  4095 loss =  0.00848399683596723 learning rate =  0.5 update =  [[-0.00099279]\n"," [-0.00099279]\n"," [ 0.00148814]]\n","Iter:  4096 loss =  0.00848190418380652 learning rate =  0.5 update =  [[-0.00099255]\n"," [-0.00099255]\n"," [ 0.00148777]]\n","Iter:  4097 loss =  0.008479812557985779 learning rate =  0.5 update =  [[-0.0009923 ]\n"," [-0.0009923 ]\n"," [ 0.00148741]]\n","Iter:  4098 loss =  0.008477721957753039 learning rate =  0.5 update =  [[-0.00099206]\n"," [-0.00099206]\n"," [ 0.00148704]]\n","Iter:  4099 loss =  0.008475632382356898 learning rate =  0.5 update =  [[-0.00099182]\n"," [-0.00099182]\n"," [ 0.00148668]]\n","Iter:  4100 loss =  0.008473543831046728 learning rate =  0.5 update =  [[-0.00099157]\n"," [-0.00099157]\n"," [ 0.00148631]]\n","Iter:  4101 loss =  0.00847145630307256 learning rate =  0.5 update =  [[-0.00099133]\n"," [-0.00099133]\n"," [ 0.00148595]]\n","Iter:  4102 loss =  0.008469369797685393 learning rate =  0.5 update =  [[-0.00099109]\n"," [-0.00099109]\n"," [ 0.00148559]]\n","Iter:  4103 loss =  0.008467284314136687 learning rate =  0.5 update =  [[-0.00099085]\n"," [-0.00099085]\n"," [ 0.00148522]]\n","Iter:  4104 loss =  0.008465199851678673 learning rate =  0.5 update =  [[-0.0009906 ]\n"," [-0.0009906 ]\n"," [ 0.00148486]]\n","Iter:  4105 loss =  0.00846311640956458 learning rate =  0.5 update =  [[-0.00099036]\n"," [-0.00099036]\n"," [ 0.0014845 ]]\n","Iter:  4106 loss =  0.0084610339870479 learning rate =  0.5 update =  [[-0.00099012]\n"," [-0.00099012]\n"," [ 0.00148413]]\n","Iter:  4107 loss =  0.008458952583383344 learning rate =  0.5 update =  [[-0.00098988]\n"," [-0.00098988]\n"," [ 0.00148377]]\n","Iter:  4108 loss =  0.00845687219782592 learning rate =  0.5 update =  [[-0.00098963]\n"," [-0.00098963]\n"," [ 0.00148341]]\n","Iter:  4109 loss =  0.008454792829631737 learning rate =  0.5 update =  [[-0.00098939]\n"," [-0.00098939]\n"," [ 0.00148304]]\n","Iter:  4110 loss =  0.00845271447805724 learning rate =  0.5 update =  [[-0.00098915]\n"," [-0.00098915]\n"," [ 0.00148268]]\n","Iter:  4111 loss =  0.008450637142359881 learning rate =  0.5 update =  [[-0.00098891]\n"," [-0.00098891]\n"," [ 0.00148232]]\n","Iter:  4112 loss =  0.008448560821797804 learning rate =  0.5 update =  [[-0.00098867]\n"," [-0.00098867]\n"," [ 0.00148196]]\n","Iter:  4113 loss =  0.008446485515629707 learning rate =  0.5 update =  [[-0.00098842]\n"," [-0.00098842]\n"," [ 0.0014816 ]]\n","Iter:  4114 loss =  0.008444411223115218 learning rate =  0.5 update =  [[-0.00098818]\n"," [-0.00098818]\n"," [ 0.00148123]]\n","Iter:  4115 loss =  0.008442337943514512 learning rate =  0.5 update =  [[-0.00098794]\n"," [-0.00098794]\n"," [ 0.00148087]]\n","Iter:  4116 loss =  0.008440265676088653 learning rate =  0.5 update =  [[-0.0009877 ]\n"," [-0.0009877 ]\n"," [ 0.00148051]]\n","Iter:  4117 loss =  0.008438194420099215 learning rate =  0.5 update =  [[-0.00098746]\n"," [-0.00098746]\n"," [ 0.00148015]]\n","Iter:  4118 loss =  0.008436124174808634 learning rate =  0.5 update =  [[-0.00098722]\n"," [-0.00098722]\n"," [ 0.00147979]]\n","Iter:  4119 loss =  0.00843405493948 learning rate =  0.5 update =  [[-0.00098698]\n"," [-0.00098698]\n"," [ 0.00147943]]\n","Iter:  4120 loss =  0.008431986713377146 learning rate =  0.5 update =  [[-0.00098674]\n"," [-0.00098674]\n"," [ 0.00147907]]\n","Iter:  4121 loss =  0.008429919495764677 learning rate =  0.5 update =  [[-0.0009865 ]\n"," [-0.0009865 ]\n"," [ 0.00147871]]\n","Iter:  4122 loss =  0.008427853285907715 learning rate =  0.5 update =  [[-0.00098625]\n"," [-0.00098625]\n"," [ 0.00147835]]\n","Iter:  4123 loss =  0.008425788083072407 learning rate =  0.5 update =  [[-0.00098601]\n"," [-0.00098601]\n"," [ 0.00147799]]\n","Iter:  4124 loss =  0.008423723886525245 learning rate =  0.5 update =  [[-0.00098577]\n"," [-0.00098577]\n"," [ 0.00147763]]\n","Iter:  4125 loss =  0.008421660695533694 learning rate =  0.5 update =  [[-0.00098553]\n"," [-0.00098553]\n"," [ 0.00147727]]\n","Iter:  4126 loss =  0.008419598509365845 learning rate =  0.5 update =  [[-0.00098529]\n"," [-0.00098529]\n"," [ 0.00147691]]\n","Iter:  4127 loss =  0.00841753732729053 learning rate =  0.5 update =  [[-0.00098505]\n"," [-0.00098505]\n"," [ 0.00147655]]\n","Iter:  4128 loss =  0.00841547714857719 learning rate =  0.5 update =  [[-0.00098481]\n"," [-0.00098481]\n"," [ 0.00147619]]\n","Iter:  4129 loss =  0.008413417972496085 learning rate =  0.5 update =  [[-0.00098457]\n"," [-0.00098457]\n"," [ 0.00147583]]\n","Iter:  4130 loss =  0.008411359798318106 learning rate =  0.5 update =  [[-0.00098433]\n"," [-0.00098433]\n"," [ 0.00147547]]\n","Iter:  4131 loss =  0.00840930262531492 learning rate =  0.5 update =  [[-0.0009841 ]\n"," [-0.0009841 ]\n"," [ 0.00147511]]\n","Iter:  4132 loss =  0.008407246452758848 learning rate =  0.5 update =  [[-0.00098386]\n"," [-0.00098386]\n"," [ 0.00147475]]\n","Iter:  4133 loss =  0.008405191279922812 learning rate =  0.5 update =  [[-0.00098362]\n"," [-0.00098362]\n"," [ 0.0014744 ]]\n","Iter:  4134 loss =  0.008403137106080759 learning rate =  0.5 update =  [[-0.00098338]\n"," [-0.00098338]\n"," [ 0.00147404]]\n","Iter:  4135 loss =  0.008401083930506929 learning rate =  0.5 update =  [[-0.00098314]\n"," [-0.00098314]\n"," [ 0.00147368]]\n","Iter:  4136 loss =  0.00839903175247661 learning rate =  0.5 update =  [[-0.0009829 ]\n"," [-0.0009829 ]\n"," [ 0.00147332]]\n","Iter:  4137 loss =  0.00839698057126553 learning rate =  0.5 update =  [[-0.00098266]\n"," [-0.00098266]\n"," [ 0.00147296]]\n","Iter:  4138 loss =  0.008394930386150237 learning rate =  0.5 update =  [[-0.00098242]\n"," [-0.00098242]\n"," [ 0.00147261]]\n","Iter:  4139 loss =  0.008392881196408023 learning rate =  0.5 update =  [[-0.00098218]\n"," [-0.00098218]\n"," [ 0.00147225]]\n","Iter:  4140 loss =  0.008390833001316782 learning rate =  0.5 update =  [[-0.00098195]\n"," [-0.00098195]\n"," [ 0.00147189]]\n","Iter:  4141 loss =  0.008388785800155152 learning rate =  0.5 update =  [[-0.00098171]\n"," [-0.00098171]\n"," [ 0.00147154]]\n","Iter:  4142 loss =  0.008386739592202397 learning rate =  0.5 update =  [[-0.00098147]\n"," [-0.00098147]\n"," [ 0.00147118]]\n","Iter:  4143 loss =  0.008384694376738583 learning rate =  0.5 update =  [[-0.00098123]\n"," [-0.00098123]\n"," [ 0.00147082]]\n","Iter:  4144 loss =  0.008382650153044457 learning rate =  0.5 update =  [[-0.00098099]\n"," [-0.00098099]\n"," [ 0.00147047]]\n","Iter:  4145 loss =  0.008380606920401375 learning rate =  0.5 update =  [[-0.00098075]\n"," [-0.00098075]\n"," [ 0.00147011]]\n","Iter:  4146 loss =  0.008378564678091458 learning rate =  0.5 update =  [[-0.00098052]\n"," [-0.00098052]\n"," [ 0.00146975]]\n","Iter:  4147 loss =  0.008376523425397513 learning rate =  0.5 update =  [[-0.00098028]\n"," [-0.00098028]\n"," [ 0.0014694 ]]\n","Iter:  4148 loss =  0.008374483161602891 learning rate =  0.5 update =  [[-0.00098004]\n"," [-0.00098004]\n"," [ 0.00146904]]\n","Iter:  4149 loss =  0.008372443885991971 learning rate =  0.5 update =  [[-0.0009798 ]\n"," [-0.0009798 ]\n"," [ 0.00146869]]\n","Iter:  4150 loss =  0.008370405597849366 learning rate =  0.5 update =  [[-0.00097957]\n"," [-0.00097957]\n"," [ 0.00146833]]\n","Iter:  4151 loss =  0.008368368296460824 learning rate =  0.5 update =  [[-0.00097933]\n"," [-0.00097933]\n"," [ 0.00146797]]\n","Iter:  4152 loss =  0.008366331981112494 learning rate =  0.5 update =  [[-0.00097909]\n"," [-0.00097909]\n"," [ 0.00146762]]\n","Iter:  4153 loss =  0.008364296651091271 learning rate =  0.5 update =  [[-0.00097886]\n"," [-0.00097886]\n"," [ 0.00146726]]\n","Iter:  4154 loss =  0.008362262305684851 learning rate =  0.5 update =  [[-0.00097862]\n"," [-0.00097862]\n"," [ 0.00146691]]\n","Iter:  4155 loss =  0.008360228944181387 learning rate =  0.5 update =  [[-0.00097838]\n"," [-0.00097838]\n"," [ 0.00146656]]\n","Iter:  4156 loss =  0.008358196565870023 learning rate =  0.5 update =  [[-0.00097815]\n"," [-0.00097815]\n"," [ 0.0014662 ]]\n","Iter:  4157 loss =  0.0083561651700402 learning rate =  0.5 update =  [[-0.00097791]\n"," [-0.00097791]\n"," [ 0.00146585]]\n","Iter:  4158 loss =  0.008354134755982415 learning rate =  0.5 update =  [[-0.00097767]\n"," [-0.00097767]\n"," [ 0.00146549]]\n","Iter:  4159 loss =  0.008352105322987672 learning rate =  0.5 update =  [[-0.00097744]\n"," [-0.00097744]\n"," [ 0.00146514]]\n","Iter:  4160 loss =  0.008350076870347637 learning rate =  0.5 update =  [[-0.0009772 ]\n"," [-0.0009772 ]\n"," [ 0.00146478]]\n","Iter:  4161 loss =  0.008348049397354692 learning rate =  0.5 update =  [[-0.00097696]\n"," [-0.00097696]\n"," [ 0.00146443]]\n","Iter:  4162 loss =  0.008346022903301843 learning rate =  0.5 update =  [[-0.00097673]\n"," [-0.00097673]\n"," [ 0.00146408]]\n","Iter:  4163 loss =  0.008343997387482927 learning rate =  0.5 update =  [[-0.00097649]\n"," [-0.00097649]\n"," [ 0.00146372]]\n","Iter:  4164 loss =  0.008341972849192296 learning rate =  0.5 update =  [[-0.00097626]\n"," [-0.00097626]\n"," [ 0.00146337]]\n","Iter:  4165 loss =  0.008339949287725098 learning rate =  0.5 update =  [[-0.00097602]\n"," [-0.00097602]\n"," [ 0.00146302]]\n","Iter:  4166 loss =  0.00833792670237703 learning rate =  0.5 update =  [[-0.00097579]\n"," [-0.00097579]\n"," [ 0.00146267]]\n","Iter:  4167 loss =  0.008335905092444529 learning rate =  0.5 update =  [[-0.00097555]\n"," [-0.00097555]\n"," [ 0.00146231]]\n","Iter:  4168 loss =  0.0083338844572248 learning rate =  0.5 update =  [[-0.00097532]\n"," [-0.00097532]\n"," [ 0.00146196]]\n","Iter:  4169 loss =  0.008331864796015566 learning rate =  0.5 update =  [[-0.00097508]\n"," [-0.00097508]\n"," [ 0.00146161]]\n","Iter:  4170 loss =  0.00832984610811529 learning rate =  0.5 update =  [[-0.00097485]\n"," [-0.00097484]\n"," [ 0.00146126]]\n","Iter:  4171 loss =  0.008327828392823127 learning rate =  0.5 update =  [[-0.00097461]\n"," [-0.00097461]\n"," [ 0.0014609 ]]\n","Iter:  4172 loss =  0.008325811649438799 learning rate =  0.5 update =  [[-0.00097438]\n"," [-0.00097438]\n"," [ 0.00146055]]\n","Iter:  4173 loss =  0.008323795877262964 learning rate =  0.5 update =  [[-0.00097414]\n"," [-0.00097414]\n"," [ 0.0014602 ]]\n","Iter:  4174 loss =  0.008321781075596578 learning rate =  0.5 update =  [[-0.00097391]\n"," [-0.00097391]\n"," [ 0.00145985]]\n","Iter:  4175 loss =  0.008319767243741503 learning rate =  0.5 update =  [[-0.00097367]\n"," [-0.00097367]\n"," [ 0.0014595 ]]\n","Iter:  4176 loss =  0.00831775438100029 learning rate =  0.5 update =  [[-0.00097344]\n"," [-0.00097344]\n"," [ 0.00145915]]\n","Iter:  4177 loss =  0.00831574248667609 learning rate =  0.5 update =  [[-0.0009732]\n"," [-0.0009732]\n"," [ 0.0014588]]\n","Iter:  4178 loss =  0.008313731560072594 learning rate =  0.5 update =  [[-0.00097297]\n"," [-0.00097297]\n"," [ 0.00145845]]\n","Iter:  4179 loss =  0.008311721600494352 learning rate =  0.5 update =  [[-0.00097273]\n"," [-0.00097273]\n"," [ 0.0014581 ]]\n","Iter:  4180 loss =  0.008309712607246514 learning rate =  0.5 update =  [[-0.0009725 ]\n"," [-0.0009725 ]\n"," [ 0.00145775]]\n","Iter:  4181 loss =  0.008307704579634886 learning rate =  0.5 update =  [[-0.00097227]\n"," [-0.00097227]\n"," [ 0.0014574 ]]\n","Iter:  4182 loss =  0.008305697516965962 learning rate =  0.5 update =  [[-0.00097203]\n"," [-0.00097203]\n"," [ 0.00145705]]\n","Iter:  4183 loss =  0.008303691418546803 learning rate =  0.5 update =  [[-0.0009718]\n"," [-0.0009718]\n"," [ 0.0014567]]\n","Iter:  4184 loss =  0.008301686283685245 learning rate =  0.5 update =  [[-0.00097157]\n"," [-0.00097157]\n"," [ 0.00145635]]\n","Iter:  4185 loss =  0.008299682111689724 learning rate =  0.5 update =  [[-0.00097133]\n"," [-0.00097133]\n"," [ 0.001456  ]]\n","Iter:  4186 loss =  0.008297678901869415 learning rate =  0.5 update =  [[-0.0009711 ]\n"," [-0.0009711 ]\n"," [ 0.00145565]]\n","Iter:  4187 loss =  0.008295676653534012 learning rate =  0.5 update =  [[-0.00097087]\n"," [-0.00097087]\n"," [ 0.0014553 ]]\n","Iter:  4188 loss =  0.008293675365994003 learning rate =  0.5 update =  [[-0.00097063]\n"," [-0.00097063]\n"," [ 0.00145495]]\n","Iter:  4189 loss =  0.008291675038560455 learning rate =  0.5 update =  [[-0.0009704]\n"," [-0.0009704]\n"," [ 0.0014546]]\n","Iter:  4190 loss =  0.008289675670545138 learning rate =  0.5 update =  [[-0.00097017]\n"," [-0.00097017]\n"," [ 0.00145425]]\n","Iter:  4191 loss =  0.00828767726126038 learning rate =  0.5 update =  [[-0.00096994]\n"," [-0.00096994]\n"," [ 0.0014539 ]]\n","Iter:  4192 loss =  0.008285679810019323 learning rate =  0.5 update =  [[-0.0009697 ]\n"," [-0.0009697 ]\n"," [ 0.00145355]]\n","Iter:  4193 loss =  0.008283683316135574 learning rate =  0.5 update =  [[-0.00096947]\n"," [-0.00096947]\n"," [ 0.00145321]]\n","Iter:  4194 loss =  0.008281687778923623 learning rate =  0.5 update =  [[-0.00096924]\n"," [-0.00096924]\n"," [ 0.00145286]]\n","Iter:  4195 loss =  0.00827969319769836 learning rate =  0.5 update =  [[-0.00096901]\n"," [-0.00096901]\n"," [ 0.00145251]]\n","Iter:  4196 loss =  0.008277699571775616 learning rate =  0.5 update =  [[-0.00096877]\n"," [-0.00096877]\n"," [ 0.00145216]]\n","Iter:  4197 loss =  0.00827570690047157 learning rate =  0.5 update =  [[-0.00096854]\n"," [-0.00096854]\n"," [ 0.00145181]]\n","Iter:  4198 loss =  0.008273715183103224 learning rate =  0.5 update =  [[-0.00096831]\n"," [-0.00096831]\n"," [ 0.00145147]]\n","Iter:  4199 loss =  0.008271724418988241 learning rate =  0.5 update =  [[-0.00096808]\n"," [-0.00096808]\n"," [ 0.00145112]]\n","Iter:  4200 loss =  0.00826973460744482 learning rate =  0.5 update =  [[-0.00096785]\n"," [-0.00096785]\n"," [ 0.00145077]]\n","Iter:  4201 loss =  0.008267745747791965 learning rate =  0.5 update =  [[-0.00096761]\n"," [-0.00096761]\n"," [ 0.00145043]]\n","Iter:  4202 loss =  0.00826575783934917 learning rate =  0.5 update =  [[-0.00096738]\n"," [-0.00096738]\n"," [ 0.00145008]]\n","Iter:  4203 loss =  0.008263770881436747 learning rate =  0.5 update =  [[-0.00096715]\n"," [-0.00096715]\n"," [ 0.00144973]]\n","Iter:  4204 loss =  0.008261784873375385 learning rate =  0.5 update =  [[-0.00096692]\n"," [-0.00096692]\n"," [ 0.00144939]]\n","Iter:  4205 loss =  0.008259799814486745 learning rate =  0.5 update =  [[-0.00096669]\n"," [-0.00096669]\n"," [ 0.00144904]]\n","Iter:  4206 loss =  0.008257815704092859 learning rate =  0.5 update =  [[-0.00096646]\n"," [-0.00096646]\n"," [ 0.00144869]]\n","Iter:  4207 loss =  0.008255832541516638 learning rate =  0.5 update =  [[-0.00096623]\n"," [-0.00096623]\n"," [ 0.00144835]]\n","Iter:  4208 loss =  0.008253850326081377 learning rate =  0.5 update =  [[-0.000966]\n"," [-0.000966]\n"," [ 0.001448]]\n","Iter:  4209 loss =  0.008251869057111326 learning rate =  0.5 update =  [[-0.00096577]\n"," [-0.00096577]\n"," [ 0.00144766]]\n","Iter:  4210 loss =  0.008249888733931034 learning rate =  0.5 update =  [[-0.00096554]\n"," [-0.00096554]\n"," [ 0.00144731]]\n","Iter:  4211 loss =  0.008247909355865876 learning rate =  0.5 update =  [[-0.0009653 ]\n"," [-0.0009653 ]\n"," [ 0.00144697]]\n","Iter:  4212 loss =  0.008245930922241986 learning rate =  0.5 update =  [[-0.00096507]\n"," [-0.00096507]\n"," [ 0.00144662]]\n","Iter:  4213 loss =  0.008243953432385834 learning rate =  0.5 update =  [[-0.00096484]\n"," [-0.00096484]\n"," [ 0.00144628]]\n","Iter:  4214 loss =  0.008241976885624811 learning rate =  0.5 update =  [[-0.00096461]\n"," [-0.00096461]\n"," [ 0.00144593]]\n","Iter:  4215 loss =  0.008240001281286777 learning rate =  0.5 update =  [[-0.00096438]\n"," [-0.00096438]\n"," [ 0.00144559]]\n","Iter:  4216 loss =  0.008238026618700302 learning rate =  0.5 update =  [[-0.00096415]\n"," [-0.00096415]\n"," [ 0.00144524]]\n","Iter:  4217 loss =  0.008236052897194559 learning rate =  0.5 update =  [[-0.00096392]\n"," [-0.00096392]\n"," [ 0.0014449 ]]\n","Iter:  4218 loss =  0.008234080116099374 learning rate =  0.5 update =  [[-0.00096369]\n"," [-0.00096369]\n"," [ 0.00144455]]\n","Iter:  4219 loss =  0.008232108274745175 learning rate =  0.5 update =  [[-0.00096347]\n"," [-0.00096347]\n"," [ 0.00144421]]\n","Iter:  4220 loss =  0.008230137372463128 learning rate =  0.5 update =  [[-0.00096324]\n"," [-0.00096324]\n"," [ 0.00144387]]\n","Iter:  4221 loss =  0.008228167408584893 learning rate =  0.5 update =  [[-0.00096301]\n"," [-0.00096301]\n"," [ 0.00144352]]\n","Iter:  4222 loss =  0.008226198382442755 learning rate =  0.5 update =  [[-0.00096278]\n"," [-0.00096278]\n"," [ 0.00144318]]\n","Iter:  4223 loss =  0.008224230293369822 learning rate =  0.5 update =  [[-0.00096255]\n"," [-0.00096255]\n"," [ 0.00144284]]\n","Iter:  4224 loss =  0.008222263140699662 learning rate =  0.5 update =  [[-0.00096232]\n"," [-0.00096232]\n"," [ 0.00144249]]\n","Iter:  4225 loss =  0.008220296923766562 learning rate =  0.5 update =  [[-0.00096209]\n"," [-0.00096209]\n"," [ 0.00144215]]\n","Iter:  4226 loss =  0.00821833164190523 learning rate =  0.5 update =  [[-0.00096186]\n"," [-0.00096186]\n"," [ 0.00144181]]\n","Iter:  4227 loss =  0.008216367294451432 learning rate =  0.5 update =  [[-0.00096163]\n"," [-0.00096163]\n"," [ 0.00144147]]\n","Iter:  4228 loss =  0.008214403880741026 learning rate =  0.5 update =  [[-0.0009614 ]\n"," [-0.0009614 ]\n"," [ 0.00144112]]\n","Iter:  4229 loss =  0.00821244140011106 learning rate =  0.5 update =  [[-0.00096117]\n"," [-0.00096117]\n"," [ 0.00144078]]\n","Iter:  4230 loss =  0.008210479851898704 learning rate =  0.5 update =  [[-0.00096095]\n"," [-0.00096095]\n"," [ 0.00144044]]\n","Iter:  4231 loss =  0.00820851923544198 learning rate =  0.5 update =  [[-0.00096072]\n"," [-0.00096072]\n"," [ 0.0014401 ]]\n","Iter:  4232 loss =  0.008206559550079626 learning rate =  0.5 update =  [[-0.00096049]\n"," [-0.00096049]\n"," [ 0.00143975]]\n","Iter:  4233 loss =  0.008204600795150839 learning rate =  0.5 update =  [[-0.00096026]\n"," [-0.00096026]\n"," [ 0.00143941]]\n","Iter:  4234 loss =  0.008202642969995521 learning rate =  0.5 update =  [[-0.00096003]\n"," [-0.00096003]\n"," [ 0.00143907]]\n","Iter:  4235 loss =  0.008200686073954128 learning rate =  0.5 update =  [[-0.00095981]\n"," [-0.00095981]\n"," [ 0.00143873]]\n","Iter:  4236 loss =  0.008198730106367796 learning rate =  0.5 update =  [[-0.00095958]\n"," [-0.00095958]\n"," [ 0.00143839]]\n","Iter:  4237 loss =  0.008196775066578373 learning rate =  0.5 update =  [[-0.00095935]\n"," [-0.00095935]\n"," [ 0.00143805]]\n","Iter:  4238 loss =  0.008194820953928192 learning rate =  0.5 update =  [[-0.00095912]\n"," [-0.00095912]\n"," [ 0.00143771]]\n","Iter:  4239 loss =  0.00819286776776011 learning rate =  0.5 update =  [[-0.0009589 ]\n"," [-0.0009589 ]\n"," [ 0.00143737]]\n","Iter:  4240 loss =  0.008190915507417885 learning rate =  0.5 update =  [[-0.00095867]\n"," [-0.00095867]\n"," [ 0.00143703]]\n","Iter:  4241 loss =  0.00818896417224568 learning rate =  0.5 update =  [[-0.00095844]\n"," [-0.00095844]\n"," [ 0.00143669]]\n","Iter:  4242 loss =  0.008187013761588344 learning rate =  0.5 update =  [[-0.00095821]\n"," [-0.00095821]\n"," [ 0.00143635]]\n","Iter:  4243 loss =  0.00818506427479138 learning rate =  0.5 update =  [[-0.00095799]\n"," [-0.00095799]\n"," [ 0.001436  ]]\n","Iter:  4244 loss =  0.008183115711200811 learning rate =  0.5 update =  [[-0.00095776]\n"," [-0.00095776]\n"," [ 0.00143567]]\n","Iter:  4245 loss =  0.008181168070163312 learning rate =  0.5 update =  [[-0.00095753]\n"," [-0.00095753]\n"," [ 0.00143533]]\n","Iter:  4246 loss =  0.008179221351026267 learning rate =  0.5 update =  [[-0.00095731]\n"," [-0.00095731]\n"," [ 0.00143499]]\n","Iter:  4247 loss =  0.008177275553137528 learning rate =  0.5 update =  [[-0.00095708]\n"," [-0.00095708]\n"," [ 0.00143465]]\n","Iter:  4248 loss =  0.008175330675845598 learning rate =  0.5 update =  [[-0.00095685]\n"," [-0.00095685]\n"," [ 0.00143431]]\n","Iter:  4249 loss =  0.00817338671849975 learning rate =  0.5 update =  [[-0.00095663]\n"," [-0.00095663]\n"," [ 0.00143397]]\n","Iter:  4250 loss =  0.008171443680449655 learning rate =  0.5 update =  [[-0.0009564 ]\n"," [-0.0009564 ]\n"," [ 0.00143363]]\n","Iter:  4251 loss =  0.008169501561045676 learning rate =  0.5 update =  [[-0.00095617]\n"," [-0.00095617]\n"," [ 0.00143329]]\n","Iter:  4252 loss =  0.008167560359638854 learning rate =  0.5 update =  [[-0.00095595]\n"," [-0.00095595]\n"," [ 0.00143295]]\n","Iter:  4253 loss =  0.008165620075580691 learning rate =  0.5 update =  [[-0.00095572]\n"," [-0.00095572]\n"," [ 0.00143261]]\n","Iter:  4254 loss =  0.008163680708223407 learning rate =  0.5 update =  [[-0.0009555 ]\n"," [-0.0009555 ]\n"," [ 0.00143227]]\n","Iter:  4255 loss =  0.008161742256919893 learning rate =  0.5 update =  [[-0.00095527]\n"," [-0.00095527]\n"," [ 0.00143194]]\n","Iter:  4256 loss =  0.008159804721023426 learning rate =  0.5 update =  [[-0.00095505]\n"," [-0.00095505]\n"," [ 0.0014316 ]]\n","Iter:  4257 loss =  0.008157868099888079 learning rate =  0.5 update =  [[-0.00095482]\n"," [-0.00095482]\n"," [ 0.00143126]]\n","Iter:  4258 loss =  0.0081559323928686 learning rate =  0.5 update =  [[-0.00095459]\n"," [-0.00095459]\n"," [ 0.00143092]]\n","Iter:  4259 loss =  0.008153997599320064 learning rate =  0.5 update =  [[-0.00095437]\n"," [-0.00095437]\n"," [ 0.00143059]]\n","Iter:  4260 loss =  0.008152063718598342 learning rate =  0.5 update =  [[-0.00095414]\n"," [-0.00095414]\n"," [ 0.00143025]]\n","Iter:  4261 loss =  0.008150130750059955 learning rate =  0.5 update =  [[-0.00095392]\n"," [-0.00095392]\n"," [ 0.00142991]]\n","Iter:  4262 loss =  0.008148198693061805 learning rate =  0.5 update =  [[-0.00095369]\n"," [-0.00095369]\n"," [ 0.00142957]]\n","Iter:  4263 loss =  0.008146267546961673 learning rate =  0.5 update =  [[-0.00095347]\n"," [-0.00095347]\n"," [ 0.00142924]]\n","Iter:  4264 loss =  0.00814433731111774 learning rate =  0.5 update =  [[-0.00095324]\n"," [-0.00095324]\n"," [ 0.0014289 ]]\n","Iter:  4265 loss =  0.008142407984888817 learning rate =  0.5 update =  [[-0.00095302]\n"," [-0.00095302]\n"," [ 0.00142856]]\n","Iter:  4266 loss =  0.008140479567634426 learning rate =  0.5 update =  [[-0.0009528 ]\n"," [-0.0009528 ]\n"," [ 0.00142823]]\n","Iter:  4267 loss =  0.008138552058714579 learning rate =  0.5 update =  [[-0.00095257]\n"," [-0.00095257]\n"," [ 0.00142789]]\n","Iter:  4268 loss =  0.008136625457489995 learning rate =  0.5 update =  [[-0.00095235]\n"," [-0.00095235]\n"," [ 0.00142756]]\n","Iter:  4269 loss =  0.008134699763321773 learning rate =  0.5 update =  [[-0.00095212]\n"," [-0.00095212]\n"," [ 0.00142722]]\n","Iter:  4270 loss =  0.008132774975571888 learning rate =  0.5 update =  [[-0.0009519 ]\n"," [-0.0009519 ]\n"," [ 0.00142688]]\n","Iter:  4271 loss =  0.008130851093602806 learning rate =  0.5 update =  [[-0.00095167]\n"," [-0.00095167]\n"," [ 0.00142655]]\n","Iter:  4272 loss =  0.00812892811677739 learning rate =  0.5 update =  [[-0.00095145]\n"," [-0.00095145]\n"," [ 0.00142621]]\n","Iter:  4273 loss =  0.008127006044459418 learning rate =  0.5 update =  [[-0.00095123]\n"," [-0.00095123]\n"," [ 0.00142588]]\n","Iter:  4274 loss =  0.008125084876013041 learning rate =  0.5 update =  [[-0.000951  ]\n"," [-0.000951  ]\n"," [ 0.00142554]]\n","Iter:  4275 loss =  0.008123164610803146 learning rate =  0.5 update =  [[-0.00095078]\n"," [-0.00095078]\n"," [ 0.00142521]]\n","Iter:  4276 loss =  0.00812124524819511 learning rate =  0.5 update =  [[-0.00095056]\n"," [-0.00095056]\n"," [ 0.00142487]]\n","Iter:  4277 loss =  0.008119326787554912 learning rate =  0.5 update =  [[-0.00095033]\n"," [-0.00095033]\n"," [ 0.00142454]]\n","Iter:  4278 loss =  0.00811740922824921 learning rate =  0.5 update =  [[-0.00095011]\n"," [-0.00095011]\n"," [ 0.0014242 ]]\n","Iter:  4279 loss =  0.008115492569645206 learning rate =  0.5 update =  [[-0.00094989]\n"," [-0.00094989]\n"," [ 0.00142387]]\n","Iter:  4280 loss =  0.008113576811110593 learning rate =  0.5 update =  [[-0.00094966]\n"," [-0.00094966]\n"," [ 0.00142353]]\n","Iter:  4281 loss =  0.008111661952013825 learning rate =  0.5 update =  [[-0.00094944]\n"," [-0.00094944]\n"," [ 0.0014232 ]]\n","Iter:  4282 loss =  0.008109747991723822 learning rate =  0.5 update =  [[-0.00094922]\n"," [-0.00094922]\n"," [ 0.00142287]]\n","Iter:  4283 loss =  0.008107834929610154 learning rate =  0.5 update =  [[-0.00094899]\n"," [-0.00094899]\n"," [ 0.00142253]]\n","Iter:  4284 loss =  0.008105922765042939 learning rate =  0.5 update =  [[-0.00094877]\n"," [-0.00094877]\n"," [ 0.0014222 ]]\n","Iter:  4285 loss =  0.008104011497392944 learning rate =  0.5 update =  [[-0.00094855]\n"," [-0.00094855]\n"," [ 0.00142187]]\n","Iter:  4286 loss =  0.008102101126031457 learning rate =  0.5 update =  [[-0.00094833]\n"," [-0.00094833]\n"," [ 0.00142153]]\n","Iter:  4287 loss =  0.008100191650330308 learning rate =  0.5 update =  [[-0.0009481]\n"," [-0.0009481]\n"," [ 0.0014212]]\n","Iter:  4288 loss =  0.008098283069662093 learning rate =  0.5 update =  [[-0.00094788]\n"," [-0.00094788]\n"," [ 0.00142087]]\n","Iter:  4289 loss =  0.00809637538339981 learning rate =  0.5 update =  [[-0.00094766]\n"," [-0.00094766]\n"," [ 0.00142053]]\n","Iter:  4290 loss =  0.008094468590917111 learning rate =  0.5 update =  [[-0.00094744]\n"," [-0.00094744]\n"," [ 0.0014202 ]]\n","Iter:  4291 loss =  0.008092562691588256 learning rate =  0.5 update =  [[-0.00094721]\n"," [-0.00094721]\n"," [ 0.00141987]]\n","Iter:  4292 loss =  0.008090657684788036 learning rate =  0.5 update =  [[-0.00094699]\n"," [-0.00094699]\n"," [ 0.00141954]]\n","Iter:  4293 loss =  0.008088753569891879 learning rate =  0.5 update =  [[-0.00094677]\n"," [-0.00094677]\n"," [ 0.0014192 ]]\n","Iter:  4294 loss =  0.008086850346275721 learning rate =  0.5 update =  [[-0.00094655]\n"," [-0.00094655]\n"," [ 0.00141887]]\n","Iter:  4295 loss =  0.008084948013316186 learning rate =  0.5 update =  [[-0.00094633]\n"," [-0.00094633]\n"," [ 0.00141854]]\n","Iter:  4296 loss =  0.008083046570390353 learning rate =  0.5 update =  [[-0.00094611]\n"," [-0.00094611]\n"," [ 0.00141821]]\n","Iter:  4297 loss =  0.008081146016875932 learning rate =  0.5 update =  [[-0.00094589]\n"," [-0.00094589]\n"," [ 0.00141788]]\n","Iter:  4298 loss =  0.008079246352151284 learning rate =  0.5 update =  [[-0.00094566]\n"," [-0.00094566]\n"," [ 0.00141755]]\n","Iter:  4299 loss =  0.008077347575595289 learning rate =  0.5 update =  [[-0.00094544]\n"," [-0.00094544]\n"," [ 0.00141721]]\n","Iter:  4300 loss =  0.008075449686587282 learning rate =  0.5 update =  [[-0.00094522]\n"," [-0.00094522]\n"," [ 0.00141688]]\n","Iter:  4301 loss =  0.008073552684507426 learning rate =  0.5 update =  [[-0.000945  ]\n"," [-0.000945  ]\n"," [ 0.00141655]]\n","Iter:  4302 loss =  0.00807165656873634 learning rate =  0.5 update =  [[-0.00094478]\n"," [-0.00094478]\n"," [ 0.00141622]]\n","Iter:  4303 loss =  0.00806976133865504 learning rate =  0.5 update =  [[-0.00094456]\n"," [-0.00094456]\n"," [ 0.00141589]]\n","Iter:  4304 loss =  0.008067866993645348 learning rate =  0.5 update =  [[-0.00094434]\n"," [-0.00094434]\n"," [ 0.00141556]]\n","Iter:  4305 loss =  0.008065973533089625 learning rate =  0.5 update =  [[-0.00094412]\n"," [-0.00094412]\n"," [ 0.00141523]]\n","Iter:  4306 loss =  0.008064080956370852 learning rate =  0.5 update =  [[-0.0009439]\n"," [-0.0009439]\n"," [ 0.0014149]]\n","Iter:  4307 loss =  0.008062189262872395 learning rate =  0.5 update =  [[-0.00094368]\n"," [-0.00094368]\n"," [ 0.00141457]]\n","Iter:  4308 loss =  0.00806029845197824 learning rate =  0.5 update =  [[-0.00094346]\n"," [-0.00094346]\n"," [ 0.00141424]]\n","Iter:  4309 loss =  0.008058408523073199 learning rate =  0.5 update =  [[-0.00094324]\n"," [-0.00094324]\n"," [ 0.00141391]]\n","Iter:  4310 loss =  0.008056519475542236 learning rate =  0.5 update =  [[-0.00094302]\n"," [-0.00094302]\n"," [ 0.00141358]]\n","Iter:  4311 loss =  0.00805463130877127 learning rate =  0.5 update =  [[-0.0009428 ]\n"," [-0.0009428 ]\n"," [ 0.00141325]]\n","Iter:  4312 loss =  0.008052744022146579 learning rate =  0.5 update =  [[-0.00094258]\n"," [-0.00094258]\n"," [ 0.00141292]]\n","Iter:  4313 loss =  0.008050857615055 learning rate =  0.5 update =  [[-0.00094236]\n"," [-0.00094236]\n"," [ 0.00141259]]\n","Iter:  4314 loss =  0.008048972086884088 learning rate =  0.5 update =  [[-0.00094214]\n"," [-0.00094214]\n"," [ 0.00141226]]\n","Iter:  4315 loss =  0.008047087437021773 learning rate =  0.5 update =  [[-0.00094192]\n"," [-0.00094192]\n"," [ 0.00141194]]\n","Iter:  4316 loss =  0.008045203664856717 learning rate =  0.5 update =  [[-0.0009417 ]\n"," [-0.0009417 ]\n"," [ 0.00141161]]\n","Iter:  4317 loss =  0.008043320769778078 learning rate =  0.5 update =  [[-0.00094148]\n"," [-0.00094148]\n"," [ 0.00141128]]\n","Iter:  4318 loss =  0.008041438751175602 learning rate =  0.5 update =  [[-0.00094126]\n"," [-0.00094126]\n"," [ 0.00141095]]\n","Iter:  4319 loss =  0.008039557608439534 learning rate =  0.5 update =  [[-0.00094104]\n"," [-0.00094104]\n"," [ 0.00141062]]\n","Iter:  4320 loss =  0.008037677340960763 learning rate =  0.5 update =  [[-0.00094082]\n"," [-0.00094082]\n"," [ 0.00141029]]\n","Iter:  4321 loss =  0.008035797948130642 learning rate =  0.5 update =  [[-0.0009406 ]\n"," [-0.0009406 ]\n"," [ 0.00140997]]\n","Iter:  4322 loss =  0.008033919429341263 learning rate =  0.5 update =  [[-0.00094039]\n"," [-0.00094038]\n"," [ 0.00140964]]\n","Iter:  4323 loss =  0.00803204178398503 learning rate =  0.5 update =  [[-0.00094017]\n"," [-0.00094017]\n"," [ 0.00140931]]\n","Iter:  4324 loss =  0.008030165011455207 learning rate =  0.5 update =  [[-0.00093995]\n"," [-0.00093995]\n"," [ 0.00140898]]\n","Iter:  4325 loss =  0.008028289111145318 learning rate =  0.5 update =  [[-0.00093973]\n"," [-0.00093973]\n"," [ 0.00140866]]\n","Iter:  4326 loss =  0.00802641408244973 learning rate =  0.5 update =  [[-0.00093951]\n"," [-0.00093951]\n"," [ 0.00140833]]\n","Iter:  4327 loss =  0.008024539924763026 learning rate =  0.5 update =  [[-0.00093929]\n"," [-0.00093929]\n"," [ 0.001408  ]]\n","Iter:  4328 loss =  0.008022666637480746 learning rate =  0.5 update =  [[-0.00093907]\n"," [-0.00093907]\n"," [ 0.00140767]]\n","Iter:  4329 loss =  0.008020794219998696 learning rate =  0.5 update =  [[-0.00093886]\n"," [-0.00093886]\n"," [ 0.00140735]]\n","Iter:  4330 loss =  0.008018922671713363 learning rate =  0.5 update =  [[-0.00093864]\n"," [-0.00093864]\n"," [ 0.00140702]]\n","Iter:  4331 loss =  0.008017051992021774 learning rate =  0.5 update =  [[-0.00093842]\n"," [-0.00093842]\n"," [ 0.0014067 ]]\n","Iter:  4332 loss =  0.008015182180321392 learning rate =  0.5 update =  [[-0.0009382 ]\n"," [-0.0009382 ]\n"," [ 0.00140637]]\n","Iter:  4333 loss =  0.008013313236010525 learning rate =  0.5 update =  [[-0.00093799]\n"," [-0.00093798]\n"," [ 0.00140604]]\n","Iter:  4334 loss =  0.008011445158487717 learning rate =  0.5 update =  [[-0.00093777]\n"," [-0.00093777]\n"," [ 0.00140572]]\n","Iter:  4335 loss =  0.00800957794715217 learning rate =  0.5 update =  [[-0.00093755]\n"," [-0.00093755]\n"," [ 0.00140539]]\n","Iter:  4336 loss =  0.008007711601403874 learning rate =  0.5 update =  [[-0.00093733]\n"," [-0.00093733]\n"," [ 0.00140507]]\n","Iter:  4337 loss =  0.00800584612064303 learning rate =  0.5 update =  [[-0.00093712]\n"," [-0.00093712]\n"," [ 0.00140474]]\n","Iter:  4338 loss =  0.008003981504270468 learning rate =  0.5 update =  [[-0.0009369 ]\n"," [-0.0009369 ]\n"," [ 0.00140441]]\n","Iter:  4339 loss =  0.008002117751687774 learning rate =  0.5 update =  [[-0.00093668]\n"," [-0.00093668]\n"," [ 0.00140409]]\n","Iter:  4340 loss =  0.00800025486229692 learning rate =  0.5 update =  [[-0.00093646]\n"," [-0.00093646]\n"," [ 0.00140376]]\n","Iter:  4341 loss =  0.007998392835500384 learning rate =  0.5 update =  [[-0.00093625]\n"," [-0.00093625]\n"," [ 0.00140344]]\n","Iter:  4342 loss =  0.007996531670701305 learning rate =  0.5 update =  [[-0.00093603]\n"," [-0.00093603]\n"," [ 0.00140311]]\n","Iter:  4343 loss =  0.007994671367303328 learning rate =  0.5 update =  [[-0.00093581]\n"," [-0.00093581]\n"," [ 0.00140279]]\n","Iter:  4344 loss =  0.007992811924710623 learning rate =  0.5 update =  [[-0.0009356 ]\n"," [-0.0009356 ]\n"," [ 0.00140247]]\n","Iter:  4345 loss =  0.007990953342328088 learning rate =  0.5 update =  [[-0.00093538]\n"," [-0.00093538]\n"," [ 0.00140214]]\n","Iter:  4346 loss =  0.007989095619560722 learning rate =  0.5 update =  [[-0.00093516]\n"," [-0.00093516]\n"," [ 0.00140182]]\n","Iter:  4347 loss =  0.007987238755814651 learning rate =  0.5 update =  [[-0.00093495]\n"," [-0.00093495]\n"," [ 0.00140149]]\n","Iter:  4348 loss =  0.00798538275049604 learning rate =  0.5 update =  [[-0.00093473]\n"," [-0.00093473]\n"," [ 0.00140117]]\n","Iter:  4349 loss =  0.007983527603012016 learning rate =  0.5 update =  [[-0.00093452]\n"," [-0.00093452]\n"," [ 0.00140085]]\n","Iter:  4350 loss =  0.007981673312769857 learning rate =  0.5 update =  [[-0.0009343 ]\n"," [-0.0009343 ]\n"," [ 0.00140052]]\n","Iter:  4351 loss =  0.007979819879177775 learning rate =  0.5 update =  [[-0.00093408]\n"," [-0.00093408]\n"," [ 0.0014002 ]]\n","Iter:  4352 loss =  0.007977967301644157 learning rate =  0.5 update =  [[-0.00093387]\n"," [-0.00093387]\n"," [ 0.00139988]]\n","Iter:  4353 loss =  0.007976115579578249 learning rate =  0.5 update =  [[-0.00093365]\n"," [-0.00093365]\n"," [ 0.00139955]]\n","Iter:  4354 loss =  0.007974264712389576 learning rate =  0.5 update =  [[-0.00093344]\n"," [-0.00093344]\n"," [ 0.00139923]]\n","Iter:  4355 loss =  0.007972414699488405 learning rate =  0.5 update =  [[-0.00093322]\n"," [-0.00093322]\n"," [ 0.00139891]]\n","Iter:  4356 loss =  0.007970565540285468 learning rate =  0.5 update =  [[-0.00093301]\n"," [-0.00093301]\n"," [ 0.00139858]]\n","Iter:  4357 loss =  0.007968717234191946 learning rate =  0.5 update =  [[-0.00093279]\n"," [-0.00093279]\n"," [ 0.00139826]]\n","Iter:  4358 loss =  0.007966869780619704 learning rate =  0.5 update =  [[-0.00093258]\n"," [-0.00093258]\n"," [ 0.00139794]]\n","Iter:  4359 loss =  0.00796502317898118 learning rate =  0.5 update =  [[-0.00093236]\n"," [-0.00093236]\n"," [ 0.00139762]]\n","Iter:  4360 loss =  0.007963177428689159 learning rate =  0.5 update =  [[-0.00093215]\n"," [-0.00093215]\n"," [ 0.0013973 ]]\n","Iter:  4361 loss =  0.007961332529157074 learning rate =  0.5 update =  [[-0.00093193]\n"," [-0.00093193]\n"," [ 0.00139697]]\n","Iter:  4362 loss =  0.007959488479798906 learning rate =  0.5 update =  [[-0.00093172]\n"," [-0.00093172]\n"," [ 0.00139665]]\n","Iter:  4363 loss =  0.007957645280029171 learning rate =  0.5 update =  [[-0.0009315 ]\n"," [-0.0009315 ]\n"," [ 0.00139633]]\n","Iter:  4364 loss =  0.007955802929262854 learning rate =  0.5 update =  [[-0.00093129]\n"," [-0.00093129]\n"," [ 0.00139601]]\n","Iter:  4365 loss =  0.007953961426915613 learning rate =  0.5 update =  [[-0.00093107]\n"," [-0.00093107]\n"," [ 0.00139569]]\n","Iter:  4366 loss =  0.007952120772403452 learning rate =  0.5 update =  [[-0.00093086]\n"," [-0.00093086]\n"," [ 0.00139537]]\n","Iter:  4367 loss =  0.007950280965143061 learning rate =  0.5 update =  [[-0.00093064]\n"," [-0.00093064]\n"," [ 0.00139505]]\n","Iter:  4368 loss =  0.007948442004551614 learning rate =  0.5 update =  [[-0.00093043]\n"," [-0.00093043]\n"," [ 0.00139472]]\n","Iter:  4369 loss =  0.007946603890046857 learning rate =  0.5 update =  [[-0.00093021]\n"," [-0.00093021]\n"," [ 0.0013944 ]]\n","Iter:  4370 loss =  0.007944766621046963 learning rate =  0.5 update =  [[-0.00093   ]\n"," [-0.00093   ]\n"," [ 0.00139408]]\n","Iter:  4371 loss =  0.007942930196970732 learning rate =  0.5 update =  [[-0.00092979]\n"," [-0.00092979]\n"," [ 0.00139376]]\n","Iter:  4372 loss =  0.007941094617237477 learning rate =  0.5 update =  [[-0.00092957]\n"," [-0.00092957]\n"," [ 0.00139344]]\n","Iter:  4373 loss =  0.007939259881266971 learning rate =  0.5 update =  [[-0.00092936]\n"," [-0.00092936]\n"," [ 0.00139312]]\n","Iter:  4374 loss =  0.007937425988479638 learning rate =  0.5 update =  [[-0.00092915]\n"," [-0.00092915]\n"," [ 0.0013928 ]]\n","Iter:  4375 loss =  0.007935592938296447 learning rate =  0.5 update =  [[-0.00092893]\n"," [-0.00092893]\n"," [ 0.00139248]]\n","Iter:  4376 loss =  0.007933760730138652 learning rate =  0.5 update =  [[-0.00092872]\n"," [-0.00092872]\n"," [ 0.00139216]]\n","Iter:  4377 loss =  0.007931929363428309 learning rate =  0.5 update =  [[-0.00092851]\n"," [-0.00092851]\n"," [ 0.00139184]]\n","Iter:  4378 loss =  0.007930098837587838 learning rate =  0.5 update =  [[-0.00092829]\n"," [-0.00092829]\n"," [ 0.00139152]]\n","Iter:  4379 loss =  0.007928269152040347 learning rate =  0.5 update =  [[-0.00092808]\n"," [-0.00092808]\n"," [ 0.0013912 ]]\n","Iter:  4380 loss =  0.007926440306209261 learning rate =  0.5 update =  [[-0.00092787]\n"," [-0.00092787]\n"," [ 0.00139089]]\n","Iter:  4381 loss =  0.007924612299518657 learning rate =  0.5 update =  [[-0.00092765]\n"," [-0.00092765]\n"," [ 0.00139057]]\n","Iter:  4382 loss =  0.007922785131393127 learning rate =  0.5 update =  [[-0.00092744]\n"," [-0.00092744]\n"," [ 0.00139025]]\n","Iter:  4383 loss =  0.007920958801257778 learning rate =  0.5 update =  [[-0.00092723]\n"," [-0.00092723]\n"," [ 0.00138993]]\n","Iter:  4384 loss =  0.007919133308538254 learning rate =  0.5 update =  [[-0.00092702]\n"," [-0.00092702]\n"," [ 0.00138961]]\n","Iter:  4385 loss =  0.007917308652660717 learning rate =  0.5 update =  [[-0.0009268 ]\n"," [-0.0009268 ]\n"," [ 0.00138929]]\n","Iter:  4386 loss =  0.007915484833051784 learning rate =  0.5 update =  [[-0.00092659]\n"," [-0.00092659]\n"," [ 0.00138897]]\n","Iter:  4387 loss =  0.007913661849138758 learning rate =  0.5 update =  [[-0.00092638]\n"," [-0.00092638]\n"," [ 0.00138866]]\n","Iter:  4388 loss =  0.00791183970034931 learning rate =  0.5 update =  [[-0.00092617]\n"," [-0.00092617]\n"," [ 0.00138834]]\n","Iter:  4389 loss =  0.0079100183861116 learning rate =  0.5 update =  [[-0.00092595]\n"," [-0.00092595]\n"," [ 0.00138802]]\n","Iter:  4390 loss =  0.007908197905854472 learning rate =  0.5 update =  [[-0.00092574]\n"," [-0.00092574]\n"," [ 0.0013877 ]]\n","Iter:  4391 loss =  0.007906378259007304 learning rate =  0.5 update =  [[-0.00092553]\n"," [-0.00092553]\n"," [ 0.00138738]]\n","Iter:  4392 loss =  0.007904559444999711 learning rate =  0.5 update =  [[-0.00092532]\n"," [-0.00092532]\n"," [ 0.00138707]]\n","Iter:  4393 loss =  0.00790274146326208 learning rate =  0.5 update =  [[-0.00092511]\n"," [-0.00092511]\n"," [ 0.00138675]]\n","Iter:  4394 loss =  0.007900924313225272 learning rate =  0.5 update =  [[-0.00092489]\n"," [-0.00092489]\n"," [ 0.00138643]]\n","Iter:  4395 loss =  0.007899107994320701 learning rate =  0.5 update =  [[-0.00092468]\n"," [-0.00092468]\n"," [ 0.00138612]]\n","Iter:  4396 loss =  0.007897292505980147 learning rate =  0.5 update =  [[-0.00092447]\n"," [-0.00092447]\n"," [ 0.0013858 ]]\n","Iter:  4397 loss =  0.00789547784763596 learning rate =  0.5 update =  [[-0.00092426]\n"," [-0.00092426]\n"," [ 0.00138548]]\n","Iter:  4398 loss =  0.00789366401872112 learning rate =  0.5 update =  [[-0.00092405]\n"," [-0.00092405]\n"," [ 0.00138517]]\n","Iter:  4399 loss =  0.007891851018669115 learning rate =  0.5 update =  [[-0.00092384]\n"," [-0.00092384]\n"," [ 0.00138485]]\n","Iter:  4400 loss =  0.007890038846913812 learning rate =  0.5 update =  [[-0.00092363]\n"," [-0.00092363]\n"," [ 0.00138453]]\n","Iter:  4401 loss =  0.007888227502889587 learning rate =  0.5 update =  [[-0.00092341]\n"," [-0.00092341]\n"," [ 0.00138422]]\n","Iter:  4402 loss =  0.007886416986031469 learning rate =  0.5 update =  [[-0.0009232]\n"," [-0.0009232]\n"," [ 0.0013839]]\n","Iter:  4403 loss =  0.007884607295774978 learning rate =  0.5 update =  [[-0.00092299]\n"," [-0.00092299]\n"," [ 0.00138359]]\n","Iter:  4404 loss =  0.007882798431556029 learning rate =  0.5 update =  [[-0.00092278]\n"," [-0.00092278]\n"," [ 0.00138327]]\n","Iter:  4405 loss =  0.007880990392811139 learning rate =  0.5 update =  [[-0.00092257]\n"," [-0.00092257]\n"," [ 0.00138295]]\n","Iter:  4406 loss =  0.007879183178977364 learning rate =  0.5 update =  [[-0.00092236]\n"," [-0.00092236]\n"," [ 0.00138264]]\n","Iter:  4407 loss =  0.007877376789492189 learning rate =  0.5 update =  [[-0.00092215]\n"," [-0.00092215]\n"," [ 0.00138232]]\n","Iter:  4408 loss =  0.007875571223793613 learning rate =  0.5 update =  [[-0.00092194]\n"," [-0.00092194]\n"," [ 0.00138201]]\n","Iter:  4409 loss =  0.007873766481320207 learning rate =  0.5 update =  [[-0.00092173]\n"," [-0.00092173]\n"," [ 0.00138169]]\n","Iter:  4410 loss =  0.007871962561511023 learning rate =  0.5 update =  [[-0.00092152]\n"," [-0.00092152]\n"," [ 0.00138138]]\n","Iter:  4411 loss =  0.00787015946380563 learning rate =  0.5 update =  [[-0.00092131]\n"," [-0.00092131]\n"," [ 0.00138106]]\n","Iter:  4412 loss =  0.007868357187644105 learning rate =  0.5 update =  [[-0.0009211 ]\n"," [-0.0009211 ]\n"," [ 0.00138075]]\n","Iter:  4413 loss =  0.007866555732467018 learning rate =  0.5 update =  [[-0.00092089]\n"," [-0.00092089]\n"," [ 0.00138044]]\n","Iter:  4414 loss =  0.007864755097715361 learning rate =  0.5 update =  [[-0.00092068]\n"," [-0.00092068]\n"," [ 0.00138012]]\n","Iter:  4415 loss =  0.007862955282830755 learning rate =  0.5 update =  [[-0.00092047]\n"," [-0.00092047]\n"," [ 0.00137981]]\n","Iter:  4416 loss =  0.00786115628725539 learning rate =  0.5 update =  [[-0.00092026]\n"," [-0.00092026]\n"," [ 0.00137949]]\n","Iter:  4417 loss =  0.007859358110431747 learning rate =  0.5 update =  [[-0.00092005]\n"," [-0.00092005]\n"," [ 0.00137918]]\n","Iter:  4418 loss =  0.007857560751802927 learning rate =  0.5 update =  [[-0.00091984]\n"," [-0.00091984]\n"," [ 0.00137887]]\n","Iter:  4419 loss =  0.007855764210812603 learning rate =  0.5 update =  [[-0.00091963]\n"," [-0.00091963]\n"," [ 0.00137855]]\n","Iter:  4420 loss =  0.007853968486904853 learning rate =  0.5 update =  [[-0.00091942]\n"," [-0.00091942]\n"," [ 0.00137824]]\n","Iter:  4421 loss =  0.007852173579524314 learning rate =  0.5 update =  [[-0.00091921]\n"," [-0.00091921]\n"," [ 0.00137793]]\n","Iter:  4422 loss =  0.007850379488116038 learning rate =  0.5 update =  [[-0.00091901]\n"," [-0.00091901]\n"," [ 0.00137761]]\n","Iter:  4423 loss =  0.007848586212125606 learning rate =  0.5 update =  [[-0.0009188]\n"," [-0.0009188]\n"," [ 0.0013773]]\n","Iter:  4424 loss =  0.007846793750999233 learning rate =  0.5 update =  [[-0.00091859]\n"," [-0.00091859]\n"," [ 0.00137699]]\n","Iter:  4425 loss =  0.007845002104183471 learning rate =  0.5 update =  [[-0.00091838]\n"," [-0.00091838]\n"," [ 0.00137667]]\n","Iter:  4426 loss =  0.007843211271125448 learning rate =  0.5 update =  [[-0.00091817]\n"," [-0.00091817]\n"," [ 0.00137636]]\n","Iter:  4427 loss =  0.007841421251272745 learning rate =  0.5 update =  [[-0.00091796]\n"," [-0.00091796]\n"," [ 0.00137605]]\n","Iter:  4428 loss =  0.007839632044073598 learning rate =  0.5 update =  [[-0.00091775]\n"," [-0.00091775]\n"," [ 0.00137574]]\n","Iter:  4429 loss =  0.007837843648976365 learning rate =  0.5 update =  [[-0.00091755]\n"," [-0.00091755]\n"," [ 0.00137542]]\n","Iter:  4430 loss =  0.007836056065430388 learning rate =  0.5 update =  [[-0.00091734]\n"," [-0.00091734]\n"," [ 0.00137511]]\n","Iter:  4431 loss =  0.00783426929288522 learning rate =  0.5 update =  [[-0.00091713]\n"," [-0.00091713]\n"," [ 0.0013748 ]]\n","Iter:  4432 loss =  0.00783248333079084 learning rate =  0.5 update =  [[-0.00091692]\n"," [-0.00091692]\n"," [ 0.00137449]]\n","Iter:  4433 loss =  0.007830698178597965 learning rate =  0.5 update =  [[-0.00091671]\n"," [-0.00091671]\n"," [ 0.00137418]]\n","Iter:  4434 loss =  0.007828913835757628 learning rate =  0.5 update =  [[-0.00091651]\n"," [-0.00091651]\n"," [ 0.00137387]]\n","Iter:  4435 loss =  0.00782713030172149 learning rate =  0.5 update =  [[-0.0009163 ]\n"," [-0.0009163 ]\n"," [ 0.00137356]]\n","Iter:  4436 loss =  0.007825347575941494 learning rate =  0.5 update =  [[-0.00091609]\n"," [-0.00091609]\n"," [ 0.00137324]]\n","Iter:  4437 loss =  0.007823565657870327 learning rate =  0.5 update =  [[-0.00091588]\n"," [-0.00091588]\n"," [ 0.00137293]]\n","Iter:  4438 loss =  0.007821784546960992 learning rate =  0.5 update =  [[-0.00091567]\n"," [-0.00091567]\n"," [ 0.00137262]]\n","Iter:  4439 loss =  0.00782000424266703 learning rate =  0.5 update =  [[-0.00091547]\n"," [-0.00091547]\n"," [ 0.00137231]]\n","Iter:  4440 loss =  0.007818224744442609 learning rate =  0.5 update =  [[-0.00091526]\n"," [-0.00091526]\n"," [ 0.001372  ]]\n","Iter:  4441 loss =  0.007816446051742157 learning rate =  0.5 update =  [[-0.00091505]\n"," [-0.00091505]\n"," [ 0.00137169]]\n","Iter:  4442 loss =  0.007814668164020697 learning rate =  0.5 update =  [[-0.00091485]\n"," [-0.00091485]\n"," [ 0.00137138]]\n","Iter:  4443 loss =  0.007812891080733853 learning rate =  0.5 update =  [[-0.00091464]\n"," [-0.00091464]\n"," [ 0.00137107]]\n","Iter:  4444 loss =  0.0078111148013375355 learning rate =  0.5 update =  [[-0.00091443]\n"," [-0.00091443]\n"," [ 0.00137076]]\n","Iter:  4445 loss =  0.00780933932528831 learning rate =  0.5 update =  [[-0.00091422]\n"," [-0.00091422]\n"," [ 0.00137045]]\n","Iter:  4446 loss =  0.007807564652043195 learning rate =  0.5 update =  [[-0.00091402]\n"," [-0.00091402]\n"," [ 0.00137014]]\n","Iter:  4447 loss =  0.007805790781059556 learning rate =  0.5 update =  [[-0.00091381]\n"," [-0.00091381]\n"," [ 0.00136983]]\n","Iter:  4448 loss =  0.00780401771179533 learning rate =  0.5 update =  [[-0.0009136 ]\n"," [-0.0009136 ]\n"," [ 0.00136952]]\n","Iter:  4449 loss =  0.007802245443709212 learning rate =  0.5 update =  [[-0.0009134 ]\n"," [-0.0009134 ]\n"," [ 0.00136921]]\n","Iter:  4450 loss =  0.007800473976259829 learning rate =  0.5 update =  [[-0.00091319]\n"," [-0.00091319]\n"," [ 0.0013689 ]]\n","Iter:  4451 loss =  0.007798703308906849 learning rate =  0.5 update =  [[-0.00091299]\n"," [-0.00091299]\n"," [ 0.00136859]]\n","Iter:  4452 loss =  0.00779693344111003 learning rate =  0.5 update =  [[-0.00091278]\n"," [-0.00091278]\n"," [ 0.00136829]]\n","Iter:  4453 loss =  0.0077951643723298755 learning rate =  0.5 update =  [[-0.00091257]\n"," [-0.00091257]\n"," [ 0.00136798]]\n","Iter:  4454 loss =  0.007793396102027198 learning rate =  0.5 update =  [[-0.00091237]\n"," [-0.00091237]\n"," [ 0.00136767]]\n","Iter:  4455 loss =  0.0077916286296633806 learning rate =  0.5 update =  [[-0.00091216]\n"," [-0.00091216]\n"," [ 0.00136736]]\n","Iter:  4456 loss =  0.007789861954700179 learning rate =  0.5 update =  [[-0.00091196]\n"," [-0.00091196]\n"," [ 0.00136705]]\n","Iter:  4457 loss =  0.007788096076600089 learning rate =  0.5 update =  [[-0.00091175]\n"," [-0.00091175]\n"," [ 0.00136674]]\n","Iter:  4458 loss =  0.007786330994825806 learning rate =  0.5 update =  [[-0.00091154]\n"," [-0.00091154]\n"," [ 0.00136643]]\n","Iter:  4459 loss =  0.0077845667088406525 learning rate =  0.5 update =  [[-0.00091134]\n"," [-0.00091134]\n"," [ 0.00136613]]\n","Iter:  4460 loss =  0.007782803218108409 learning rate =  0.5 update =  [[-0.00091113]\n"," [-0.00091113]\n"," [ 0.00136582]]\n","Iter:  4461 loss =  0.007781040522093311 learning rate =  0.5 update =  [[-0.00091093]\n"," [-0.00091093]\n"," [ 0.00136551]]\n","Iter:  4462 loss =  0.007779278620260077 learning rate =  0.5 update =  [[-0.00091072]\n"," [-0.00091072]\n"," [ 0.0013652 ]]\n","Iter:  4463 loss =  0.007777517512073971 learning rate =  0.5 update =  [[-0.00091052]\n"," [-0.00091052]\n"," [ 0.0013649 ]]\n","Iter:  4464 loss =  0.007775757197000568 learning rate =  0.5 update =  [[-0.00091031]\n"," [-0.00091031]\n"," [ 0.00136459]]\n","Iter:  4465 loss =  0.0077739976745062125 learning rate =  0.5 update =  [[-0.00091011]\n"," [-0.00091011]\n"," [ 0.00136428]]\n","Iter:  4466 loss =  0.0077722389440574215 learning rate =  0.5 update =  [[-0.0009099 ]\n"," [-0.0009099 ]\n"," [ 0.00136398]]\n","Iter:  4467 loss =  0.007770481005121394 learning rate =  0.5 update =  [[-0.0009097 ]\n"," [-0.0009097 ]\n"," [ 0.00136367]]\n","Iter:  4468 loss =  0.00776872385716562 learning rate =  0.5 update =  [[-0.00090949]\n"," [-0.00090949]\n"," [ 0.00136336]]\n","Iter:  4469 loss =  0.007766967499658323 learning rate =  0.5 update =  [[-0.00090929]\n"," [-0.00090929]\n"," [ 0.00136306]]\n","Iter:  4470 loss =  0.007765211932067875 learning rate =  0.5 update =  [[-0.00090908]\n"," [-0.00090908]\n"," [ 0.00136275]]\n","Iter:  4471 loss =  0.0077634571538635275 learning rate =  0.5 update =  [[-0.00090888]\n"," [-0.00090888]\n"," [ 0.00136244]]\n","Iter:  4472 loss =  0.007761703164514565 learning rate =  0.5 update =  [[-0.00090868]\n"," [-0.00090868]\n"," [ 0.00136214]]\n","Iter:  4473 loss =  0.0077599499634911796 learning rate =  0.5 update =  [[-0.00090847]\n"," [-0.00090847]\n"," [ 0.00136183]]\n","Iter:  4474 loss =  0.007758197550263598 learning rate =  0.5 update =  [[-0.00090827]\n"," [-0.00090827]\n"," [ 0.00136152]]\n","Iter:  4475 loss =  0.007756445924302837 learning rate =  0.5 update =  [[-0.00090806]\n"," [-0.00090806]\n"," [ 0.00136122]]\n","Iter:  4476 loss =  0.007754695085080349 learning rate =  0.5 update =  [[-0.00090786]\n"," [-0.00090786]\n"," [ 0.00136091]]\n","Iter:  4477 loss =  0.007752945032067982 learning rate =  0.5 update =  [[-0.00090765]\n"," [-0.00090765]\n"," [ 0.00136061]]\n","Iter:  4478 loss =  0.00775119576473801 learning rate =  0.5 update =  [[-0.00090745]\n"," [-0.00090745]\n"," [ 0.0013603 ]]\n","Iter:  4479 loss =  0.007749447282563279 learning rate =  0.5 update =  [[-0.00090725]\n"," [-0.00090725]\n"," [ 0.00136   ]]\n","Iter:  4480 loss =  0.007747699585017092 learning rate =  0.5 update =  [[-0.00090704]\n"," [-0.00090704]\n"," [ 0.00135969]]\n","Iter:  4481 loss =  0.007745952671573179 learning rate =  0.5 update =  [[-0.00090684]\n"," [-0.00090684]\n"," [ 0.00135939]]\n","Iter:  4482 loss =  0.007744206541705783 learning rate =  0.5 update =  [[-0.00090664]\n"," [-0.00090664]\n"," [ 0.00135908]]\n","Iter:  4483 loss =  0.007742461194889491 learning rate =  0.5 update =  [[-0.00090643]\n"," [-0.00090643]\n"," [ 0.00135878]]\n","Iter:  4484 loss =  0.007740716630599569 learning rate =  0.5 update =  [[-0.00090623]\n"," [-0.00090623]\n"," [ 0.00135847]]\n","Iter:  4485 loss =  0.007738972848311602 learning rate =  0.5 update =  [[-0.00090603]\n"," [-0.00090603]\n"," [ 0.00135817]]\n","Iter:  4486 loss =  0.007737229847501656 learning rate =  0.5 update =  [[-0.00090582]\n"," [-0.00090582]\n"," [ 0.00135786]]\n","Iter:  4487 loss =  0.007735487627646398 learning rate =  0.5 update =  [[-0.00090562]\n"," [-0.00090562]\n"," [ 0.00135756]]\n","Iter:  4488 loss =  0.007733746188222695 learning rate =  0.5 update =  [[-0.00090542]\n"," [-0.00090542]\n"," [ 0.00135726]]\n","Iter:  4489 loss =  0.007732005528708125 learning rate =  0.5 update =  [[-0.00090521]\n"," [-0.00090521]\n"," [ 0.00135695]]\n","Iter:  4490 loss =  0.007730265648580665 learning rate =  0.5 update =  [[-0.00090501]\n"," [-0.00090501]\n"," [ 0.00135665]]\n","Iter:  4491 loss =  0.0077285265473186655 learning rate =  0.5 update =  [[-0.00090481]\n"," [-0.00090481]\n"," [ 0.00135635]]\n","Iter:  4492 loss =  0.007726788224401097 learning rate =  0.5 update =  [[-0.00090461]\n"," [-0.00090461]\n"," [ 0.00135604]]\n","Iter:  4493 loss =  0.0077250506793071685 learning rate =  0.5 update =  [[-0.0009044 ]\n"," [-0.0009044 ]\n"," [ 0.00135574]]\n","Iter:  4494 loss =  0.007723313911516851 learning rate =  0.5 update =  [[-0.0009042 ]\n"," [-0.0009042 ]\n"," [ 0.00135544]]\n","Iter:  4495 loss =  0.007721577920510318 learning rate =  0.5 update =  [[-0.000904  ]\n"," [-0.000904  ]\n"," [ 0.00135513]]\n","Iter:  4496 loss =  0.007719842705768256 learning rate =  0.5 update =  [[-0.0009038 ]\n"," [-0.0009038 ]\n"," [ 0.00135483]]\n","Iter:  4497 loss =  0.007718108266772059 learning rate =  0.5 update =  [[-0.0009036 ]\n"," [-0.0009036 ]\n"," [ 0.00135453]]\n","Iter:  4498 loss =  0.007716374603003189 learning rate =  0.5 update =  [[-0.00090339]\n"," [-0.00090339]\n"," [ 0.00135422]]\n","Iter:  4499 loss =  0.007714641713943782 learning rate =  0.5 update =  [[-0.00090319]\n"," [-0.00090319]\n"," [ 0.00135392]]\n","Iter:  4500 loss =  0.00771290959907655 learning rate =  0.5 update =  [[-0.00090299]\n"," [-0.00090299]\n"," [ 0.00135362]]\n","Iter:  4501 loss =  0.007711178257884431 learning rate =  0.5 update =  [[-0.00090279]\n"," [-0.00090279]\n"," [ 0.00135332]]\n","Iter:  4502 loss =  0.007709447689850957 learning rate =  0.5 update =  [[-0.00090259]\n"," [-0.00090259]\n"," [ 0.00135302]]\n","Iter:  4503 loss =  0.007707717894460042 learning rate =  0.5 update =  [[-0.00090239]\n"," [-0.00090239]\n"," [ 0.00135271]]\n","Iter:  4504 loss =  0.0077059888711961035 learning rate =  0.5 update =  [[-0.00090218]\n"," [-0.00090218]\n"," [ 0.00135241]]\n","Iter:  4505 loss =  0.007704260619544073 learning rate =  0.5 update =  [[-0.00090198]\n"," [-0.00090198]\n"," [ 0.00135211]]\n","Iter:  4506 loss =  0.0077025331389892 learning rate =  0.5 update =  [[-0.00090178]\n"," [-0.00090178]\n"," [ 0.00135181]]\n","Iter:  4507 loss =  0.00770080642901733 learning rate =  0.5 update =  [[-0.00090158]\n"," [-0.00090158]\n"," [ 0.00135151]]\n","Iter:  4508 loss =  0.007699080489114678 learning rate =  0.5 update =  [[-0.00090138]\n"," [-0.00090138]\n"," [ 0.00135121]]\n","Iter:  4509 loss =  0.007697355318767944 learning rate =  0.5 update =  [[-0.00090118]\n"," [-0.00090118]\n"," [ 0.00135091]]\n","Iter:  4510 loss =  0.007695630917464287 learning rate =  0.5 update =  [[-0.00090098]\n"," [-0.00090098]\n"," [ 0.0013506 ]]\n","Iter:  4511 loss =  0.0076939072846912895 learning rate =  0.5 update =  [[-0.00090078]\n"," [-0.00090078]\n"," [ 0.0013503 ]]\n","Iter:  4512 loss =  0.007692184419936996 learning rate =  0.5 update =  [[-0.00090058]\n"," [-0.00090058]\n"," [ 0.00135   ]]\n","Iter:  4513 loss =  0.007690462322689984 learning rate =  0.5 update =  [[-0.00090037]\n"," [-0.00090037]\n"," [ 0.0013497 ]]\n","Iter:  4514 loss =  0.00768874099243915 learning rate =  0.5 update =  [[-0.00090017]\n"," [-0.00090017]\n"," [ 0.0013494 ]]\n","Iter:  4515 loss =  0.007687020428673906 learning rate =  0.5 update =  [[-0.00089997]\n"," [-0.00089997]\n"," [ 0.0013491 ]]\n","Iter:  4516 loss =  0.007685300630884169 learning rate =  0.5 update =  [[-0.00089977]\n"," [-0.00089977]\n"," [ 0.0013488 ]]\n","Iter:  4517 loss =  0.007683581598560292 learning rate =  0.5 update =  [[-0.00089957]\n"," [-0.00089957]\n"," [ 0.0013485 ]]\n","Iter:  4518 loss =  0.007681863331192964 learning rate =  0.5 update =  [[-0.00089937]\n"," [-0.00089937]\n"," [ 0.0013482 ]]\n","Iter:  4519 loss =  0.007680145828273479 learning rate =  0.5 update =  [[-0.00089917]\n"," [-0.00089917]\n"," [ 0.0013479 ]]\n","Iter:  4520 loss =  0.007678429089293433 learning rate =  0.5 update =  [[-0.00089897]\n"," [-0.00089897]\n"," [ 0.0013476 ]]\n","Iter:  4521 loss =  0.007676713113745008 learning rate =  0.5 update =  [[-0.00089877]\n"," [-0.00089877]\n"," [ 0.0013473 ]]\n","Iter:  4522 loss =  0.007674997901120771 learning rate =  0.5 update =  [[-0.00089857]\n"," [-0.00089857]\n"," [ 0.001347  ]]\n","Iter:  4523 loss =  0.0076732834509136965 learning rate =  0.5 update =  [[-0.00089837]\n"," [-0.00089837]\n"," [ 0.0013467 ]]\n","Iter:  4524 loss =  0.007671569762617325 learning rate =  0.5 update =  [[-0.00089817]\n"," [-0.00089817]\n"," [ 0.0013464 ]]\n","Iter:  4525 loss =  0.0076698568357255106 learning rate =  0.5 update =  [[-0.00089797]\n"," [-0.00089797]\n"," [ 0.00134611]]\n","Iter:  4526 loss =  0.0076681446697326235 learning rate =  0.5 update =  [[-0.00089777]\n"," [-0.00089777]\n"," [ 0.00134581]]\n","Iter:  4527 loss =  0.007666433264133486 learning rate =  0.5 update =  [[-0.00089757]\n"," [-0.00089757]\n"," [ 0.00134551]]\n","Iter:  4528 loss =  0.007664722618423353 learning rate =  0.5 update =  [[-0.00089738]\n"," [-0.00089738]\n"," [ 0.00134521]]\n","Iter:  4529 loss =  0.0076630127320980145 learning rate =  0.5 update =  [[-0.00089718]\n"," [-0.00089718]\n"," [ 0.00134491]]\n","Iter:  4530 loss =  0.007661303604653381 learning rate =  0.5 update =  [[-0.00089698]\n"," [-0.00089698]\n"," [ 0.00134461]]\n","Iter:  4531 loss =  0.007659595235586241 learning rate =  0.5 update =  [[-0.00089678]\n"," [-0.00089678]\n"," [ 0.00134431]]\n","Iter:  4532 loss =  0.007657887624393559 learning rate =  0.5 update =  [[-0.00089658]\n"," [-0.00089658]\n"," [ 0.00134402]]\n","Iter:  4533 loss =  0.007656180770572863 learning rate =  0.5 update =  [[-0.00089638]\n"," [-0.00089638]\n"," [ 0.00134372]]\n","Iter:  4534 loss =  0.007654474673621973 learning rate =  0.5 update =  [[-0.00089618]\n"," [-0.00089618]\n"," [ 0.00134342]]\n","Iter:  4535 loss =  0.00765276933303936 learning rate =  0.5 update =  [[-0.00089598]\n"," [-0.00089598]\n"," [ 0.00134312]]\n","Iter:  4536 loss =  0.007651064748323782 learning rate =  0.5 update =  [[-0.00089578]\n"," [-0.00089578]\n"," [ 0.00134282]]\n","Iter:  4537 loss =  0.007649360918974426 learning rate =  0.5 update =  [[-0.00089559]\n"," [-0.00089559]\n"," [ 0.00134253]]\n","Iter:  4538 loss =  0.0076476578444911285 learning rate =  0.5 update =  [[-0.00089539]\n"," [-0.00089539]\n"," [ 0.00134223]]\n","Iter:  4539 loss =  0.007645955524373848 learning rate =  0.5 update =  [[-0.00089519]\n"," [-0.00089519]\n"," [ 0.00134193]]\n","Iter:  4540 loss =  0.007644253958123279 learning rate =  0.5 update =  [[-0.00089499]\n"," [-0.00089499]\n"," [ 0.00134164]]\n","Iter:  4541 loss =  0.007642553145240398 learning rate =  0.5 update =  [[-0.00089479]\n"," [-0.00089479]\n"," [ 0.00134134]]\n","Iter:  4542 loss =  0.007640853085226588 learning rate =  0.5 update =  [[-0.00089459]\n"," [-0.00089459]\n"," [ 0.00134104]]\n","Iter:  4543 loss =  0.007639153777583773 learning rate =  0.5 update =  [[-0.0008944 ]\n"," [-0.0008944 ]\n"," [ 0.00134075]]\n","Iter:  4544 loss =  0.007637455221814351 learning rate =  0.5 update =  [[-0.0008942 ]\n"," [-0.0008942 ]\n"," [ 0.00134045]]\n","Iter:  4545 loss =  0.00763575741742099 learning rate =  0.5 update =  [[-0.000894  ]\n"," [-0.000894  ]\n"," [ 0.00134015]]\n","Iter:  4546 loss =  0.007634060363906948 learning rate =  0.5 update =  [[-0.0008938 ]\n"," [-0.0008938 ]\n"," [ 0.00133986]]\n","Iter:  4547 loss =  0.0076323640607758834 learning rate =  0.5 update =  [[-0.0008936 ]\n"," [-0.0008936 ]\n"," [ 0.00133956]]\n","Iter:  4548 loss =  0.007630668507531715 learning rate =  0.5 update =  [[-0.00089341]\n"," [-0.00089341]\n"," [ 0.00133926]]\n","Iter:  4549 loss =  0.007628973703679043 learning rate =  0.5 update =  [[-0.00089321]\n"," [-0.00089321]\n"," [ 0.00133897]]\n","Iter:  4550 loss =  0.00762727964872289 learning rate =  0.5 update =  [[-0.00089301]\n"," [-0.00089301]\n"," [ 0.00133867]]\n","Iter:  4551 loss =  0.007625586342168601 learning rate =  0.5 update =  [[-0.00089282]\n"," [-0.00089282]\n"," [ 0.00133838]]\n","Iter:  4552 loss =  0.007623893783521856 learning rate =  0.5 update =  [[-0.00089262]\n"," [-0.00089262]\n"," [ 0.00133808]]\n","Iter:  4553 loss =  0.007622201972289081 learning rate =  0.5 update =  [[-0.00089242]\n"," [-0.00089242]\n"," [ 0.00133779]]\n","Iter:  4554 loss =  0.007620510907976843 learning rate =  0.5 update =  [[-0.00089222]\n"," [-0.00089222]\n"," [ 0.00133749]]\n","Iter:  4555 loss =  0.007618820590092302 learning rate =  0.5 update =  [[-0.00089203]\n"," [-0.00089203]\n"," [ 0.0013372 ]]\n","Iter:  4556 loss =  0.0076171310181429985 learning rate =  0.5 update =  [[-0.00089183]\n"," [-0.00089183]\n"," [ 0.0013369 ]]\n","Iter:  4557 loss =  0.007615442191636892 learning rate =  0.5 update =  [[-0.00089163]\n"," [-0.00089163]\n"," [ 0.00133661]]\n","Iter:  4558 loss =  0.00761375411008243 learning rate =  0.5 update =  [[-0.00089144]\n"," [-0.00089144]\n"," [ 0.00133631]]\n","Iter:  4559 loss =  0.0076120667729883745 learning rate =  0.5 update =  [[-0.00089124]\n"," [-0.00089124]\n"," [ 0.00133602]]\n","Iter:  4560 loss =  0.007610380179864082 learning rate =  0.5 update =  [[-0.00089104]\n"," [-0.00089104]\n"," [ 0.00133572]]\n","Iter:  4561 loss =  0.007608694330219254 learning rate =  0.5 update =  [[-0.00089085]\n"," [-0.00089085]\n"," [ 0.00133543]]\n","Iter:  4562 loss =  0.007607009223563963 learning rate =  0.5 update =  [[-0.00089065]\n"," [-0.00089065]\n"," [ 0.00133513]]\n","Iter:  4563 loss =  0.0076053248594087075 learning rate =  0.5 update =  [[-0.00089045]\n"," [-0.00089045]\n"," [ 0.00133484]]\n","Iter:  4564 loss =  0.0076036412372646415 learning rate =  0.5 update =  [[-0.00089026]\n"," [-0.00089026]\n"," [ 0.00133455]]\n","Iter:  4565 loss =  0.007601958356643118 learning rate =  0.5 update =  [[-0.00089006]\n"," [-0.00089006]\n"," [ 0.00133425]]\n","Iter:  4566 loss =  0.007600276217055918 learning rate =  0.5 update =  [[-0.00088987]\n"," [-0.00088987]\n"," [ 0.00133396]]\n","Iter:  4567 loss =  0.0075985948180153935 learning rate =  0.5 update =  [[-0.00088967]\n"," [-0.00088967]\n"," [ 0.00133367]]\n","Iter:  4568 loss =  0.007596914159034181 learning rate =  0.5 update =  [[-0.00088947]\n"," [-0.00088947]\n"," [ 0.00133337]]\n","Iter:  4569 loss =  0.007595234239625455 learning rate =  0.5 update =  [[-0.00088928]\n"," [-0.00088928]\n"," [ 0.00133308]]\n","Iter:  4570 loss =  0.007593555059302794 learning rate =  0.5 update =  [[-0.00088908]\n"," [-0.00088908]\n"," [ 0.00133279]]\n","Iter:  4571 loss =  0.007591876617580059 learning rate =  0.5 update =  [[-0.00088889]\n"," [-0.00088889]\n"," [ 0.00133249]]\n","Iter:  4572 loss =  0.0075901989139717935 learning rate =  0.5 update =  [[-0.00088869]\n"," [-0.00088869]\n"," [ 0.0013322 ]]\n","Iter:  4573 loss =  0.007588521947992661 learning rate =  0.5 update =  [[-0.0008885 ]\n"," [-0.0008885 ]\n"," [ 0.00133191]]\n","Iter:  4574 loss =  0.007586845719158056 learning rate =  0.5 update =  [[-0.0008883 ]\n"," [-0.0008883 ]\n"," [ 0.00133161]]\n","Iter:  4575 loss =  0.007585170226983582 learning rate =  0.5 update =  [[-0.00088811]\n"," [-0.00088811]\n"," [ 0.00133132]]\n","Iter:  4576 loss =  0.007583495470985403 learning rate =  0.5 update =  [[-0.00088791]\n"," [-0.00088791]\n"," [ 0.00133103]]\n","Iter:  4577 loss =  0.0075818214506798935 learning rate =  0.5 update =  [[-0.00088772]\n"," [-0.00088772]\n"," [ 0.00133074]]\n","Iter:  4578 loss =  0.007580148165584189 learning rate =  0.5 update =  [[-0.00088752]\n"," [-0.00088752]\n"," [ 0.00133044]]\n","Iter:  4579 loss =  0.007578475615215541 learning rate =  0.5 update =  [[-0.00088733]\n"," [-0.00088733]\n"," [ 0.00133015]]\n","Iter:  4580 loss =  0.007576803799091715 learning rate =  0.5 update =  [[-0.00088713]\n"," [-0.00088713]\n"," [ 0.00132986]]\n","Iter:  4581 loss =  0.007575132716730904 learning rate =  0.5 update =  [[-0.00088694]\n"," [-0.00088694]\n"," [ 0.00132957]]\n","Iter:  4582 loss =  0.0075734623676518955 learning rate =  0.5 update =  [[-0.00088674]\n"," [-0.00088674]\n"," [ 0.00132928]]\n","Iter:  4583 loss =  0.007571792751373542 learning rate =  0.5 update =  [[-0.00088655]\n"," [-0.00088655]\n"," [ 0.00132899]]\n","Iter:  4584 loss =  0.0075701238674154 learning rate =  0.5 update =  [[-0.00088635]\n"," [-0.00088635]\n"," [ 0.00132869]]\n","Iter:  4585 loss =  0.0075684557152974 learning rate =  0.5 update =  [[-0.00088616]\n"," [-0.00088616]\n"," [ 0.0013284 ]]\n","Iter:  4586 loss =  0.007566788294539703 learning rate =  0.5 update =  [[-0.00088596]\n"," [-0.00088596]\n"," [ 0.00132811]]\n","Iter:  4587 loss =  0.0075651216046632025 learning rate =  0.5 update =  [[-0.00088577]\n"," [-0.00088577]\n"," [ 0.00132782]]\n","Iter:  4588 loss =  0.00756345564518889 learning rate =  0.5 update =  [[-0.00088557]\n"," [-0.00088557]\n"," [ 0.00132753]]\n","Iter:  4589 loss =  0.007561790415638457 learning rate =  0.5 update =  [[-0.00088538]\n"," [-0.00088538]\n"," [ 0.00132724]]\n","Iter:  4590 loss =  0.007560125915533745 learning rate =  0.5 update =  [[-0.00088519]\n"," [-0.00088519]\n"," [ 0.00132695]]\n","Iter:  4591 loss =  0.007558462144397164 learning rate =  0.5 update =  [[-0.00088499]\n"," [-0.00088499]\n"," [ 0.00132666]]\n","Iter:  4592 loss =  0.00755679910175166 learning rate =  0.5 update =  [[-0.0008848 ]\n"," [-0.0008848 ]\n"," [ 0.00132637]]\n","Iter:  4593 loss =  0.007555136787120246 learning rate =  0.5 update =  [[-0.00088461]\n"," [-0.00088461]\n"," [ 0.00132608]]\n","Iter:  4594 loss =  0.007553475200026726 learning rate =  0.5 update =  [[-0.00088441]\n"," [-0.00088441]\n"," [ 0.00132579]]\n","Iter:  4595 loss =  0.007551814339995101 learning rate =  0.5 update =  [[-0.00088422]\n"," [-0.00088422]\n"," [ 0.0013255 ]]\n","Iter:  4596 loss =  0.007550154206549833 learning rate =  0.5 update =  [[-0.00088402]\n"," [-0.00088402]\n"," [ 0.00132521]]\n","Iter:  4597 loss =  0.007548494799215698 learning rate =  0.5 update =  [[-0.00088383]\n"," [-0.00088383]\n"," [ 0.00132492]]\n","Iter:  4598 loss =  0.007546836117518097 learning rate =  0.5 update =  [[-0.00088364]\n"," [-0.00088364]\n"," [ 0.00132463]]\n","Iter:  4599 loss =  0.00754517816098274 learning rate =  0.5 update =  [[-0.00088344]\n"," [-0.00088344]\n"," [ 0.00132434]]\n","Iter:  4600 loss =  0.007543520929135741 learning rate =  0.5 update =  [[-0.00088325]\n"," [-0.00088325]\n"," [ 0.00132405]]\n","Iter:  4601 loss =  0.007541864421503586 learning rate =  0.5 update =  [[-0.00088306]\n"," [-0.00088306]\n"," [ 0.00132376]]\n","Iter:  4602 loss =  0.007540208637613214 learning rate =  0.5 update =  [[-0.00088287]\n"," [-0.00088287]\n"," [ 0.00132347]]\n","Iter:  4603 loss =  0.007538553576992016 learning rate =  0.5 update =  [[-0.00088267]\n"," [-0.00088267]\n"," [ 0.00132318]]\n","Iter:  4604 loss =  0.007536899239167703 learning rate =  0.5 update =  [[-0.00088248]\n"," [-0.00088248]\n"," [ 0.00132289]]\n","Iter:  4605 loss =  0.007535245623668468 learning rate =  0.5 update =  [[-0.00088229]\n"," [-0.00088229]\n"," [ 0.00132261]]\n","Iter:  4606 loss =  0.007533592730022902 learning rate =  0.5 update =  [[-0.00088209]\n"," [-0.00088209]\n"," [ 0.00132232]]\n","Iter:  4607 loss =  0.0075319405577599936 learning rate =  0.5 update =  [[-0.0008819 ]\n"," [-0.0008819 ]\n"," [ 0.00132203]]\n","Iter:  4608 loss =  0.007530289106409135 learning rate =  0.5 update =  [[-0.00088171]\n"," [-0.00088171]\n"," [ 0.00132174]]\n","Iter:  4609 loss =  0.007528638375500166 learning rate =  0.5 update =  [[-0.00088152]\n"," [-0.00088152]\n"," [ 0.00132145]]\n","Iter:  4610 loss =  0.007526988364563195 learning rate =  0.5 update =  [[-0.00088133]\n"," [-0.00088133]\n"," [ 0.00132116]]\n","Iter:  4611 loss =  0.0075253390731289446 learning rate =  0.5 update =  [[-0.00088113]\n"," [-0.00088113]\n"," [ 0.00132088]]\n","Iter:  4612 loss =  0.007523690500728401 learning rate =  0.5 update =  [[-0.00088094]\n"," [-0.00088094]\n"," [ 0.00132059]]\n","Iter:  4613 loss =  0.007522042646892976 learning rate =  0.5 update =  [[-0.00088075]\n"," [-0.00088075]\n"," [ 0.0013203 ]]\n","Iter:  4614 loss =  0.0075203955111545675 learning rate =  0.5 update =  [[-0.00088056]\n"," [-0.00088056]\n"," [ 0.00132001]]\n","Iter:  4615 loss =  0.00751874909304541 learning rate =  0.5 update =  [[-0.00088036]\n"," [-0.00088036]\n"," [ 0.00131973]]\n","Iter:  4616 loss =  0.007517103392098171 learning rate =  0.5 update =  [[-0.00088017]\n"," [-0.00088017]\n"," [ 0.00131944]]\n","Iter:  4617 loss =  0.0075154584078458014 learning rate =  0.5 update =  [[-0.00087998]\n"," [-0.00087998]\n"," [ 0.00131915]]\n","Iter:  4618 loss =  0.007513814139821873 learning rate =  0.5 update =  [[-0.00087979]\n"," [-0.00087979]\n"," [ 0.00131886]]\n","Iter:  4619 loss =  0.007512170587560226 learning rate =  0.5 update =  [[-0.0008796 ]\n"," [-0.0008796 ]\n"," [ 0.00131858]]\n","Iter:  4620 loss =  0.0075105277505950864 learning rate =  0.5 update =  [[-0.00087941]\n"," [-0.00087941]\n"," [ 0.00131829]]\n","Iter:  4621 loss =  0.007508885628461203 learning rate =  0.5 update =  [[-0.00087922]\n"," [-0.00087922]\n"," [ 0.001318  ]]\n","Iter:  4622 loss =  0.007507244220693574 learning rate =  0.5 update =  [[-0.00087902]\n"," [-0.00087902]\n"," [ 0.00131772]]\n","Iter:  4623 loss =  0.007505603526827744 learning rate =  0.5 update =  [[-0.00087883]\n"," [-0.00087883]\n"," [ 0.00131743]]\n","Iter:  4624 loss =  0.007503963546399484 learning rate =  0.5 update =  [[-0.00087864]\n"," [-0.00087864]\n"," [ 0.00131714]]\n","Iter:  4625 loss =  0.007502324278945131 learning rate =  0.5 update =  [[-0.00087845]\n"," [-0.00087845]\n"," [ 0.00131686]]\n","Iter:  4626 loss =  0.007500685724001397 learning rate =  0.5 update =  [[-0.00087826]\n"," [-0.00087826]\n"," [ 0.00131657]]\n","Iter:  4627 loss =  0.007499047881105362 learning rate =  0.5 update =  [[-0.00087807]\n"," [-0.00087807]\n"," [ 0.00131629]]\n","Iter:  4628 loss =  0.007497410749794448 learning rate =  0.5 update =  [[-0.00087788]\n"," [-0.00087788]\n"," [ 0.001316  ]]\n","Iter:  4629 loss =  0.007495774329606591 learning rate =  0.5 update =  [[-0.00087769]\n"," [-0.00087769]\n"," [ 0.00131571]]\n","Iter:  4630 loss =  0.007494138620079985 learning rate =  0.5 update =  [[-0.0008775 ]\n"," [-0.0008775 ]\n"," [ 0.00131543]]\n","Iter:  4631 loss =  0.007492503620753387 learning rate =  0.5 update =  [[-0.00087731]\n"," [-0.00087731]\n"," [ 0.00131514]]\n","Iter:  4632 loss =  0.007490869331165845 learning rate =  0.5 update =  [[-0.00087712]\n"," [-0.00087712]\n"," [ 0.00131486]]\n","Iter:  4633 loss =  0.007489235750856887 learning rate =  0.5 update =  [[-0.00087693]\n"," [-0.00087693]\n"," [ 0.00131457]]\n","Iter:  4634 loss =  0.00748760287936622 learning rate =  0.5 update =  [[-0.00087673]\n"," [-0.00087673]\n"," [ 0.00131429]]\n","Iter:  4635 loss =  0.0074859707162343355 learning rate =  0.5 update =  [[-0.00087654]\n"," [-0.00087654]\n"," [ 0.001314  ]]\n","Iter:  4636 loss =  0.007484339261001737 learning rate =  0.5 update =  [[-0.00087635]\n"," [-0.00087635]\n"," [ 0.00131372]]\n","Iter:  4637 loss =  0.007482708513209519 learning rate =  0.5 update =  [[-0.00087616]\n"," [-0.00087616]\n"," [ 0.00131343]]\n","Iter:  4638 loss =  0.00748107847239918 learning rate =  0.5 update =  [[-0.00087597]\n"," [-0.00087597]\n"," [ 0.00131315]]\n","Iter:  4639 loss =  0.007479449138112499 learning rate =  0.5 update =  [[-0.00087578]\n"," [-0.00087578]\n"," [ 0.00131286]]\n","Iter:  4640 loss =  0.007477820509891768 learning rate =  0.5 update =  [[-0.00087559]\n"," [-0.00087559]\n"," [ 0.00131258]]\n","Iter:  4641 loss =  0.007476192587279651 learning rate =  0.5 update =  [[-0.00087541]\n"," [-0.00087541]\n"," [ 0.0013123 ]]\n","Iter:  4642 loss =  0.007474565369819098 learning rate =  0.5 update =  [[-0.00087522]\n"," [-0.00087522]\n"," [ 0.00131201]]\n","Iter:  4643 loss =  0.007472938857053654 learning rate =  0.5 update =  [[-0.00087503]\n"," [-0.00087503]\n"," [ 0.00131173]]\n","Iter:  4644 loss =  0.007471313048527065 learning rate =  0.5 update =  [[-0.00087484]\n"," [-0.00087484]\n"," [ 0.00131144]]\n","Iter:  4645 loss =  0.007469687943783505 learning rate =  0.5 update =  [[-0.00087465]\n"," [-0.00087465]\n"," [ 0.00131116]]\n","Iter:  4646 loss =  0.0074680635423676305 learning rate =  0.5 update =  [[-0.00087446]\n"," [-0.00087446]\n"," [ 0.00131088]]\n","Iter:  4647 loss =  0.007466439843824469 learning rate =  0.5 update =  [[-0.00087427]\n"," [-0.00087427]\n"," [ 0.00131059]]\n","Iter:  4648 loss =  0.0074648168476993335 learning rate =  0.5 update =  [[-0.00087408]\n"," [-0.00087408]\n"," [ 0.00131031]]\n","Iter:  4649 loss =  0.007463194553538048 learning rate =  0.5 update =  [[-0.00087389]\n"," [-0.00087389]\n"," [ 0.00131003]]\n","Iter:  4650 loss =  0.0074615729608868645 learning rate =  0.5 update =  [[-0.0008737 ]\n"," [-0.0008737 ]\n"," [ 0.00130974]]\n","Iter:  4651 loss =  0.007459952069292182 learning rate =  0.5 update =  [[-0.00087351]\n"," [-0.00087351]\n"," [ 0.00130946]]\n","Iter:  4652 loss =  0.007458331878301043 learning rate =  0.5 update =  [[-0.00087332]\n"," [-0.00087332]\n"," [ 0.00130918]]\n","Iter:  4653 loss =  0.0074567123874607595 learning rate =  0.5 update =  [[-0.00087313]\n"," [-0.00087313]\n"," [ 0.00130889]]\n","Iter:  4654 loss =  0.007455093596319148 learning rate =  0.5 update =  [[-0.00087295]\n"," [-0.00087295]\n"," [ 0.00130861]]\n","Iter:  4655 loss =  0.007453475504424169 learning rate =  0.5 update =  [[-0.00087276]\n"," [-0.00087276]\n"," [ 0.00130833]]\n","Iter:  4656 loss =  0.007451858111324412 learning rate =  0.5 update =  [[-0.00087257]\n"," [-0.00087257]\n"," [ 0.00130805]]\n","Iter:  4657 loss =  0.007450241416568833 learning rate =  0.5 update =  [[-0.00087238]\n"," [-0.00087238]\n"," [ 0.00130776]]\n","Iter:  4658 loss =  0.00744862541970662 learning rate =  0.5 update =  [[-0.00087219]\n"," [-0.00087219]\n"," [ 0.00130748]]\n","Iter:  4659 loss =  0.007447010120287442 learning rate =  0.5 update =  [[-0.000872 ]\n"," [-0.000872 ]\n"," [ 0.0013072]]\n","Iter:  4660 loss =  0.007445395517861367 learning rate =  0.5 update =  [[-0.00087182]\n"," [-0.00087182]\n"," [ 0.00130692]]\n","Iter:  4661 loss =  0.007443781611978891 learning rate =  0.5 update =  [[-0.00087163]\n"," [-0.00087163]\n"," [ 0.00130664]]\n","Iter:  4662 loss =  0.007442168402190739 learning rate =  0.5 update =  [[-0.00087144]\n"," [-0.00087144]\n"," [ 0.00130635]]\n","Iter:  4663 loss =  0.007440555888048232 learning rate =  0.5 update =  [[-0.00087125]\n"," [-0.00087125]\n"," [ 0.00130607]]\n","Iter:  4664 loss =  0.00743894406910289 learning rate =  0.5 update =  [[-0.00087106]\n"," [-0.00087106]\n"," [ 0.00130579]]\n","Iter:  4665 loss =  0.007437332944906665 learning rate =  0.5 update =  [[-0.00087088]\n"," [-0.00087088]\n"," [ 0.00130551]]\n","Iter:  4666 loss =  0.0074357225150120144 learning rate =  0.5 update =  [[-0.00087069]\n"," [-0.00087069]\n"," [ 0.00130523]]\n","Iter:  4667 loss =  0.007434112778971571 learning rate =  0.5 update =  [[-0.0008705 ]\n"," [-0.0008705 ]\n"," [ 0.00130495]]\n","Iter:  4668 loss =  0.007432503736338566 learning rate =  0.5 update =  [[-0.00087031]\n"," [-0.00087031]\n"," [ 0.00130467]]\n","Iter:  4669 loss =  0.007430895386666483 learning rate =  0.5 update =  [[-0.00087013]\n"," [-0.00087013]\n"," [ 0.00130439]]\n","Iter:  4670 loss =  0.007429287729509154 learning rate =  0.5 update =  [[-0.00086994]\n"," [-0.00086994]\n"," [ 0.00130411]]\n","Iter:  4671 loss =  0.0074276807644209734 learning rate =  0.5 update =  [[-0.00086975]\n"," [-0.00086975]\n"," [ 0.00130382]]\n","Iter:  4672 loss =  0.007426074490956536 learning rate =  0.5 update =  [[-0.00086956]\n"," [-0.00086956]\n"," [ 0.00130354]]\n","Iter:  4673 loss =  0.0074244689086708435 learning rate =  0.5 update =  [[-0.00086938]\n"," [-0.00086938]\n"," [ 0.00130326]]\n","Iter:  4674 loss =  0.007422864017119399 learning rate =  0.5 update =  [[-0.00086919]\n"," [-0.00086919]\n"," [ 0.00130298]]\n","Iter:  4675 loss =  0.007421259815857914 learning rate =  0.5 update =  [[-0.000869 ]\n"," [-0.000869 ]\n"," [ 0.0013027]]\n","Iter:  4676 loss =  0.0074196563044426605 learning rate =  0.5 update =  [[-0.00086882]\n"," [-0.00086882]\n"," [ 0.00130242]]\n","Iter:  4677 loss =  0.007418053482430064 learning rate =  0.5 update =  [[-0.00086863]\n"," [-0.00086863]\n"," [ 0.00130214]]\n","Iter:  4678 loss =  0.007416451349377221 learning rate =  0.5 update =  [[-0.00086844]\n"," [-0.00086844]\n"," [ 0.00130186]]\n","Iter:  4679 loss =  0.007414849904841382 learning rate =  0.5 update =  [[-0.00086826]\n"," [-0.00086826]\n"," [ 0.00130158]]\n","Iter:  4680 loss =  0.007413249148380189 learning rate =  0.5 update =  [[-0.00086807]\n"," [-0.00086807]\n"," [ 0.0013013 ]]\n","Iter:  4681 loss =  0.007411649079551797 learning rate =  0.5 update =  [[-0.00086788]\n"," [-0.00086788]\n"," [ 0.00130103]]\n","Iter:  4682 loss =  0.007410049697914652 learning rate =  0.5 update =  [[-0.0008677 ]\n"," [-0.0008677 ]\n"," [ 0.00130075]]\n","Iter:  4683 loss =  0.0074084510030275035 learning rate =  0.5 update =  [[-0.00086751]\n"," [-0.00086751]\n"," [ 0.00130047]]\n","Iter:  4684 loss =  0.007406852994449648 learning rate =  0.5 update =  [[-0.00086732]\n"," [-0.00086732]\n"," [ 0.00130019]]\n","Iter:  4685 loss =  0.007405255671740555 learning rate =  0.5 update =  [[-0.00086714]\n"," [-0.00086714]\n"," [ 0.00129991]]\n","Iter:  4686 loss =  0.007403659034460344 learning rate =  0.5 update =  [[-0.00086695]\n"," [-0.00086695]\n"," [ 0.00129963]]\n","Iter:  4687 loss =  0.007402063082169221 learning rate =  0.5 update =  [[-0.00086677]\n"," [-0.00086677]\n"," [ 0.00129935]]\n","Iter:  4688 loss =  0.007400467814427906 learning rate =  0.5 update =  [[-0.00086658]\n"," [-0.00086658]\n"," [ 0.00129907]]\n","Iter:  4689 loss =  0.007398873230797574 learning rate =  0.5 update =  [[-0.00086639]\n"," [-0.00086639]\n"," [ 0.00129879]]\n","Iter:  4690 loss =  0.007397279330839459 learning rate =  0.5 update =  [[-0.00086621]\n"," [-0.00086621]\n"," [ 0.00129852]]\n","Iter:  4691 loss =  0.0073956861141157015 learning rate =  0.5 update =  [[-0.00086602]\n"," [-0.00086602]\n"," [ 0.00129824]]\n","Iter:  4692 loss =  0.0073940935801882456 learning rate =  0.5 update =  [[-0.00086584]\n"," [-0.00086584]\n"," [ 0.00129796]]\n","Iter:  4693 loss =  0.007392501728619834 learning rate =  0.5 update =  [[-0.00086565]\n"," [-0.00086565]\n"," [ 0.00129768]]\n","Iter:  4694 loss =  0.007390910558973264 learning rate =  0.5 update =  [[-0.00086547]\n"," [-0.00086547]\n"," [ 0.0012974 ]]\n","Iter:  4695 loss =  0.007389320070812073 learning rate =  0.5 update =  [[-0.00086528]\n"," [-0.00086528]\n"," [ 0.00129713]]\n","Iter:  4696 loss =  0.007387730263699744 learning rate =  0.5 update =  [[-0.00086509]\n"," [-0.00086509]\n"," [ 0.00129685]]\n","Iter:  4697 loss =  0.007386141137200441 learning rate =  0.5 update =  [[-0.00086491]\n"," [-0.00086491]\n"," [ 0.00129657]]\n","Iter:  4698 loss =  0.00738455269087853 learning rate =  0.5 update =  [[-0.00086472]\n"," [-0.00086472]\n"," [ 0.00129629]]\n","Iter:  4699 loss =  0.007382964924298885 learning rate =  0.5 update =  [[-0.00086454]\n"," [-0.00086454]\n"," [ 0.00129602]]\n","Iter:  4700 loss =  0.007381377837026697 learning rate =  0.5 update =  [[-0.00086435]\n"," [-0.00086435]\n"," [ 0.00129574]]\n","Iter:  4701 loss =  0.007379791428627497 learning rate =  0.5 update =  [[-0.00086417]\n"," [-0.00086417]\n"," [ 0.00129546]]\n","Iter:  4702 loss =  0.007378205698667215 learning rate =  0.5 update =  [[-0.00086398]\n"," [-0.00086398]\n"," [ 0.00129519]]\n","Iter:  4703 loss =  0.007376620646712041 learning rate =  0.5 update =  [[-0.0008638 ]\n"," [-0.0008638 ]\n"," [ 0.00129491]]\n","Iter:  4704 loss =  0.007375036272328727 learning rate =  0.5 update =  [[-0.00086361]\n"," [-0.00086361]\n"," [ 0.00129463]]\n","Iter:  4705 loss =  0.007373452575084259 learning rate =  0.5 update =  [[-0.00086343]\n"," [-0.00086343]\n"," [ 0.00129436]]\n","Iter:  4706 loss =  0.007371869554546045 learning rate =  0.5 update =  [[-0.00086325]\n"," [-0.00086325]\n"," [ 0.00129408]]\n","Iter:  4707 loss =  0.007370287210281757 learning rate =  0.5 update =  [[-0.00086306]\n"," [-0.00086306]\n"," [ 0.0012938 ]]\n","Iter:  4708 loss =  0.0073687055418596836 learning rate =  0.5 update =  [[-0.00086288]\n"," [-0.00086288]\n"," [ 0.00129353]]\n","Iter:  4709 loss =  0.007367124548848205 learning rate =  0.5 update =  [[-0.00086269]\n"," [-0.00086269]\n"," [ 0.00129325]]\n","Iter:  4710 loss =  0.007365544230816186 learning rate =  0.5 update =  [[-0.00086251]\n"," [-0.00086251]\n"," [ 0.00129297]]\n","Iter:  4711 loss =  0.007363964587332886 learning rate =  0.5 update =  [[-0.00086232]\n"," [-0.00086232]\n"," [ 0.0012927 ]]\n","Iter:  4712 loss =  0.007362385617967881 learning rate =  0.5 update =  [[-0.00086214]\n"," [-0.00086214]\n"," [ 0.00129242]]\n","Iter:  4713 loss =  0.007360807322291144 learning rate =  0.5 update =  [[-0.00086196]\n"," [-0.00086196]\n"," [ 0.00129215]]\n","Iter:  4714 loss =  0.007359229699872877 learning rate =  0.5 update =  [[-0.00086177]\n"," [-0.00086177]\n"," [ 0.00129187]]\n","Iter:  4715 loss =  0.007357652750283961 learning rate =  0.5 update =  [[-0.00086159]\n"," [-0.00086159]\n"," [ 0.0012916 ]]\n","Iter:  4716 loss =  0.007356076473095285 learning rate =  0.5 update =  [[-0.0008614 ]\n"," [-0.0008614 ]\n"," [ 0.00129132]]\n","Iter:  4717 loss =  0.007354500867878356 learning rate =  0.5 update =  [[-0.00086122]\n"," [-0.00086122]\n"," [ 0.00129105]]\n","Iter:  4718 loss =  0.007352925934204887 learning rate =  0.5 update =  [[-0.00086104]\n"," [-0.00086104]\n"," [ 0.00129077]]\n","Iter:  4719 loss =  0.007351351671647014 learning rate =  0.5 update =  [[-0.00086085]\n"," [-0.00086085]\n"," [ 0.0012905 ]]\n","Iter:  4720 loss =  0.007349778079777328 learning rate =  0.5 update =  [[-0.00086067]\n"," [-0.00086067]\n"," [ 0.00129022]]\n","Iter:  4721 loss =  0.007348205158168622 learning rate =  0.5 update =  [[-0.00086049]\n"," [-0.00086049]\n"," [ 0.00128995]]\n","Iter:  4722 loss =  0.007346632906394059 learning rate =  0.5 update =  [[-0.0008603 ]\n"," [-0.0008603 ]\n"," [ 0.00128967]]\n","Iter:  4723 loss =  0.007345061324027428 learning rate =  0.5 update =  [[-0.00086012]\n"," [-0.00086012]\n"," [ 0.0012894 ]]\n","Iter:  4724 loss =  0.007343490410642431 learning rate =  0.5 update =  [[-0.00085994]\n"," [-0.00085994]\n"," [ 0.00128912]]\n","Iter:  4725 loss =  0.007341920165813566 learning rate =  0.5 update =  [[-0.00085975]\n"," [-0.00085975]\n"," [ 0.00128885]]\n","Iter:  4726 loss =  0.007340350589115337 learning rate =  0.5 update =  [[-0.00085957]\n"," [-0.00085957]\n"," [ 0.00128857]]\n","Iter:  4727 loss =  0.007338781680122897 learning rate =  0.5 update =  [[-0.00085939]\n"," [-0.00085939]\n"," [ 0.0012883 ]]\n","Iter:  4728 loss =  0.007337213438411656 learning rate =  0.5 update =  [[-0.00085921]\n"," [-0.00085921]\n"," [ 0.00128803]]\n","Iter:  4729 loss =  0.007335645863557226 learning rate =  0.5 update =  [[-0.00085902]\n"," [-0.00085902]\n"," [ 0.00128775]]\n","Iter:  4730 loss =  0.007334078955135787 learning rate =  0.5 update =  [[-0.00085884]\n"," [-0.00085884]\n"," [ 0.00128748]]\n","Iter:  4731 loss =  0.007332512712723805 learning rate =  0.5 update =  [[-0.00085866]\n"," [-0.00085866]\n"," [ 0.00128721]]\n","Iter:  4732 loss =  0.007330947135898083 learning rate =  0.5 update =  [[-0.00085848]\n"," [-0.00085848]\n"," [ 0.00128693]]\n","Iter:  4733 loss =  0.0073293822242358276 learning rate =  0.5 update =  [[-0.00085829]\n"," [-0.00085829]\n"," [ 0.00128666]]\n","Iter:  4734 loss =  0.007327817977314667 learning rate =  0.5 update =  [[-0.00085811]\n"," [-0.00085811]\n"," [ 0.00128639]]\n","Iter:  4735 loss =  0.007326254394712239 learning rate =  0.5 update =  [[-0.00085793]\n"," [-0.00085793]\n"," [ 0.00128611]]\n","Iter:  4736 loss =  0.007324691476007025 learning rate =  0.5 update =  [[-0.00085775]\n"," [-0.00085775]\n"," [ 0.00128584]]\n","Iter:  4737 loss =  0.0073231292207775415 learning rate =  0.5 update =  [[-0.00085756]\n"," [-0.00085756]\n"," [ 0.00128557]]\n","Iter:  4738 loss =  0.007321567628602673 learning rate =  0.5 update =  [[-0.00085738]\n"," [-0.00085738]\n"," [ 0.00128529]]\n","Iter:  4739 loss =  0.007320006699061871 learning rate =  0.5 update =  [[-0.0008572 ]\n"," [-0.0008572 ]\n"," [ 0.00128502]]\n","Iter:  4740 loss =  0.0073184464317348165 learning rate =  0.5 update =  [[-0.00085702]\n"," [-0.00085702]\n"," [ 0.00128475]]\n","Iter:  4741 loss =  0.007316886826201421 learning rate =  0.5 update =  [[-0.00085684]\n"," [-0.00085684]\n"," [ 0.00128448]]\n","Iter:  4742 loss =  0.007315327882042106 learning rate =  0.5 update =  [[-0.00085665]\n"," [-0.00085665]\n"," [ 0.0012842 ]]\n","Iter:  4743 loss =  0.00731376959883769 learning rate =  0.5 update =  [[-0.00085647]\n"," [-0.00085647]\n"," [ 0.00128393]]\n","Iter:  4744 loss =  0.007312211976169108 learning rate =  0.5 update =  [[-0.00085629]\n"," [-0.00085629]\n"," [ 0.00128366]]\n","Iter:  4745 loss =  0.00731065501361792 learning rate =  0.5 update =  [[-0.00085611]\n"," [-0.00085611]\n"," [ 0.00128339]]\n","Iter:  4746 loss =  0.007309098710765885 learning rate =  0.5 update =  [[-0.00085593]\n"," [-0.00085593]\n"," [ 0.00128312]]\n","Iter:  4747 loss =  0.007307543067195135 learning rate =  0.5 update =  [[-0.00085575]\n"," [-0.00085575]\n"," [ 0.00128285]]\n","Iter:  4748 loss =  0.007305988082488223 learning rate =  0.5 update =  [[-0.00085557]\n"," [-0.00085557]\n"," [ 0.00128257]]\n","Iter:  4749 loss =  0.00730443375622788 learning rate =  0.5 update =  [[-0.00085538]\n"," [-0.00085538]\n"," [ 0.0012823 ]]\n","Iter:  4750 loss =  0.007302880087997456 learning rate =  0.5 update =  [[-0.0008552 ]\n"," [-0.0008552 ]\n"," [ 0.00128203]]\n","Iter:  4751 loss =  0.007301327077380309 learning rate =  0.5 update =  [[-0.00085502]\n"," [-0.00085502]\n"," [ 0.00128176]]\n","Iter:  4752 loss =  0.007299774723960557 learning rate =  0.5 update =  [[-0.00085484]\n"," [-0.00085484]\n"," [ 0.00128149]]\n","Iter:  4753 loss =  0.0072982230273223505 learning rate =  0.5 update =  [[-0.00085466]\n"," [-0.00085466]\n"," [ 0.00128122]]\n","Iter:  4754 loss =  0.007296671987050271 learning rate =  0.5 update =  [[-0.00085448]\n"," [-0.00085448]\n"," [ 0.00128095]]\n","Iter:  4755 loss =  0.007295121602729293 learning rate =  0.5 update =  [[-0.0008543 ]\n"," [-0.0008543 ]\n"," [ 0.00128068]]\n","Iter:  4756 loss =  0.007293571873944731 learning rate =  0.5 update =  [[-0.00085412]\n"," [-0.00085412]\n"," [ 0.00128041]]\n","Iter:  4757 loss =  0.0072920228002822775 learning rate =  0.5 update =  [[-0.00085394]\n"," [-0.00085394]\n"," [ 0.00128013]]\n","Iter:  4758 loss =  0.007290474381327761 learning rate =  0.5 update =  [[-0.00085376]\n"," [-0.00085376]\n"," [ 0.00127986]]\n","Iter:  4759 loss =  0.0072889266166677215 learning rate =  0.5 update =  [[-0.00085358]\n"," [-0.00085358]\n"," [ 0.00127959]]\n","Iter:  4760 loss =  0.00728737950588873 learning rate =  0.5 update =  [[-0.0008534 ]\n"," [-0.0008534 ]\n"," [ 0.00127932]]\n","Iter:  4761 loss =  0.007285833048577899 learning rate =  0.5 update =  [[-0.00085322]\n"," [-0.00085322]\n"," [ 0.00127905]]\n","Iter:  4762 loss =  0.007284287244322538 learning rate =  0.5 update =  [[-0.00085304]\n"," [-0.00085304]\n"," [ 0.00127878]]\n","Iter:  4763 loss =  0.007282742092710442 learning rate =  0.5 update =  [[-0.00085286]\n"," [-0.00085286]\n"," [ 0.00127851]]\n","Iter:  4764 loss =  0.007281197593329743 learning rate =  0.5 update =  [[-0.00085268]\n"," [-0.00085268]\n"," [ 0.00127824]]\n","Iter:  4765 loss =  0.007279653745768695 learning rate =  0.5 update =  [[-0.0008525 ]\n"," [-0.0008525 ]\n"," [ 0.00127797]]\n","Iter:  4766 loss =  0.0072781105496162275 learning rate =  0.5 update =  [[-0.00085232]\n"," [-0.00085232]\n"," [ 0.0012777 ]]\n","Iter:  4767 loss =  0.007276568004461361 learning rate =  0.5 update =  [[-0.00085214]\n"," [-0.00085214]\n"," [ 0.00127744]]\n","Iter:  4768 loss =  0.007275026109893651 learning rate =  0.5 update =  [[-0.00085196]\n"," [-0.00085196]\n"," [ 0.00127717]]\n","Iter:  4769 loss =  0.007273484865502748 learning rate =  0.5 update =  [[-0.00085178]\n"," [-0.00085178]\n"," [ 0.0012769 ]]\n","Iter:  4770 loss =  0.007271944270878945 learning rate =  0.5 update =  [[-0.0008516 ]\n"," [-0.0008516 ]\n"," [ 0.00127663]]\n","Iter:  4771 loss =  0.007270404325612711 learning rate =  0.5 update =  [[-0.00085142]\n"," [-0.00085142]\n"," [ 0.00127636]]\n","Iter:  4772 loss =  0.0072688650292948905 learning rate =  0.5 update =  [[-0.00085124]\n"," [-0.00085124]\n"," [ 0.00127609]]\n","Iter:  4773 loss =  0.0072673263815165486 learning rate =  0.5 update =  [[-0.00085106]\n"," [-0.00085106]\n"," [ 0.00127582]]\n","Iter:  4774 loss =  0.00726578838186935 learning rate =  0.5 update =  [[-0.00085088]\n"," [-0.00085088]\n"," [ 0.00127555]]\n","Iter:  4775 loss =  0.007264251029945102 learning rate =  0.5 update =  [[-0.0008507 ]\n"," [-0.0008507 ]\n"," [ 0.00127528]]\n","Iter:  4776 loss =  0.007262714325335954 learning rate =  0.5 update =  [[-0.00085052]\n"," [-0.00085052]\n"," [ 0.00127502]]\n","Iter:  4777 loss =  0.00726117826763454 learning rate =  0.5 update =  [[-0.00085034]\n"," [-0.00085034]\n"," [ 0.00127475]]\n","Iter:  4778 loss =  0.007259642856433633 learning rate =  0.5 update =  [[-0.00085016]\n"," [-0.00085016]\n"," [ 0.00127448]]\n","Iter:  4779 loss =  0.007258108091326579 learning rate =  0.5 update =  [[-0.00084998]\n"," [-0.00084998]\n"," [ 0.00127421]]\n","Iter:  4780 loss =  0.007256573971906949 learning rate =  0.5 update =  [[-0.00084981]\n"," [-0.00084981]\n"," [ 0.00127394]]\n","Iter:  4781 loss =  0.0072550404977685425 learning rate =  0.5 update =  [[-0.00084963]\n"," [-0.00084963]\n"," [ 0.00127368]]\n","Iter:  4782 loss =  0.0072535076685057015 learning rate =  0.5 update =  [[-0.00084945]\n"," [-0.00084945]\n"," [ 0.00127341]]\n","Iter:  4783 loss =  0.0072519754837129915 learning rate =  0.5 update =  [[-0.00084927]\n"," [-0.00084927]\n"," [ 0.00127314]]\n","Iter:  4784 loss =  0.007250443942985297 learning rate =  0.5 update =  [[-0.00084909]\n"," [-0.00084909]\n"," [ 0.00127287]]\n","Iter:  4785 loss =  0.007248913045917895 learning rate =  0.5 update =  [[-0.00084891]\n"," [-0.00084891]\n"," [ 0.00127261]]\n","Iter:  4786 loss =  0.007247382792106493 learning rate =  0.5 update =  [[-0.00084873]\n"," [-0.00084873]\n"," [ 0.00127234]]\n","Iter:  4787 loss =  0.007245853181146856 learning rate =  0.5 update =  [[-0.00084856]\n"," [-0.00084856]\n"," [ 0.00127207]]\n","Iter:  4788 loss =  0.0072443242126354005 learning rate =  0.5 update =  [[-0.00084838]\n"," [-0.00084838]\n"," [ 0.0012718 ]]\n","Iter:  4789 loss =  0.007242795886168686 learning rate =  0.5 update =  [[-0.0008482 ]\n"," [-0.0008482 ]\n"," [ 0.00127154]]\n","Iter:  4790 loss =  0.007241268201343701 learning rate =  0.5 update =  [[-0.00084802]\n"," [-0.00084802]\n"," [ 0.00127127]]\n","Iter:  4791 loss =  0.007239741157757662 learning rate =  0.5 update =  [[-0.00084784]\n"," [-0.00084784]\n"," [ 0.001271  ]]\n","Iter:  4792 loss =  0.007238214755008264 learning rate =  0.5 update =  [[-0.00084767]\n"," [-0.00084767]\n"," [ 0.00127074]]\n","Iter:  4793 loss =  0.007236688992693494 learning rate =  0.5 update =  [[-0.00084749]\n"," [-0.00084749]\n"," [ 0.00127047]]\n","Iter:  4794 loss =  0.007235163870411533 learning rate =  0.5 update =  [[-0.00084731]\n"," [-0.00084731]\n"," [ 0.0012702 ]]\n","Iter:  4795 loss =  0.007233639387761103 learning rate =  0.5 update =  [[-0.00084713]\n"," [-0.00084713]\n"," [ 0.00126994]]\n","Iter:  4796 loss =  0.007232115544341158 learning rate =  0.5 update =  [[-0.00084695]\n"," [-0.00084695]\n"," [ 0.00126967]]\n","Iter:  4797 loss =  0.0072305923397509315 learning rate =  0.5 update =  [[-0.00084678]\n"," [-0.00084678]\n"," [ 0.00126941]]\n","Iter:  4798 loss =  0.007229069773590198 learning rate =  0.5 update =  [[-0.0008466 ]\n"," [-0.0008466 ]\n"," [ 0.00126914]]\n","Iter:  4799 loss =  0.007227547845458825 learning rate =  0.5 update =  [[-0.00084642]\n"," [-0.00084642]\n"," [ 0.00126887]]\n","Iter:  4800 loss =  0.007226026554957068 learning rate =  0.5 update =  [[-0.00084624]\n"," [-0.00084624]\n"," [ 0.00126861]]\n","Iter:  4801 loss =  0.007224505901685643 learning rate =  0.5 update =  [[-0.00084607]\n"," [-0.00084607]\n"," [ 0.00126834]]\n","Iter:  4802 loss =  0.007222985885245549 learning rate =  0.5 update =  [[-0.00084589]\n"," [-0.00084589]\n"," [ 0.00126808]]\n","Iter:  4803 loss =  0.007221466505238016 learning rate =  0.5 update =  [[-0.00084571]\n"," [-0.00084571]\n"," [ 0.00126781]]\n","Iter:  4804 loss =  0.007219947761264696 learning rate =  0.5 update =  [[-0.00084554]\n"," [-0.00084554]\n"," [ 0.00126755]]\n","Iter:  4805 loss =  0.007218429652927587 learning rate =  0.5 update =  [[-0.00084536]\n"," [-0.00084536]\n"," [ 0.00126728]]\n","Iter:  4806 loss =  0.007216912179828913 learning rate =  0.5 update =  [[-0.00084518]\n"," [-0.00084518]\n"," [ 0.00126702]]\n","Iter:  4807 loss =  0.007215395341571351 learning rate =  0.5 update =  [[-0.000845  ]\n"," [-0.000845  ]\n"," [ 0.00126675]]\n","Iter:  4808 loss =  0.007213879137757811 learning rate =  0.5 update =  [[-0.00084483]\n"," [-0.00084483]\n"," [ 0.00126649]]\n","Iter:  4809 loss =  0.00721236356799165 learning rate =  0.5 update =  [[-0.00084465]\n"," [-0.00084465]\n"," [ 0.00126622]]\n","Iter:  4810 loss =  0.007210848631876432 learning rate =  0.5 update =  [[-0.00084447]\n"," [-0.00084447]\n"," [ 0.00126596]]\n","Iter:  4811 loss =  0.007209334329016061 learning rate =  0.5 update =  [[-0.0008443 ]\n"," [-0.0008443 ]\n"," [ 0.00126569]]\n","Iter:  4812 loss =  0.007207820659014921 learning rate =  0.5 update =  [[-0.00084412]\n"," [-0.00084412]\n"," [ 0.00126543]]\n","Iter:  4813 loss =  0.007206307621477569 learning rate =  0.5 update =  [[-0.00084395]\n"," [-0.00084395]\n"," [ 0.00126516]]\n","Iter:  4814 loss =  0.007204795216008906 learning rate =  0.5 update =  [[-0.00084377]\n"," [-0.00084377]\n"," [ 0.0012649 ]]\n","Iter:  4815 loss =  0.007203283442214227 learning rate =  0.5 update =  [[-0.00084359]\n"," [-0.00084359]\n"," [ 0.00126464]]\n","Iter:  4816 loss =  0.007201772299699113 learning rate =  0.5 update =  [[-0.00084342]\n"," [-0.00084342]\n"," [ 0.00126437]]\n","Iter:  4817 loss =  0.00720026178806946 learning rate =  0.5 update =  [[-0.00084324]\n"," [-0.00084324]\n"," [ 0.00126411]]\n","Iter:  4818 loss =  0.007198751906931531 learning rate =  0.5 update =  [[-0.00084306]\n"," [-0.00084306]\n"," [ 0.00126384]]\n","Iter:  4819 loss =  0.007197242655891934 learning rate =  0.5 update =  [[-0.00084289]\n"," [-0.00084289]\n"," [ 0.00126358]]\n","Iter:  4820 loss =  0.007195734034557473 learning rate =  0.5 update =  [[-0.00084271]\n"," [-0.00084271]\n"," [ 0.00126332]]\n","Iter:  4821 loss =  0.00719422604253552 learning rate =  0.5 update =  [[-0.00084254]\n"," [-0.00084254]\n"," [ 0.00126305]]\n","Iter:  4822 loss =  0.007192718679433427 learning rate =  0.5 update =  [[-0.00084236]\n"," [-0.00084236]\n"," [ 0.00126279]]\n","Iter:  4823 loss =  0.007191211944859188 learning rate =  0.5 update =  [[-0.00084219]\n"," [-0.00084219]\n"," [ 0.00126253]]\n","Iter:  4824 loss =  0.007189705838421004 learning rate =  0.5 update =  [[-0.00084201]\n"," [-0.00084201]\n"," [ 0.00126226]]\n","Iter:  4825 loss =  0.007188200359727389 learning rate =  0.5 update =  [[-0.00084183]\n"," [-0.00084183]\n"," [ 0.001262  ]]\n","Iter:  4826 loss =  0.007186695508387138 learning rate =  0.5 update =  [[-0.00084166]\n"," [-0.00084166]\n"," [ 0.00126174]]\n","Iter:  4827 loss =  0.007185191284009504 learning rate =  0.5 update =  [[-0.00084148]\n"," [-0.00084148]\n"," [ 0.00126148]]\n","Iter:  4828 loss =  0.0071836876862039125 learning rate =  0.5 update =  [[-0.00084131]\n"," [-0.00084131]\n"," [ 0.00126121]]\n","Iter:  4829 loss =  0.00718218471458021 learning rate =  0.5 update =  [[-0.00084113]\n"," [-0.00084113]\n"," [ 0.00126095]]\n","Iter:  4830 loss =  0.007180682368748616 learning rate =  0.5 update =  [[-0.00084096]\n"," [-0.00084096]\n"," [ 0.00126069]]\n","Iter:  4831 loss =  0.007179180648319437 learning rate =  0.5 update =  [[-0.00084078]\n"," [-0.00084078]\n"," [ 0.00126043]]\n","Iter:  4832 loss =  0.007177679552903632 learning rate =  0.5 update =  [[-0.00084061]\n"," [-0.00084061]\n"," [ 0.00126016]]\n","Iter:  4833 loss =  0.007176179082112245 learning rate =  0.5 update =  [[-0.00084043]\n"," [-0.00084043]\n"," [ 0.0012599 ]]\n","Iter:  4834 loss =  0.007174679235556607 learning rate =  0.5 update =  [[-0.00084026]\n"," [-0.00084026]\n"," [ 0.00125964]]\n","Iter:  4835 loss =  0.007173180012848644 learning rate =  0.5 update =  [[-0.00084008]\n"," [-0.00084008]\n"," [ 0.00125938]]\n","Iter:  4836 loss =  0.007171681413600256 learning rate =  0.5 update =  [[-0.00083991]\n"," [-0.00083991]\n"," [ 0.00125912]]\n","Iter:  4837 loss =  0.0071701834374239635 learning rate =  0.5 update =  [[-0.00083973]\n"," [-0.00083973]\n"," [ 0.00125885]]\n","Iter:  4838 loss =  0.007168686083932464 learning rate =  0.5 update =  [[-0.00083956]\n"," [-0.00083956]\n"," [ 0.00125859]]\n","Iter:  4839 loss =  0.007167189352738794 learning rate =  0.5 update =  [[-0.00083938]\n"," [-0.00083938]\n"," [ 0.00125833]]\n","Iter:  4840 loss =  0.0071656932434562455 learning rate =  0.5 update =  [[-0.00083921]\n"," [-0.00083921]\n"," [ 0.00125807]]\n","Iter:  4841 loss =  0.007164197755698567 learning rate =  0.5 update =  [[-0.00083904]\n"," [-0.00083904]\n"," [ 0.00125781]]\n","Iter:  4842 loss =  0.007162702889079763 learning rate =  0.5 update =  [[-0.00083886]\n"," [-0.00083886]\n"," [ 0.00125755]]\n","Iter:  4843 loss =  0.007161208643214035 learning rate =  0.5 update =  [[-0.00083869]\n"," [-0.00083869]\n"," [ 0.00125729]]\n","Iter:  4844 loss =  0.007159715017716129 learning rate =  0.5 update =  [[-0.00083851]\n"," [-0.00083851]\n"," [ 0.00125703]]\n","Iter:  4845 loss =  0.007158222012200984 learning rate =  0.5 update =  [[-0.00083834]\n"," [-0.00083834]\n"," [ 0.00125676]]\n","Iter:  4846 loss =  0.007156729626283829 learning rate =  0.5 update =  [[-0.00083817]\n"," [-0.00083817]\n"," [ 0.0012565 ]]\n","Iter:  4847 loss =  0.007155237859580263 learning rate =  0.5 update =  [[-0.00083799]\n"," [-0.00083799]\n"," [ 0.00125624]]\n","Iter:  4848 loss =  0.0071537467117062225 learning rate =  0.5 update =  [[-0.00083782]\n"," [-0.00083782]\n"," [ 0.00125598]]\n","Iter:  4849 loss =  0.007152256182277817 learning rate =  0.5 update =  [[-0.00083764]\n"," [-0.00083764]\n"," [ 0.00125572]]\n","Iter:  4850 loss =  0.007150766270911697 learning rate =  0.5 update =  [[-0.00083747]\n"," [-0.00083747]\n"," [ 0.00125546]]\n","Iter:  4851 loss =  0.007149276977224738 learning rate =  0.5 update =  [[-0.0008373]\n"," [-0.0008373]\n"," [ 0.0012552]]\n","Iter:  4852 loss =  0.007147788300833992 learning rate =  0.5 update =  [[-0.00083712]\n"," [-0.00083712]\n"," [ 0.00125494]]\n","Iter:  4853 loss =  0.0071463002413570155 learning rate =  0.5 update =  [[-0.00083695]\n"," [-0.00083695]\n"," [ 0.00125468]]\n","Iter:  4854 loss =  0.007144812798411571 learning rate =  0.5 update =  [[-0.00083678]\n"," [-0.00083678]\n"," [ 0.00125442]]\n","Iter:  4855 loss =  0.0071433259716158146 learning rate =  0.5 update =  [[-0.0008366 ]\n"," [-0.0008366 ]\n"," [ 0.00125416]]\n","Iter:  4856 loss =  0.007141839760588159 learning rate =  0.5 update =  [[-0.00083643]\n"," [-0.00083643]\n"," [ 0.0012539 ]]\n","Iter:  4857 loss =  0.007140354164947305 learning rate =  0.5 update =  [[-0.00083626]\n"," [-0.00083626]\n"," [ 0.00125364]]\n","Iter:  4858 loss =  0.007138869184312403 learning rate =  0.5 update =  [[-0.00083608]\n"," [-0.00083608]\n"," [ 0.00125338]]\n","Iter:  4859 loss =  0.007137384818302751 learning rate =  0.5 update =  [[-0.00083591]\n"," [-0.00083591]\n"," [ 0.00125312]]\n","Iter:  4860 loss =  0.007135901066538015 learning rate =  0.5 update =  [[-0.00083574]\n"," [-0.00083574]\n"," [ 0.00125287]]\n","Iter:  4861 loss =  0.007134417928638257 learning rate =  0.5 update =  [[-0.00083556]\n"," [-0.00083556]\n"," [ 0.00125261]]\n","Iter:  4862 loss =  0.007132935404223769 learning rate =  0.5 update =  [[-0.00083539]\n"," [-0.00083539]\n"," [ 0.00125235]]\n","Iter:  4863 loss =  0.007131453492915127 learning rate =  0.5 update =  [[-0.00083522]\n"," [-0.00083522]\n"," [ 0.00125209]]\n","Iter:  4864 loss =  0.007129972194333334 learning rate =  0.5 update =  [[-0.00083505]\n"," [-0.00083505]\n"," [ 0.00125183]]\n","Iter:  4865 loss =  0.007128491508099592 learning rate =  0.5 update =  [[-0.00083487]\n"," [-0.00083487]\n"," [ 0.00125157]]\n","Iter:  4866 loss =  0.00712701143383547 learning rate =  0.5 update =  [[-0.0008347 ]\n"," [-0.0008347 ]\n"," [ 0.00125131]]\n","Iter:  4867 loss =  0.007125531971162829 learning rate =  0.5 update =  [[-0.00083453]\n"," [-0.00083453]\n"," [ 0.00125105]]\n","Iter:  4868 loss =  0.007124053119703806 learning rate =  0.5 update =  [[-0.00083436]\n"," [-0.00083436]\n"," [ 0.0012508 ]]\n","Iter:  4869 loss =  0.007122574879080997 learning rate =  0.5 update =  [[-0.00083418]\n"," [-0.00083418]\n"," [ 0.00125054]]\n","Iter:  4870 loss =  0.007121097248917085 learning rate =  0.5 update =  [[-0.00083401]\n"," [-0.00083401]\n"," [ 0.00125028]]\n","Iter:  4871 loss =  0.007119620228835317 learning rate =  0.5 update =  [[-0.00083384]\n"," [-0.00083384]\n"," [ 0.00125002]]\n","Iter:  4872 loss =  0.007118143818458945 learning rate =  0.5 update =  [[-0.00083367]\n"," [-0.00083367]\n"," [ 0.00124976]]\n","Iter:  4873 loss =  0.007116668017411844 learning rate =  0.5 update =  [[-0.00083349]\n"," [-0.00083349]\n"," [ 0.00124951]]\n","Iter:  4874 loss =  0.007115192825317893 learning rate =  0.5 update =  [[-0.00083332]\n"," [-0.00083332]\n"," [ 0.00124925]]\n","Iter:  4875 loss =  0.007113718241801646 learning rate =  0.5 update =  [[-0.00083315]\n"," [-0.00083315]\n"," [ 0.00124899]]\n","Iter:  4876 loss =  0.007112244266487524 learning rate =  0.5 update =  [[-0.00083298]\n"," [-0.00083298]\n"," [ 0.00124873]]\n","Iter:  4877 loss =  0.007110770899000707 learning rate =  0.5 update =  [[-0.00083281]\n"," [-0.00083281]\n"," [ 0.00124848]]\n","Iter:  4878 loss =  0.007109298138966298 learning rate =  0.5 update =  [[-0.00083263]\n"," [-0.00083263]\n"," [ 0.00124822]]\n","Iter:  4879 loss =  0.007107825986009961 learning rate =  0.5 update =  [[-0.00083246]\n"," [-0.00083246]\n"," [ 0.00124796]]\n","Iter:  4880 loss =  0.007106354439757566 learning rate =  0.5 update =  [[-0.00083229]\n"," [-0.00083229]\n"," [ 0.0012477 ]]\n","Iter:  4881 loss =  0.0071048834998352645 learning rate =  0.5 update =  [[-0.00083212]\n"," [-0.00083212]\n"," [ 0.00124745]]\n","Iter:  4882 loss =  0.007103413165869661 learning rate =  0.5 update =  [[-0.00083195]\n"," [-0.00083195]\n"," [ 0.00124719]]\n","Iter:  4883 loss =  0.0071019434374874225 learning rate =  0.5 update =  [[-0.00083178]\n"," [-0.00083178]\n"," [ 0.00124693]]\n","Iter:  4884 loss =  0.007100474314315665 learning rate =  0.5 update =  [[-0.00083161]\n"," [-0.00083161]\n"," [ 0.00124668]]\n","Iter:  4885 loss =  0.007099005795981907 learning rate =  0.5 update =  [[-0.00083143]\n"," [-0.00083143]\n"," [ 0.00124642]]\n","Iter:  4886 loss =  0.00709753788211378 learning rate =  0.5 update =  [[-0.00083126]\n"," [-0.00083126]\n"," [ 0.00124616]]\n","Iter:  4887 loss =  0.0070960705723393126 learning rate =  0.5 update =  [[-0.00083109]\n"," [-0.00083109]\n"," [ 0.00124591]]\n","Iter:  4888 loss =  0.0070946038662868735 learning rate =  0.5 update =  [[-0.00083092]\n"," [-0.00083092]\n"," [ 0.00124565]]\n","Iter:  4889 loss =  0.007093137763585061 learning rate =  0.5 update =  [[-0.00083075]\n"," [-0.00083075]\n"," [ 0.00124539]]\n","Iter:  4890 loss =  0.007091672263862846 learning rate =  0.5 update =  [[-0.00083058]\n"," [-0.00083058]\n"," [ 0.00124514]]\n","Iter:  4891 loss =  0.007090207366749419 learning rate =  0.5 update =  [[-0.00083041]\n"," [-0.00083041]\n"," [ 0.00124488]]\n","Iter:  4892 loss =  0.0070887430718744055 learning rate =  0.5 update =  [[-0.00083024]\n"," [-0.00083024]\n"," [ 0.00124463]]\n","Iter:  4893 loss =  0.007087279378867568 learning rate =  0.5 update =  [[-0.00083007]\n"," [-0.00083007]\n"," [ 0.00124437]]\n","Iter:  4894 loss =  0.0070858162873589856 learning rate =  0.5 update =  [[-0.0008299 ]\n"," [-0.0008299 ]\n"," [ 0.00124412]]\n","Iter:  4895 loss =  0.007084353796979302 learning rate =  0.5 update =  [[-0.00082973]\n"," [-0.00082973]\n"," [ 0.00124386]]\n","Iter:  4896 loss =  0.007082891907359079 learning rate =  0.5 update =  [[-0.00082956]\n"," [-0.00082956]\n"," [ 0.00124361]]\n","Iter:  4897 loss =  0.007081430618129503 learning rate =  0.5 update =  [[-0.00082939]\n"," [-0.00082939]\n"," [ 0.00124335]]\n","Iter:  4898 loss =  0.007079969928921959 learning rate =  0.5 update =  [[-0.00082921]\n"," [-0.00082921]\n"," [ 0.00124309]]\n","Iter:  4899 loss =  0.007078509839367888 learning rate =  0.5 update =  [[-0.00082904]\n"," [-0.00082904]\n"," [ 0.00124284]]\n","Iter:  4900 loss =  0.007077050349099475 learning rate =  0.5 update =  [[-0.00082887]\n"," [-0.00082887]\n"," [ 0.00124258]]\n","Iter:  4901 loss =  0.007075591457748843 learning rate =  0.5 update =  [[-0.0008287 ]\n"," [-0.0008287 ]\n"," [ 0.00124233]]\n","Iter:  4902 loss =  0.0070741331649486 learning rate =  0.5 update =  [[-0.00082853]\n"," [-0.00082853]\n"," [ 0.00124207]]\n","Iter:  4903 loss =  0.0070726754703316404 learning rate =  0.5 update =  [[-0.00082836]\n"," [-0.00082836]\n"," [ 0.00124182]]\n","Iter:  4904 loss =  0.007071218373531057 learning rate =  0.5 update =  [[-0.00082819]\n"," [-0.00082819]\n"," [ 0.00124157]]\n","Iter:  4905 loss =  0.007069761874180283 learning rate =  0.5 update =  [[-0.00082802]\n"," [-0.00082802]\n"," [ 0.00124131]]\n","Iter:  4906 loss =  0.00706830597191312 learning rate =  0.5 update =  [[-0.00082785]\n"," [-0.00082785]\n"," [ 0.00124106]]\n","Iter:  4907 loss =  0.007066850666363627 learning rate =  0.5 update =  [[-0.00082769]\n"," [-0.00082769]\n"," [ 0.0012408 ]]\n","Iter:  4908 loss =  0.007065395957166178 learning rate =  0.5 update =  [[-0.00082752]\n"," [-0.00082752]\n"," [ 0.00124055]]\n","Iter:  4909 loss =  0.007063941843955342 learning rate =  0.5 update =  [[-0.00082735]\n"," [-0.00082735]\n"," [ 0.00124029]]\n","Iter:  4910 loss =  0.0070624883263661455 learning rate =  0.5 update =  [[-0.00082718]\n"," [-0.00082718]\n"," [ 0.00124004]]\n","Iter:  4911 loss =  0.007061035404033731 learning rate =  0.5 update =  [[-0.00082701]\n"," [-0.00082701]\n"," [ 0.00123979]]\n","Iter:  4912 loss =  0.007059583076593747 learning rate =  0.5 update =  [[-0.00082684]\n"," [-0.00082684]\n"," [ 0.00123953]]\n","Iter:  4913 loss =  0.007058131343681991 learning rate =  0.5 update =  [[-0.00082667]\n"," [-0.00082667]\n"," [ 0.00123928]]\n","Iter:  4914 loss =  0.007056680204934596 learning rate =  0.5 update =  [[-0.0008265 ]\n"," [-0.0008265 ]\n"," [ 0.00123903]]\n","Iter:  4915 loss =  0.0070552296599880665 learning rate =  0.5 update =  [[-0.00082633]\n"," [-0.00082633]\n"," [ 0.00123877]]\n","Iter:  4916 loss =  0.007053779708478938 learning rate =  0.5 update =  [[-0.00082616]\n"," [-0.00082616]\n"," [ 0.00123852]]\n","Iter:  4917 loss =  0.0070523303500444265 learning rate =  0.5 update =  [[-0.00082599]\n"," [-0.00082599]\n"," [ 0.00123827]]\n","Iter:  4918 loss =  0.007050881584321775 learning rate =  0.5 update =  [[-0.00082582]\n"," [-0.00082582]\n"," [ 0.00123801]]\n","Iter:  4919 loss =  0.007049433410948571 learning rate =  0.5 update =  [[-0.00082565]\n"," [-0.00082565]\n"," [ 0.00123776]]\n","Iter:  4920 loss =  0.007047985829562769 learning rate =  0.5 update =  [[-0.00082549]\n"," [-0.00082549]\n"," [ 0.00123751]]\n","Iter:  4921 loss =  0.007046538839802525 learning rate =  0.5 update =  [[-0.00082532]\n"," [-0.00082532]\n"," [ 0.00123725]]\n","Iter:  4922 loss =  0.007045092441306392 learning rate =  0.5 update =  [[-0.00082515]\n"," [-0.00082515]\n"," [ 0.001237  ]]\n","Iter:  4923 loss =  0.007043646633713119 learning rate =  0.5 update =  [[-0.00082498]\n"," [-0.00082498]\n"," [ 0.00123675]]\n","Iter:  4924 loss =  0.007042201416661774 learning rate =  0.5 update =  [[-0.00082481]\n"," [-0.00082481]\n"," [ 0.0012365 ]]\n","Iter:  4925 loss =  0.007040756789791762 learning rate =  0.5 update =  [[-0.00082464]\n"," [-0.00082464]\n"," [ 0.00123624]]\n","Iter:  4926 loss =  0.007039312752742801 learning rate =  0.5 update =  [[-0.00082447]\n"," [-0.00082447]\n"," [ 0.00123599]]\n","Iter:  4927 loss =  0.007037869305154699 learning rate =  0.5 update =  [[-0.00082431]\n"," [-0.00082431]\n"," [ 0.00123574]]\n","Iter:  4928 loss =  0.00703642644666788 learning rate =  0.5 update =  [[-0.00082414]\n"," [-0.00082414]\n"," [ 0.00123549]]\n","Iter:  4929 loss =  0.007034984176922807 learning rate =  0.5 update =  [[-0.00082397]\n"," [-0.00082397]\n"," [ 0.00123524]]\n","Iter:  4930 loss =  0.007033542495560334 learning rate =  0.5 update =  [[-0.0008238 ]\n"," [-0.0008238 ]\n"," [ 0.00123498]]\n","Iter:  4931 loss =  0.007032101402221574 learning rate =  0.5 update =  [[-0.00082363]\n"," [-0.00082363]\n"," [ 0.00123473]]\n","Iter:  4932 loss =  0.007030660896547982 learning rate =  0.5 update =  [[-0.00082347]\n"," [-0.00082347]\n"," [ 0.00123448]]\n","Iter:  4933 loss =  0.007029220978181266 learning rate =  0.5 update =  [[-0.0008233 ]\n"," [-0.0008233 ]\n"," [ 0.00123423]]\n","Iter:  4934 loss =  0.007027781646763364 learning rate =  0.5 update =  [[-0.00082313]\n"," [-0.00082313]\n"," [ 0.00123398]]\n","Iter:  4935 loss =  0.007026342901936612 learning rate =  0.5 update =  [[-0.00082296]\n"," [-0.00082296]\n"," [ 0.00123373]]\n","Iter:  4936 loss =  0.007024904743343571 learning rate =  0.5 update =  [[-0.00082279]\n"," [-0.00082279]\n"," [ 0.00123347]]\n","Iter:  4937 loss =  0.007023467170627204 learning rate =  0.5 update =  [[-0.00082263]\n"," [-0.00082263]\n"," [ 0.00123322]]\n","Iter:  4938 loss =  0.007022030183430582 learning rate =  0.5 update =  [[-0.00082246]\n"," [-0.00082246]\n"," [ 0.00123297]]\n","Iter:  4939 loss =  0.007020593781397125 learning rate =  0.5 update =  [[-0.00082229]\n"," [-0.00082229]\n"," [ 0.00123272]]\n","Iter:  4940 loss =  0.007019157964170646 learning rate =  0.5 update =  [[-0.00082212]\n"," [-0.00082212]\n"," [ 0.00123247]]\n","Iter:  4941 loss =  0.007017722731395158 learning rate =  0.5 update =  [[-0.00082196]\n"," [-0.00082196]\n"," [ 0.00123222]]\n","Iter:  4942 loss =  0.007016288082714986 learning rate =  0.5 update =  [[-0.00082179]\n"," [-0.00082179]\n"," [ 0.00123197]]\n","Iter:  4943 loss =  0.007014854017774684 learning rate =  0.5 update =  [[-0.00082162]\n"," [-0.00082162]\n"," [ 0.00123172]]\n","Iter:  4944 loss =  0.007013420536219147 learning rate =  0.5 update =  [[-0.00082145]\n"," [-0.00082145]\n"," [ 0.00123147]]\n","Iter:  4945 loss =  0.007011987637693639 learning rate =  0.5 update =  [[-0.00082129]\n"," [-0.00082129]\n"," [ 0.00123122]]\n","Iter:  4946 loss =  0.007010555321843539 learning rate =  0.5 update =  [[-0.00082112]\n"," [-0.00082112]\n"," [ 0.00123097]]\n","Iter:  4947 loss =  0.007009123588314622 learning rate =  0.5 update =  [[-0.00082095]\n"," [-0.00082095]\n"," [ 0.00123072]]\n","Iter:  4948 loss =  0.007007692436752892 learning rate =  0.5 update =  [[-0.00082079]\n"," [-0.00082079]\n"," [ 0.00123047]]\n","Iter:  4949 loss =  0.007006261866804808 learning rate =  0.5 update =  [[-0.00082062]\n"," [-0.00082062]\n"," [ 0.00123022]]\n","Iter:  4950 loss =  0.007004831878116773 learning rate =  0.5 update =  [[-0.00082045]\n"," [-0.00082045]\n"," [ 0.00122997]]\n","Iter:  4951 loss =  0.007003402470335868 learning rate =  0.5 update =  [[-0.00082029]\n"," [-0.00082029]\n"," [ 0.00122972]]\n","Iter:  4952 loss =  0.007001973643109181 learning rate =  0.5 update =  [[-0.00082012]\n"," [-0.00082012]\n"," [ 0.00122947]]\n","Iter:  4953 loss =  0.007000545396084193 learning rate =  0.5 update =  [[-0.00081995]\n"," [-0.00081995]\n"," [ 0.00122922]]\n","Iter:  4954 loss =  0.006999117728908643 learning rate =  0.5 update =  [[-0.00081979]\n"," [-0.00081979]\n"," [ 0.00122897]]\n","Iter:  4955 loss =  0.0069976906412305235 learning rate =  0.5 update =  [[-0.00081962]\n"," [-0.00081962]\n"," [ 0.00122872]]\n","Iter:  4956 loss =  0.0069962641326983405 learning rate =  0.5 update =  [[-0.00081945]\n"," [-0.00081945]\n"," [ 0.00122847]]\n","Iter:  4957 loss =  0.0069948382029604884 learning rate =  0.5 update =  [[-0.00081929]\n"," [-0.00081929]\n"," [ 0.00122822]]\n","Iter:  4958 loss =  0.0069934128516659 learning rate =  0.5 update =  [[-0.00081912]\n"," [-0.00081912]\n"," [ 0.00122797]]\n","Iter:  4959 loss =  0.00699198807846382 learning rate =  0.5 update =  [[-0.00081896]\n"," [-0.00081896]\n"," [ 0.00122772]]\n","Iter:  4960 loss =  0.006990563883003689 learning rate =  0.5 update =  [[-0.00081879]\n"," [-0.00081879]\n"," [ 0.00122747]]\n","Iter:  4961 loss =  0.006989140264935241 learning rate =  0.5 update =  [[-0.00081862]\n"," [-0.00081862]\n"," [ 0.00122723]]\n","Iter:  4962 loss =  0.006987717223908403 learning rate =  0.5 update =  [[-0.00081846]\n"," [-0.00081846]\n"," [ 0.00122698]]\n","Iter:  4963 loss =  0.006986294759573556 learning rate =  0.5 update =  [[-0.00081829]\n"," [-0.00081829]\n"," [ 0.00122673]]\n","Iter:  4964 loss =  0.006984872871581338 learning rate =  0.5 update =  [[-0.00081813]\n"," [-0.00081813]\n"," [ 0.00122648]]\n","Iter:  4965 loss =  0.006983451559582476 learning rate =  0.5 update =  [[-0.00081796]\n"," [-0.00081796]\n"," [ 0.00122623]]\n","Iter:  4966 loss =  0.006982030823228177 learning rate =  0.5 update =  [[-0.00081779]\n"," [-0.00081779]\n"," [ 0.00122598]]\n","Iter:  4967 loss =  0.006980610662169902 learning rate =  0.5 update =  [[-0.00081763]\n"," [-0.00081763]\n"," [ 0.00122574]]\n","Iter:  4968 loss =  0.006979191076059315 learning rate =  0.5 update =  [[-0.00081746]\n"," [-0.00081746]\n"," [ 0.00122549]]\n","Iter:  4969 loss =  0.006977772064548419 learning rate =  0.5 update =  [[-0.0008173 ]\n"," [-0.0008173 ]\n"," [ 0.00122524]]\n","Iter:  4970 loss =  0.006976353627289473 learning rate =  0.5 update =  [[-0.00081713]\n"," [-0.00081713]\n"," [ 0.00122499]]\n","Iter:  4971 loss =  0.006974935763935106 learning rate =  0.5 update =  [[-0.00081697]\n"," [-0.00081697]\n"," [ 0.00122474]]\n","Iter:  4972 loss =  0.006973518474137951 learning rate =  0.5 update =  [[-0.0008168]\n"," [-0.0008168]\n"," [ 0.0012245]]\n","Iter:  4973 loss =  0.006972101757551341 learning rate =  0.5 update =  [[-0.00081664]\n"," [-0.00081664]\n"," [ 0.00122425]]\n","Iter:  4974 loss =  0.006970685613828506 learning rate =  0.5 update =  [[-0.00081647]\n"," [-0.00081647]\n"," [ 0.001224  ]]\n","Iter:  4975 loss =  0.006969270042623184 learning rate =  0.5 update =  [[-0.00081631]\n"," [-0.00081631]\n"," [ 0.00122375]]\n","Iter:  4976 loss =  0.006967855043589256 learning rate =  0.5 update =  [[-0.00081614]\n"," [-0.00081614]\n"," [ 0.00122351]]\n","Iter:  4977 loss =  0.006966440616380999 learning rate =  0.5 update =  [[-0.00081598]\n"," [-0.00081598]\n"," [ 0.00122326]]\n","Iter:  4978 loss =  0.0069650267606529166 learning rate =  0.5 update =  [[-0.00081581]\n"," [-0.00081581]\n"," [ 0.00122301]]\n","Iter:  4979 loss =  0.006963613476059717 learning rate =  0.5 update =  [[-0.00081565]\n"," [-0.00081565]\n"," [ 0.00122277]]\n","Iter:  4980 loss =  0.006962200762256501 learning rate =  0.5 update =  [[-0.00081548]\n"," [-0.00081548]\n"," [ 0.00122252]]\n","Iter:  4981 loss =  0.006960788618898655 learning rate =  0.5 update =  [[-0.00081532]\n"," [-0.00081532]\n"," [ 0.00122227]]\n","Iter:  4982 loss =  0.006959377045641682 learning rate =  0.5 update =  [[-0.00081515]\n"," [-0.00081515]\n"," [ 0.00122203]]\n","Iter:  4983 loss =  0.006957966042141564 learning rate =  0.5 update =  [[-0.00081499]\n"," [-0.00081499]\n"," [ 0.00122178]]\n","Iter:  4984 loss =  0.006956555608054428 learning rate =  0.5 update =  [[-0.00081482]\n"," [-0.00081482]\n"," [ 0.00122153]]\n","Iter:  4985 loss =  0.0069551457430366 learning rate =  0.5 update =  [[-0.00081466]\n"," [-0.00081466]\n"," [ 0.00122129]]\n","Iter:  4986 loss =  0.006953736446744971 learning rate =  0.5 update =  [[-0.00081449]\n"," [-0.00081449]\n"," [ 0.00122104]]\n","Iter:  4987 loss =  0.006952327718836412 learning rate =  0.5 update =  [[-0.00081433]\n"," [-0.00081433]\n"," [ 0.00122079]]\n","Iter:  4988 loss =  0.0069509195589681815 learning rate =  0.5 update =  [[-0.00081417]\n"," [-0.00081417]\n"," [ 0.00122055]]\n","Iter:  4989 loss =  0.006949511966797942 learning rate =  0.5 update =  [[-0.000814 ]\n"," [-0.000814 ]\n"," [ 0.0012203]]\n","Iter:  4990 loss =  0.006948104941983356 learning rate =  0.5 update =  [[-0.00081384]\n"," [-0.00081384]\n"," [ 0.00122006]]\n","Iter:  4991 loss =  0.006946698484182625 learning rate =  0.5 update =  [[-0.00081367]\n"," [-0.00081367]\n"," [ 0.00121981]]\n","Iter:  4992 loss =  0.006945292593054065 learning rate =  0.5 update =  [[-0.00081351]\n"," [-0.00081351]\n"," [ 0.00121956]]\n","Iter:  4993 loss =  0.006943887268256274 learning rate =  0.5 update =  [[-0.00081335]\n"," [-0.00081335]\n"," [ 0.00121932]]\n","Iter:  4994 loss =  0.006942482509448164 learning rate =  0.5 update =  [[-0.00081318]\n"," [-0.00081318]\n"," [ 0.00121907]]\n","Iter:  4995 loss =  0.006941078316288993 learning rate =  0.5 update =  [[-0.00081302]\n"," [-0.00081302]\n"," [ 0.00121883]]\n","Iter:  4996 loss =  0.006939674688438125 learning rate =  0.5 update =  [[-0.00081285]\n"," [-0.00081285]\n"," [ 0.00121858]]\n","Iter:  4997 loss =  0.006938271625555326 learning rate =  0.5 update =  [[-0.00081269]\n"," [-0.00081269]\n"," [ 0.00121834]]\n","Iter:  4998 loss =  0.006936869127300615 learning rate =  0.5 update =  [[-0.00081253]\n"," [-0.00081253]\n"," [ 0.00121809]]\n","Iter:  4999 loss =  0.0069354671933343 learning rate =  0.5 update =  [[-0.00081236]\n"," [-0.00081236]\n"," [ 0.00121785]]\n","Iter:  5000 loss =  0.006934065823316801 learning rate =  0.5 update =  [[-0.0008122]\n"," [-0.0008122]\n"," [ 0.0012176]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L_Z-gyIYfdKS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"61204e99-8535-42ce-98c4-908807495582","executionInfo":{"status":"ok","timestamp":1573830273889,"user_tz":-420,"elapsed":909,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["print(ww)"],"execution_count":77,"outputs":[{"output_type":"stream","text":["[[  9.27145888]\n"," [  9.27145888]\n"," [-14.07712714]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6UguMJTVmQjl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6a5535af-3cd7-4926-ce97-c8f90bc8c768","executionInfo":{"status":"ok","timestamp":1573832107334,"user_tz":-420,"elapsed":941,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["s = ww[2]/ww[1]\n","s = float(s)\n","print(s)"],"execution_count":98,"outputs":[{"output_type":"stream","text":["-1.5183292421427312\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KZuo2Lx4j8tH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"46eaca4c-ee64-425a-ff78-a74b802aa1d9","executionInfo":{"status":"ok","timestamp":1573832122391,"user_tz":-420,"elapsed":762,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["xs = [x / 100.0 for x in range(0, 200)]\n","ys = [-x - s for x in xs]\n","print(ys)\n","print(xs)"],"execution_count":99,"outputs":[{"output_type":"stream","text":["[1.5183292421427312, 1.5083292421427312, 1.4983292421427312, 1.4883292421427312, 1.4783292421427312, 1.4683292421427312, 1.4583292421427312, 1.4483292421427312, 1.4383292421427312, 1.4283292421427312, 1.4183292421427312, 1.4083292421427311, 1.3983292421427311, 1.3883292421427313, 1.3783292421427311, 1.3683292421427313, 1.3583292421427313, 1.3483292421427313, 1.3383292421427313, 1.3283292421427313, 1.3183292421427313, 1.3083292421427313, 1.2983292421427313, 1.2883292421427313, 1.2783292421427312, 1.2683292421427312, 1.2583292421427312, 1.2483292421427312, 1.2383292421427312, 1.2283292421427312, 1.2183292421427312, 1.2083292421427312, 1.1983292421427312, 1.1883292421427312, 1.1783292421427312, 1.1683292421427312, 1.1583292421427314, 1.1483292421427311, 1.1383292421427313, 1.1283292421427311, 1.1183292421427313, 1.1083292421427313, 1.0983292421427313, 1.0883292421427313, 1.0783292421427313, 1.0683292421427313, 1.0583292421427313, 1.0483292421427313, 1.0383292421427313, 1.0283292421427312, 1.0183292421427312, 1.0083292421427312, 0.9983292421427312, 0.9883292421427312, 0.9783292421427312, 0.9683292421427312, 0.9583292421427312, 0.9483292421427313, 0.9383292421427313, 0.9283292421427313, 0.9183292421427313, 0.9083292421427313, 0.8983292421427312, 0.8883292421427312, 0.8783292421427312, 0.8683292421427312, 0.8583292421427312, 0.8483292421427312, 0.8383292421427312, 0.8283292421427313, 0.8183292421427313, 0.8083292421427313, 0.7983292421427313, 0.7883292421427313, 0.7783292421427312, 0.7683292421427312, 0.7583292421427312, 0.7483292421427312, 0.7383292421427312, 0.7283292421427312, 0.7183292421427312, 0.7083292421427312, 0.6983292421427313, 0.6883292421427313, 0.6783292421427313, 0.6683292421427313, 0.6583292421427313, 0.6483292421427312, 0.6383292421427312, 0.6283292421427312, 0.6183292421427312, 0.6083292421427312, 0.5983292421427312, 0.5883292421427312, 0.5783292421427313, 0.5683292421427313, 0.5583292421427313, 0.5483292421427313, 0.5383292421427313, 0.5283292421427312, 0.5183292421427312, 0.5083292421427312, 0.4983292421427312, 0.4883292421427312, 0.4783292421427312, 0.4683292421427312, 0.4583292421427312, 0.4483292421427312, 0.43832924214273117, 0.42832924214273116, 0.41832924214273115, 0.40832924214273114, 0.39832924214273113, 0.38832924214273135, 0.37832924214273134, 0.36832924214273133, 0.3583292421427313, 0.3483292421427313, 0.3383292421427313, 0.3283292421427313, 0.3183292421427313, 0.3083292421427313, 0.29832924214273127, 0.28832924214273126, 0.27832924214273125, 0.26832924214273124, 0.25832924214273123, 0.24832924214273122, 0.2383292421427312, 0.2283292421427312, 0.2183292421427312, 0.20832924214273119, 0.19832924214273118, 0.18832924214273117, 0.17832924214273116, 0.16832924214273115, 0.15832924214273114, 0.14832924214273113, 0.13832924214273135, 0.12832924214273134, 0.11832924214273133, 0.10832924214273132, 0.09832924214273131, 0.0883292421427313, 0.07832924214273129, 0.06832924214273128, 0.058329242142731275, 0.048329242142731266, 0.03832924214273126, 0.028329242142731248, 0.01832924214273124, 0.00832924214273123, -0.0016707578572687787, -0.011670757857268788, -0.021670757857268796, -0.031670757857268805, -0.041670757857268814, -0.05167075785726882, -0.06167075785726883, -0.07167075785726884, -0.08167075785726885, -0.09167075785726886, -0.10167075785726887, -0.11167075785726865, -0.12167075785726866, -0.13167075785726867, -0.14167075785726868, -0.1516707578572687, -0.1616707578572687, -0.1716707578572687, -0.18167075785726872, -0.19167075785726873, -0.20167075785726873, -0.21167075785726874, -0.22167075785726875, -0.23167075785726876, -0.24167075785726877, -0.2516707578572688, -0.2616707578572688, -0.2716707578572688, -0.2816707578572688, -0.2916707578572688, -0.3016707578572688, -0.31167075785726883, -0.32167075785726884, -0.33167075785726885, -0.34167075785726886, -0.35167075785726887, -0.36167075785726865, -0.37167075785726866, -0.38167075785726867, -0.3916707578572687, -0.4016707578572687, -0.4116707578572687, -0.4216707578572687, -0.4316707578572687, -0.4416707578572687, -0.45167075785726873, -0.46167075785726874, -0.47167075785726875]\n","[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09, 1.1, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.2, 1.21, 1.22, 1.23, 1.24, 1.25, 1.26, 1.27, 1.28, 1.29, 1.3, 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37, 1.38, 1.39, 1.4, 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48, 1.49, 1.5, 1.51, 1.52, 1.53, 1.54, 1.55, 1.56, 1.57, 1.58, 1.59, 1.6, 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.7, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 1.77, 1.78, 1.79, 1.8, 1.81, 1.82, 1.83, 1.84, 1.85, 1.86, 1.87, 1.88, 1.89, 1.9, 1.91, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98, 1.99]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pv8qfn33syoD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":282},"outputId":"330ff10e-a1b8-4680-f978-a570acffe797","executionInfo":{"status":"ok","timestamp":1573832372536,"user_tz":-420,"elapsed":930,"user":{"displayName":"Khải Bùi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP1vyDqEt5dg-W2V2I-8Vb_Hb9eB-nGlk2qsV_qg=s64","userId":"10762598478693915184"}}},"source":["plt.gca().set_aspect('equal', adjustable='box')\n","plt.plot(xs, ys)\n","plt.plot(X[0], X[1], 'y^')"],"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fab44c34358>]"]},"metadata":{"tags":[]},"execution_count":104},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQ0AAAD4CAYAAAD2OrMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Hi3WWFtAtJ1u","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}